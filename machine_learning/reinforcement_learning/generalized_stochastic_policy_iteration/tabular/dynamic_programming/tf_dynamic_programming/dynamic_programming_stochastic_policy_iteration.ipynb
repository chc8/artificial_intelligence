{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.repeat(a = 1, repeats = number_of_states * max_number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.reshape(a = number_of_state_action_successor_states, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_state_action_successor_states = np.max(a = number_of_state_action_successor_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_transition_probabilities = np.repeat(a = 1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_rewards = np.repeat(a = -1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.reshape(a = state_action_successor_state_indices, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_transition_probabilities = np.reshape(a = state_action_successor_state_transition_probabilities, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_rewards = np.reshape(a = state_action_successor_state_rewards, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounting_factor_gamma = 1.0\n",
    "convergence_threshold = 0.001\n",
    "maximum_number_of_sweeps = 30\n",
    "maximum_number_of_policy_evaluations = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the value functions given the current policy\n",
    "def policy_evaluation(number_of_non_terminal_states, state_action_successor_state_indices_tensor, state_action_successor_state_transition_probabilities_tensor_tensor, state_action_successor_state_rewards_tensor, policy_tensor, convergence_threshold, discounting_factor_gamma, maximum_number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor):\n",
    "    delta = np.finfo(np.float64).max\n",
    "    number_of_policy_evaluations = 0\n",
    "\n",
    "    def while_loop_condition(delta, number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor):\n",
    "        return tf.logical_and(x = tf.greater_equal(x = delta, y = convergence_threshold), y = tf.less(x = number_of_policy_evaluations, y = maximum_number_of_policy_evaluations))\n",
    "    \n",
    "    def while_loop_body(delta, number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor):\n",
    "        def value_non_terminal_state_for_loop(state_index, delta, number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor):\n",
    "            # Cache state-value function for state state_index\n",
    "            temp_state_value_function = tf.gather(params = state_value_function_tensor, indices = state_index)\n",
    "            \n",
    "            # Gather state action successor state slices\n",
    "            state_action_successor_state_indices_tensor_slice = tf.gather(params = state_action_successor_state_indices_tensor, indices = state_index)\n",
    "            state_action_successor_state_transition_probabilities_tensor_slice = tf.gather(params = state_action_successor_state_transition_probabilities_tensor, indices = state_index)\n",
    "            state_action_successor_state_rewards_tensor_slice = tf.gather(params = state_action_successor_state_rewards_tensor, indices = state_index)\n",
    "            \n",
    "            # Update state-action value function based on successor states, transition probabilities, and rewards\n",
    "            x = state_action_successor_state_transition_probabilities_tensor_slice * (state_action_successor_state_rewards_tensor_slice + discounting_factor_gamma * temp_state_value_function)\n",
    "            y = state_action_successor_state_transition_probabilities_tensor_slice * (state_action_successor_state_rewards_tensor_slice + discounting_factor_gamma * tf.gather(params = state_value_function_tensor, indices = state_action_successor_state_indices_tensor_slice))\n",
    "\n",
    "            state_action_value_function_tensor_updated = tf.squeeze(input = tf.where(condition = state_action_successor_state_indices_tensor_slice == state_index, x = x, y = y), axis = 1)\n",
    "            \n",
    "            # Update state value function based on current policy\n",
    "            state_value_function_updated = tf.reduce_sum(input_tensor = tf.gather(params = policy_tensor, indices = state_index) * state_action_value_function_tensor_updated)\n",
    "\n",
    "            # Update delta for convergence criteria to break while loop and update policy\n",
    "            delta = tf.reduce_max(input_tensor = (delta, tf.abs(x = temp_state_value_function - state_value_function_updated)))\n",
    "            \n",
    "            return state_value_function_updated, state_action_value_function_tensor_updated\n",
    "\n",
    "        # Replace non-terminal state for loop with map function\n",
    "        state_value_function_tensor, state_action_value_function_tensor = tf.map_fn(\n",
    "            fn = lambda x: value_non_terminal_state_for_loop(x, delta, number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor), \n",
    "            elems = tf.range(number_of_non_terminal_states), \n",
    "            dtype = (tf.float64, tf.float64))\n",
    "        \n",
    "        # Concat terminal state values back onto state value function\n",
    "        state_value_function_tensor = tf.concat(values = [state_value_function_tensor, tf.constant(value = 0.0, shape = [number_of_terminal_states], dtype = tf.float64)], axis = 0)\n",
    "\n",
    "        number_of_policy_evaluations += 1\n",
    "        \n",
    "        return delta, number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor\n",
    "        \n",
    "    delta, number_of_value_iterations, state_value_function_tensor, state_action_value_function_tensor = tf.while_loop(cond = while_loop_condition, body = while_loop_body, loop_vars = [delta, number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor])\n",
    "        \n",
    "    return state_value_function_tensor, state_action_value_function_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function greedily updates the policy based on the current value function\n",
    "def policy_improvement(number_of_non_terminal_states, state_action_successor_state_indices_tensor, state_action_successor_state_transition_probabilities_tensor, state_action_successor_state_rewards_tensor, policy_tensor, discounting_factor_gamma, state_value_function_tensor):\n",
    "    policy_stable = tf.constant(value = True, dtype = tf.bool)\n",
    "    \n",
    "    def policy_non_terminal_state_for_loop(state_index, policy_stable, policy_tensor):\n",
    "        # Gather state action successor state slices\n",
    "        state_action_successor_state_indices_tensor_slice = tf.gather(params = state_action_successor_state_indices_tensor, indices = state_index)\n",
    "        state_action_successor_state_transition_probabilities_tensor_slice = tf.gather(params = state_action_successor_state_transition_probabilities_tensor, indices = state_index)\n",
    "        state_action_successor_state_rewards_tensor_slice = tf.gather(params = state_action_successor_state_rewards_tensor, indices = state_index)\n",
    "    \n",
    "        # Cache policy for comparison later\n",
    "        old_policy = tf.gather(params = policy_tensor, indices = state_index)\n",
    "\n",
    "        # Update policy greedily from state-value function\n",
    "        policy_tensor_updated = tf.squeeze(input = state_action_successor_state_transition_probabilities_tensor_slice * (state_action_successor_state_rewards_tensor_slice + discounting_factor_gamma * tf.gather(params = state_value_function_tensor, indices = state_action_successor_state_indices_tensor_slice)), axis = 1)\n",
    "\n",
    "        # Save max policy value and find the number of actions that have the same max policy value\n",
    "        max_policy_value = tf.reduce_max(input_tensor = policy_tensor_updated)\n",
    "        max_policy_count = tf.count_nonzero(input_tensor = tf.equal(x = policy_tensor_updated, y = max_policy_value))\n",
    "            \n",
    "        # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "        x = tf.fill(dims = [max_number_of_actions], value = 1.0 / tf.cast(max_policy_count, dtype = tf.float64))\n",
    "        y = tf.cast(tf.fill(dims = [max_number_of_actions], value = 0.0), dtype = tf.float64)\n",
    "                    \n",
    "        policy_tensor_updated = tf.where(condition = tf.equal(x = policy_tensor_updated, y = max_policy_value), x = x, y = y)\n",
    "        \n",
    "        # If policy has changed from old policy\n",
    "        policy_stable_updated = tf.reduce_all(input_tensor = tf.equal(x = policy_tensor_updated, y = old_policy))\n",
    "        \n",
    "        return policy_stable_updated, policy_tensor_updated\n",
    "    \n",
    "    # Replace non-terminal state for loop with map function\n",
    "    policy_stable, policy_tensor = tf.map_fn(\n",
    "        fn = lambda x: policy_non_terminal_state_for_loop(x, policy_stable, policy_tensor), \n",
    "        elems = tf.range(number_of_non_terminal_states), \n",
    "        dtype = (tf.bool, tf.float64))\n",
    "\n",
    "    # Reduce policy stable back to a scalar across all non-terminal states\n",
    "    policy_stable = tf.reduce_all(input_tensor = policy_stable)\n",
    "\n",
    "    return policy_stable, policy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(number_of_non_terminal_states, state_action_successor_state_indices_tensor, state_action_successor_state_transition_probabilities_tensor, state_action_successor_state_rewards_tensor, policy_tensor, convergence_threshold, discounting_factor_gamma, maximum_number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor, maximum_number_of_sweeps):\n",
    "    policy_stable = tf.constant(value = False, dtype = tf.bool)\n",
    "    number_of_sweeps = 0\n",
    "    \n",
    "    def while_loop_condition(policy_stable, number_of_sweeps, state_value_function_tensor, state_action_value_function_tensor, policy_tensor):\n",
    "        return tf.logical_and(x = tf.equal(x = policy_stable, y = tf.constant(value = False, dtype = tf.bool)), y = tf.less(x = number_of_sweeps, y = maximum_number_of_sweeps))\n",
    "      \n",
    "    def while_loop_body(policy_stable, number_of_sweeps, state_value_function_tensor, state_action_value_function_tensor, policy_tensor):\n",
    "        # Policy evaluation\n",
    "        state_value_function_tensor, state_action_value_function_tensor = policy_evaluation(number_of_non_terminal_states, state_action_successor_state_indices_tensor, state_action_successor_state_transition_probabilities_tensor, state_action_successor_state_rewards_tensor, policy_tensor, convergence_threshold, discounting_factor_gamma, maximum_number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor)\n",
    "\n",
    "        # Policy improvement\n",
    "        policy_stable, policy_tensor = policy_improvement(number_of_non_terminal_states, state_action_successor_state_indices_tensor, state_action_successor_state_transition_probabilities_tensor, state_action_successor_state_rewards_tensor, policy_tensor, discounting_factor_gamma, state_value_function_tensor)\n",
    "\n",
    "        number_of_sweeps += 1\n",
    "        \n",
    "        return policy_stable, number_of_sweeps, state_value_function_tensor, state_action_value_function_tensor, policy_tensor\n",
    "      \n",
    "    policy_stable, number_of_sweeps, state_value_function_tensor, state_action_value_function_tensor, policy_tensor = tf.while_loop(cond = while_loop_condition, body = while_loop_body, loop_vars = [policy_stable, number_of_sweeps, state_value_function_tensor, state_action_value_function_tensor, policy_tensor])\n",
    "      \n",
    "    return state_value_function_tensor, state_action_value_function_tensor, policy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final state value function\n",
      "[-1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.  0.]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-4. -3. -2. -4.]\n",
      " [-4. -4. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -4. -4. -2.]\n",
      " [-4. -2. -3. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -4. -4.]\n",
      " [-2. -4. -4. -3.]\n",
      " [-1. -3. -3. -2.]]\n",
      "\n",
      "Final policy\n",
      "[[0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Read in environment\n",
    "    state_action_successor_state_indices_tensor = tf.placeholder(dtype = tf.int64, shape = [number_of_non_terminal_states, max_number_of_actions, max_number_of_state_action_successor_states])\n",
    "    state_action_successor_state_transition_probabilities_tensor = tf.placeholder(dtype = tf.float64, shape = [number_of_non_terminal_states, max_number_of_actions, max_number_of_state_action_successor_states])\n",
    "    state_action_successor_state_rewards_tensor = tf.placeholder(dtype = tf.float64, shape = [number_of_non_terminal_states, max_number_of_actions, max_number_of_state_action_successor_states])\n",
    "\n",
    "    # Create value functions\n",
    "    state_value_function_tensor = tf.zeros(shape = number_of_states, dtype = tf.float64)\n",
    "    state_action_value_function_tensor = tf.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = tf.float64)\n",
    "\n",
    "    # Create policy\n",
    "    policy_tensor = tf.tile(input = [tf.constant(value = 1.0 / max_number_of_actions, dtype = tf.float64)], multiples = [number_of_non_terminal_states * max_number_of_actions])\n",
    "    policy_tensor = tf.reshape(tensor = policy_tensor, shape = [number_of_non_terminal_states, max_number_of_actions])\n",
    "\n",
    "    # Create algorithm\n",
    "    algorithm = policy_iteration(number_of_non_terminal_states, state_action_successor_state_indices_tensor, state_action_successor_state_transition_probabilities_tensor, state_action_successor_state_rewards_tensor, policy_tensor, convergence_threshold, discounting_factor_gamma, maximum_number_of_policy_evaluations, state_value_function_tensor, state_action_value_function_tensor, maximum_number_of_sweeps)\n",
    "\n",
    "    # Run graph\n",
    "    state_value_function, state_action_value_function, policy = sess.run(fetches = algorithm, feed_dict = {state_action_successor_state_indices_tensor: state_action_successor_state_indices, \n",
    "                                                                                                                                state_action_successor_state_transition_probabilities_tensor: state_action_successor_state_transition_probabilities, \n",
    "                                                                                                                                state_action_successor_state_rewards_tensor: state_action_successor_state_rewards})\n",
    "print(\"\\nFinal state value function\")\n",
    "print(state_value_function)\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
