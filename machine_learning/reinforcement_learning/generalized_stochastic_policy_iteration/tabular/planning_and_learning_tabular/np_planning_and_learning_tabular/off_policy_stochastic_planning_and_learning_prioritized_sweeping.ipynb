{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all environment properties in\n",
    "class Environment:\n",
    "    def __init__(self, number_of_states, number_of_non_terminal_states, max_number_of_actions):\n",
    "        # Create environment state-action successor state arrrays\n",
    "        self.number_of_state_action_successor_states = np.ones(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "\n",
    "        self.state_action_successor_state_indices = np.reshape(a= np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        self.state_action_successor_state_transition_probabilities = np.reshape(a = np.repeat(a = 1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        self.state_action_successor_state_rewards = np.reshape(a = np.repeat(a = -1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        \n",
    "environment = Environment(number_of_states, number_of_non_terminal_states, max_number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all model properties in\n",
    "class Model:\n",
    "    def __init__(self, number_of_states, number_of_non_terminal_states, max_number_of_actions):\n",
    "        # Create model state visit counters\n",
    "        self.number_of_seen_non_terminal_states = 0\n",
    "        self.seen_non_terminal_states_stack = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_stack_reverse_lookup = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        \n",
    "        # Create model state-action visit counters\n",
    "        self.number_of_seen_non_terminal_states_actions = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_actions_stack = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_actions_stack_reverse_lookup = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        self.state_action_time_since_last_visit = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        \n",
    "        # Create model state-action successor state arrrays\n",
    "        self.number_of_state_action_successor_states = np.zeros(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "\n",
    "        self.state_action_successor_state_indices = np.array(object = [[[0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_transition_probabilities = np.array(object = [[[0.0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_rewards = np.array(object = [[[0.0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_number_of_visits = np.array(object = [[[0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        del self.state_action_successor_state_indices[0, 0][0]\n",
    "        del self.state_action_successor_state_transition_probabilities[0, 0][0]\n",
    "        del self.state_action_successor_state_rewards[0, 0][0]\n",
    "        del self.state_action_successor_state_number_of_visits[0, 0][0]\n",
    "        \n",
    "        self.number_of_state_predecessor_state_action_pairs = np.zeros(shape = [number_of_states], dtype = np.int64)\n",
    "        self.state_predecessor_state_action_pairs = {state_index: [] for state_index in range(0, number_of_states)}\n",
    "        \n",
    "model = Model(number_of_states, number_of_non_terminal_states, max_number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create priority queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all environment properties in\n",
    "class PriorityQueueNode:\n",
    "    def __init__(self, i):\n",
    "        # Create environment state-action successor state arrrays\n",
    "        self.state_index = -i\n",
    "        self.action_index = i\n",
    "        self.priority = np.finfo(float).min\n",
    "\n",
    "priority_queue = np.empty(shape = [number_of_non_terminal_states * max_number_of_actions], dtype = object)\n",
    "for i in range(0, number_of_non_terminal_states * max_number_of_actions):\n",
    "    priority_queue[i] = PriorityQueueNode(i)\n",
    "priority_queue[0].priority = np.finfo(float).max\n",
    "\n",
    "current_priority_queue_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes\n",
    "number_of_episodes = 10000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 200\n",
    "# Set the number of steps for the planning stage\n",
    "number_of_planning_steps = 1\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.1\n",
    "# Set discounting factor gamma\n",
    "discounting_factor_gamma = 1.0\n",
    "# Set small threshold for adding state-action pairs to priority queue\n",
    "theta = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_value_function = np.repeat(a = 0.0, repeats = number_of_states * max_number_of_actions)\n",
    "state_action_value_function = np.reshape(a = state_action_value_function, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(a = 1.0 / max_number_of_actions, repeats = number_of_non_terminal_states * max_number_of_actions)\n",
    "policy = np.reshape(a = policy, newshape = (number_of_non_terminal_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(number_of_non_terminal_states):\n",
    "    # Initial state\n",
    "    initial_state_index = np.random.randint(low = 0, high = number_of_non_terminal_states, dtype = np.int64) # randomly choose an initial state from all non-terminal states\n",
    "\n",
    "    return initial_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy):\n",
    "    # Save max state-action value and find the number of actions that have the same max state-action value\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function[state_index, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if max_action_count == max_number_of_actions:\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remaining_apportioned_probability_per_action = epsilon / (max_number_of_actions - max_action_count)\n",
    "\n",
    "    policy[state_index, :] = np.where(state_action_value_function[state_index, :] == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, theta, maximum_episode_length, number_of_planning_steps, state_index, current_priority_queue_size, priority_queue):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Get epsilon-greedy action\n",
    "        action_index, policy = select_action_from_epsilon_greedy_policy(max_number_of_actions, state_action_value_function, epsilon, state_index, policy)\n",
    "        \n",
    "        # Update what state and actions the model has seen\n",
    "        model = update_model_seen_state_actions(state_index, action_index, model)\n",
    "        \n",
    "        # Get reward\n",
    "        reward, successor_state_transition_index = observe_reward(state_index, action_index, environment)\n",
    "\n",
    "        # Get next state\n",
    "        next_state_index = environment.state_action_successor_state_indices[state_index, action_index, successor_state_transition_index]\n",
    "        \n",
    "        # Update model from environment experience\n",
    "        model = update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model)\n",
    "\n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            # Calculate priority\n",
    "            priority = np.abs(reward - state_action_value_function[state_index, action_index])\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            next_action_index = select_max_state_action_value_function_action(next_state_index, max_number_of_actions, state_action_value_function)\n",
    "\n",
    "            # Calculate priority\n",
    "            priority = np.abs(reward + discounting_factor_gamma * state_action_value_function[next_state_index][next_action_index] - state_action_value_function[state_index][action_index])\n",
    "            \n",
    "        # Check if priority is over threshold to add to priority queue\n",
    "        if priority > theta:\n",
    "            priority_queue, current_priority_queue_size = search_and_update_priority_queue(state_index, action_index, priority, current_priority_queue_size, priority_queue)\n",
    "\n",
    "        # Use updated model to simulate experience in planning phase\n",
    "        state_action_value_function, priority_queue, current_priority_queue_size = model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, theta, state_action_value_function, current_priority_queue_size, priority_queue)\n",
    "        \n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            break # break i loop, episode terminated since we ended up in a terminal state\n",
    "            \n",
    "        # Update state to next state\n",
    "        state_index = next_state_index\n",
    "\n",
    "    return state_action_value_function, policy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects an action in state state_index from epsilon-greedy policy\n",
    "def select_action_from_epsilon_greedy_policy(max_number_of_actions, state_action_value_function, epsilon, state_index, policy):\n",
    "    # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "    policy = epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy)\n",
    "\n",
    "    # Get epsilon-greedy action\n",
    "    action_index = np.random.choice(a = max_number_of_actions, p = policy[state_index, :])\n",
    "\n",
    "    return action_index, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function observes the reward from the environment by taking action action_index in state state_index\n",
    "def observe_reward(state_index, action_index, system):\n",
    "    successor_state_transition_index = np.random.choice(a = system.number_of_state_action_successor_states[state_index, action_index], p = system.state_action_successor_state_transition_probabilities[state_index, action_index][:])\n",
    "\n",
    "    reward = system.state_action_successor_state_rewards[state_index, action_index][successor_state_transition_index]\n",
    "\n",
    "    return reward, successor_state_transition_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects the action that leads gives the maximum state-action value function for the given state\n",
    "def select_max_state_action_value_function_action(state_index, max_number_of_actions, state_action_value_function):\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_stack = np.extract(condition = state_action_value_function[state_index, :] == max_action_value, arr = np.arange(max_number_of_actions))\n",
    "\n",
    "    next_action_index = np.random.choice(a = max_action_stack)\n",
    "\n",
    "    return next_action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates what state and actions the model has seen\n",
    "def update_model_seen_state_actions(state_index, action_index, model):\n",
    "    # Check to see if state has already been visited\n",
    "    if model.number_of_seen_non_terminal_states == 0 or (model.seen_non_terminal_states_stack_reverse_lookup[state_index] == 0 and model.seen_non_terminal_states_stack[0] != state_index): # if new state\n",
    "        # Add to state stack\n",
    "        model.seen_non_terminal_states_stack[model.number_of_seen_non_terminal_states] = state_index # 1, 3, 2, 0, 4\n",
    "        model.seen_non_terminal_states_stack_reverse_lookup[state_index] = model.number_of_seen_non_terminal_states # 3, 0, 2, 1, 4\n",
    "\n",
    "        # Add to action stack\n",
    "        model.seen_non_terminal_states_actions_stack[state_index][model.number_of_seen_non_terminal_states_actions[state_index]] = action_index # 2, 0, 3, 1\n",
    "        model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] = model.number_of_seen_non_terminal_states_actions[state_index] # 1, 3, 0, 2\n",
    "\n",
    "        # Increment counters\n",
    "        model.number_of_seen_non_terminal_states_actions[state_index] += 1\n",
    "        model.number_of_seen_non_terminal_states += 1\n",
    "    else: # if already visited state\n",
    "        # Check to see if action has already been visited\n",
    "        if model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] == 0 and model.seen_non_terminal_states_actions_stack[state_index][0] != action_index:\n",
    "            # Add to action stack\n",
    "            model.seen_non_terminal_states_actions_stack[state_index][model.number_of_seen_non_terminal_states_actions[state_index]] = action_index # 2, 0, 3, 1\n",
    "            model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] = model.number_of_seen_non_terminal_states_actions[state_index] # 1, 3, 0, 2\n",
    "\n",
    "            # Increment counters\n",
    "            model.number_of_seen_non_terminal_states_actions[state_index] += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the model from environment experience\n",
    "def update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model):\n",
    "    # Update model successor arrays\n",
    "    if next_state_index in model.state_action_successor_state_indices[state_index, action_index]:\n",
    "        model.successor_index = model.state_action_successor_state_indices[state_index, action_index].index(next_state_index)\n",
    "        model.state_action_successor_state_number_of_visits[state_index, action_index][model.successor_index] += 1\n",
    "    else:\n",
    "        model.number_of_state_action_successor_states[state_index, action_index] += 1\n",
    "        model.state_action_successor_state_indices[state_index, action_index].append(next_state_index)\n",
    "        model.state_action_successor_state_rewards[state_index, action_index].append(reward)\n",
    "        model.state_action_successor_state_number_of_visits[state_index, action_index].append(1)\n",
    "\n",
    "    model.state_action_successor_state_number_of_visits_sum = np.sum(a = np.asarray(a = model.state_action_successor_state_number_of_visits[state_index, action_index]))\n",
    "    model.state_action_successor_state_transition_probabilities[state_index, action_index] = [float(model.state_action_successor_state_number_of_visits[state_index, action_index][successor_index]) / model.state_action_successor_state_number_of_visits_sum for successor_index in range(0, model.number_of_state_action_successor_states[state_index, action_index])]\n",
    "    \n",
    "    # Update model state predecessors\n",
    "    if (state_index, action_index) not in model.state_predecessor_state_action_pairs[next_state_index]:\n",
    "        model.state_predecessor_state_action_pairs[next_state_index].append((state_index, action_index))\n",
    "        \n",
    "        model.number_of_state_predecessor_state_action_pairs[next_state_index] += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, theta, state_action_value_function, current_priority_queue_size, priority_queue):\n",
    "    for i in range(0, number_of_planning_steps):\n",
    "        # Check if priority queue is empty\n",
    "        if current_priority_queue_size == 0:\n",
    "            break # break i loop since priority queue is empty\n",
    "            \n",
    "        # Get max priority state-action pair from queue\n",
    "        state_index, action_index, priority_queue, current_priority_queue_size = pop_max_node_from_priority_queue(current_priority_queue_size, priority_queue)\n",
    "        \n",
    "        # Get reward\n",
    "        reward, successor_state_transition_index = observe_reward(state_index, action_index, model)\n",
    "\n",
    "        # Get next state\n",
    "        next_state_index = model.state_action_successor_state_indices[state_index, action_index][successor_state_transition_index]\n",
    "        \n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward - state_action_value_function[state_index, action_index])\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            next_action_index = select_max_state_action_value_function_action(next_state_index, max_number_of_actions, state_action_value_function)\n",
    "\n",
    "            # Calculate state-action-function using quintuple SARSargmax(a,Q)\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward + discounting_factor_gamma * state_action_value_function[next_state_index, next_action_index] - state_action_value_function[state_index, action_index])\n",
    "        \n",
    "        # Loop for all predicted Sbar and Abar to lead to S\n",
    "        for j in range(0, model.number_of_state_predecessor_state_action_pairs[state_index]):\n",
    "            predecessor_state_index = model.state_predecessor_state_action_pairs[state_index][j][0]\n",
    "            predecessor_action_index = model.state_predecessor_state_action_pairs[state_index][j][1]\n",
    "\n",
    "            # Get reward\n",
    "            if state_index in model.state_action_successor_state_indices[predecessor_state_index, predecessor_action_index]:\n",
    "                successor_state_transition_index = model.state_action_successor_state_indices[predecessor_state_index, predecessor_action_index].index(state_index)\n",
    "            \n",
    "            # Get reward from predecessor state and action\n",
    "            reward = model.state_action_successor_state_rewards[state_index, action_index][successor_state_transition_index]\n",
    "\n",
    "            # Get next action, max action of next state\n",
    "            next_action_index = select_max_state_action_value_function_action(state_index, max_number_of_actions, state_action_value_function)\n",
    "\n",
    "            # Calculate priority\n",
    "            priority = np.abs(reward + discounting_factor_gamma * state_action_value_function[state_index, next_action_index] - state_action_value_function[predecessor_state_index, predecessor_action_index])\n",
    "              \n",
    "            # Check if priority is over threshold to add to priority queue\n",
    "            if priority > theta:\n",
    "                priority_queue, current_priority_queue_size = search_and_update_priority_queue(predecessor_state_index, predecessor_action_index, priority, current_priority_queue_size, priority_queue)\n",
    "    \n",
    "    return state_action_value_function, priority_queue, current_priority_queue_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function searches for and updates a node in the priority queue\n",
    "def search_and_update_priority_queue(state_index, action_index, priority, current_priority_queue_size, priority_queue):\n",
    "    priority_queue_index = -1\n",
    "    priority_queue_index = search_priority_queue(state_index, action_index, current_priority_queue_size, priority_queue)\n",
    "    \n",
    "    # Check if node was found\n",
    "    if priority_queue_index >= 0:\n",
    "        # Check if found node has a lower priority saved than new priority\n",
    "        if priority_queue[priority_queue_index].priority < priority:\n",
    "            priority_queue = priority_queue_node_increase_priority(priority_queue_index, priority, priority_queue)\n",
    "    else:\n",
    "        # Node wasn't found so insert into priority queue\n",
    "        priority_queue, current_priority_queue_size = insert_into_priority_queue(state_index, action_index, priority, current_priority_queue_size, priority_queue)\n",
    "\n",
    "    return priority_queue, current_priority_queue_size\n",
    "\n",
    "# This function searches for a node in the priority queue\n",
    "def search_priority_queue(state_index, action_index, current_priority_queue_size, priority_queue):\n",
    "    priority_queue_index = -1\n",
    "\n",
    "    # Search up to all nodes in worst case\n",
    "    for i in range(0, current_priority_queue_size):\n",
    "        if priority_queue[i].state_index == state_index and priority_queue[i].action_index == action_index:\n",
    "            priority_queue_index = i\n",
    "            break # break i loop since we found node\n",
    "\n",
    "    return priority_queue_index\n",
    "\n",
    "# This function increases priority at priority_queue_index to new_priority, where it is assumed that new_priority is greater than priority_queue[priority_queue_index]\n",
    "def priority_queue_node_increase_priority(priority_queue_index, new_priority, priority_queue):\n",
    "    priority_queue[priority_queue_index].priority = new_priority\n",
    "    \n",
    "    while priority_queue_index != 0 and priority_queue[parent_priority_queue_node_index(priority_queue_index)].priority < priority_queue[priority_queue_index].priority:\n",
    "        priority_queue[priority_queue_index], priority_queue[parent_priority_queue_node_index(priority_queue_index)] = swap_priority_queue_nodes(priority_queue[priority_queue_index], priority_queue[parent_priority_queue_node_index(priority_queue_index)])\n",
    "        priority_queue_index = parent_priority_queue_node_index(priority_queue_index)\n",
    "\n",
    "    return priority_queue\n",
    "\n",
    "# This function inserts a node into the priority queue\n",
    "def insert_into_priority_queue(state_index, action_index, priority, current_priority_queue_size, priority_queue):\n",
    "    # First insert the new node at the end\n",
    "    current_priority_queue_size += 1\n",
    "    priority_queue_index = current_priority_queue_size - 1\n",
    "    \n",
    "    priority_queue[priority_queue_index].state_index = state_index\n",
    "    priority_queue[priority_queue_index].action_index = action_index\n",
    "    priority_queue[priority_queue_index].priority = priority\n",
    "\n",
    "    # Fix the max heap property if it is violated\n",
    "    while priority_queue_index != 0 and priority_queue[parent_priority_queue_node_index(priority_queue_index)].priority < priority_queue[priority_queue_index].priority:\n",
    "        priority_queue[priority_queue_index], priority_queue[parent_priority_queue_node_index(priority_queue_index)] = swap_priority_queue_nodes(priority_queue[priority_queue_index], priority_queue[parent_priority_queue_node_index(priority_queue_index)])\n",
    "        priority_queue_index = parent_priority_queue_node_index(priority_queue_index)\n",
    "\n",
    "    return priority_queue, current_priority_queue_size\n",
    "\n",
    "# This function pops max node off from priority queue\n",
    "def pop_max_node_from_priority_queue(current_priority_queue_size, priority_queue):\n",
    "    if current_priority_queue_size == 1:\n",
    "        current_priority_queue_size -= 1\n",
    "        return priority_queue[0].state_index, priority_queue[0].action_index, priority_queue, current_priority_queue_size\n",
    "\n",
    "    # Store the maximum value, and remove it from heap\n",
    "    state_index = priority_queue[0].state_index\n",
    "    action_index = priority_queue[0].action_index\n",
    "\n",
    "    priority_queue[0].state_index = priority_queue[current_priority_queue_size - 1].state_index\n",
    "    priority_queue[0].action_index = priority_queue[current_priority_queue_size - 1].action_index\n",
    "    priority_queue[0].priority = priority_queue[current_priority_queue_size - 1].priority\n",
    "    current_priority_queue_size -= 1\n",
    "\n",
    "    # Fix the max heap property if it is violated\n",
    "    priority_queue = max_heapify_priority_queue(0, current_priority_queue_size, priority_queue)\n",
    "    \n",
    "    return state_index, action_index, priority_queue, current_priority_queue_size\n",
    "\n",
    "# This function recursively heapifies a subtree with the root at given index, however assumes that the subtrees are already heapified\n",
    "def max_heapify_priority_queue(priority_queue_index, current_priority_queue_size, priority_queue):\n",
    "    l = left_priority_queue_node_index(priority_queue_index)\n",
    "    r = right_priority_queue_node_index(priority_queue_index)\n",
    "    biggest = priority_queue_index\n",
    "\n",
    "    if l < current_priority_queue_size and priority_queue[l].priority > priority_queue[priority_queue_index].priority:\n",
    "        biggest = l\n",
    "\n",
    "    if r < current_priority_queue_size and priority_queue[r].priority > priority_queue[biggest].priority:\n",
    "        biggest = r\n",
    "\n",
    "    if biggest != priority_queue_index:\n",
    "        temp_state_index = priority_queue[priority_queue_index].state_index\n",
    "        temp_action_index = priority_queue[priority_queue_index].action_index\n",
    "        temp_priority = priority_queue[priority_queue_index].priority\n",
    "\n",
    "        priority_queue[priority_queue_index].state_index = priority_queue[biggest].state_index\n",
    "        priority_queue[priority_queue_index].action_index = priority_queue[biggest].action_index\n",
    "        priority_queue[priority_queue_index].priority = priority_queue[biggest].priority\n",
    "\n",
    "        priority_queue[biggest].state_index = temp_state_index\n",
    "        priority_queue[biggest].action_index = temp_action_index\n",
    "        priority_queue[biggest].priority = temp_priority\n",
    "\n",
    "        priority_queue = max_heapify_priority_queue(biggest, current_priority_queue_size, priority_queue)\n",
    "\n",
    "    return priority_queue\n",
    "\n",
    "def swap_priority_queue_nodes(x, y):\n",
    "    temp_state_index = x.state_index\n",
    "    temp_action_index = x.action_index\n",
    "    temp_priority = x.priority\n",
    "    \n",
    "    x.state_index = y.state_index\n",
    "    x.action_index = y.action_index\n",
    "    x.priority = y.priority\n",
    "    \n",
    "    y.state_index = temp_state_index\n",
    "    y.action_index = temp_action_index\n",
    "    y.priority = temp_priority\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# This function gets the parent index of the given priority queue node's index\n",
    "def parent_priority_queue_node_index(priority_queue_index):\n",
    "    return (priority_queue_index - 1) // 2\n",
    "\n",
    "# This function gets the left child index of the given priority queue node's index\n",
    "def left_priority_queue_node_index(priority_queue_index):\n",
    "    return (2 * priority_queue_index + 1)\n",
    "\n",
    "# This function gets the right child index of the given priority queue node's index\n",
    "def right_priority_queue_node_index(priority_queue_index):\n",
    "    return (2 * priority_queue_index + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_planning_and_learning_prioritized_sweeping(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, theta, maximum_episode_length, number_of_planning_steps, current_priority_queue_size, priority_queue):\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        # Initialize episode to get initial state\n",
    "        initial_state_index = initialize_epsiode(number_of_non_terminal_states)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        state_action_value_function, policy, model = loop_through_episode(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, theta, maximum_episode_length, number_of_planning_steps, initial_state_index, current_priority_queue_size, priority_queue)\n",
    "    \n",
    "    return state_action_value_function, policy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-3. -2. -1. -3.]\n",
      " [-4. -3. -2. -4.]\n",
      " [-4. -4. -3. -3.]\n",
      " [-3. -1. -2. -3.]\n",
      " [-4. -2. -2. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-3. -4. -4. -2.]\n",
      " [-4. -2. -3. -4.]\n",
      " [-3. -3. -3. -3.]\n",
      " [-2. -4. -4. -2.]\n",
      " [-2. -3. -3. -1.]\n",
      " [-3. -3. -4. -4.]\n",
      " [-2. -4. -4. -3.]\n",
      " [-1. -3. -3. -2.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "Final policy\n",
      "[[0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.05       0.05       0.45       0.45      ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.05       0.45       0.45       0.05      ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run off policy planning and learning prioritized sweeping\n",
    "state_action_value_function, policy, model = off_policy_planning_and_learning_prioritized_sweeping(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, theta, maximum_episode_length, number_of_planning_steps, current_priority_queue_size, priority_queue)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.number_of_seen_non_terminal_states\n",
      "14\n",
      "model.seen_non_terminal_states_stack\n",
      "[12 11  7  3 13  8  4  5  6  2  1  0  9 10]\n",
      "model.seen_non_terminal_states_stack_reverse_lookup\n",
      "[11 10  9  3  6  7  8  2  5 12 13  1  0  4]\n",
      "model.number_of_seen_non_terminal_states_actions\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "model.seen_non_terminal_states_actions_stack\n",
      "[[0 2 3 1]\n",
      " [0 2 3 1]\n",
      " [0 2 1 3]\n",
      " [3 2 1 0]\n",
      " [0 2 3 1]\n",
      " [0 1 3 2]\n",
      " [1 0 2 3]\n",
      " [1 2 3 0]\n",
      " [1 0 2 3]\n",
      " [0 2 1 3]\n",
      " [3 0 2 1]\n",
      " [2 1 3 0]\n",
      " [2 0 1 3]\n",
      " [2 1 3 0]]\n",
      "model.seen_non_terminal_states_actions_stack_reverse_lookup\n",
      "[[0 3 1 2]\n",
      " [0 3 1 2]\n",
      " [0 2 1 3]\n",
      " [3 2 1 0]\n",
      " [0 3 1 2]\n",
      " [0 1 3 2]\n",
      " [1 0 2 3]\n",
      " [3 0 1 2]\n",
      " [1 0 2 3]\n",
      " [0 2 1 3]\n",
      " [1 3 2 0]\n",
      " [3 1 0 2]\n",
      " [1 2 0 3]\n",
      " [3 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Print model seen arrays\n",
    "print(\"model.number_of_seen_non_terminal_states\")\n",
    "print(model.number_of_seen_non_terminal_states)\n",
    "print(\"model.seen_non_terminal_states_stack\")\n",
    "print(model.seen_non_terminal_states_stack)\n",
    "print(\"model.seen_non_terminal_states_stack_reverse_lookup\")\n",
    "print(model.seen_non_terminal_states_stack_reverse_lookup)\n",
    "print(\"model.number_of_seen_non_terminal_states_actions\")\n",
    "print(model.number_of_seen_non_terminal_states_actions)\n",
    "print(\"model.seen_non_terminal_states_actions_stack\")\n",
    "print(model.seen_non_terminal_states_actions_stack)\n",
    "print(\"model.seen_non_terminal_states_actions_stack_reverse_lookup\")\n",
    "print(model.seen_non_terminal_states_actions_stack_reverse_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.number_of_state_action_successor_states\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "model.state_action_successor_state_indices\n",
      "[[list([1]) list([0]) list([14]) list([4])]\n",
      " [list([2]) list([1]) list([0]) list([5])]\n",
      " [list([2]) list([2]) list([1]) list([6])]\n",
      " [list([4]) list([14]) list([3]) list([7])]\n",
      " [list([5]) list([0]) list([3]) list([8])]\n",
      " [list([6]) list([1]) list([4]) list([9])]\n",
      " [list([6]) list([2]) list([5]) list([10])]\n",
      " [list([8]) list([3]) list([7]) list([11])]\n",
      " [list([9]) list([4]) list([7]) list([12])]\n",
      " [list([10]) list([5]) list([8]) list([13])]\n",
      " [list([10]) list([6]) list([9]) list([15])]\n",
      " [list([12]) list([7]) list([11]) list([11])]\n",
      " [list([13]) list([8]) list([11]) list([12])]\n",
      " [list([15]) list([9]) list([12]) list([13])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_transition_probabilities\n",
      "[[list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_rewards\n",
      "[[list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_number_of_visits\n",
      "[[list([102]) list([124]) list([2461]) list([96])]\n",
      " [list([57]) list([45]) list([1232]) list([47])]\n",
      " [list([40]) list([43]) list([385]) list([402])]\n",
      " [list([103]) list([2506]) list([123]) list([90])]\n",
      " [list([55]) list([731]) list([773]) list([43])]\n",
      " [list([126]) list([166]) list([264]) list([357])]\n",
      " [list([49]) list([42]) list([45]) list([1265])]\n",
      " [list([61]) list([1203]) list([41]) list([41])]\n",
      " [list([407]) list([364]) list([74]) list([103])]\n",
      " [list([805]) list([55]) list([62]) list([740])]\n",
      " [list([91]) list([103]) list([108]) list([2570])]\n",
      " [list([387]) list([411]) list([32]) list([29])]\n",
      " [list([1193]) list([44]) list([45]) list([53])]\n",
      " [list([2463]) list([94]) list([81]) list([93])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n"
     ]
    }
   ],
   "source": [
    "# Print model successor arrays\n",
    "print(\"model.number_of_state_action_successor_states\")\n",
    "print(model.number_of_state_action_successor_states)\n",
    "print(\"model.state_action_successor_state_indices\")\n",
    "print(model.state_action_successor_state_indices)\n",
    "print(\"model.state_action_successor_state_transition_probabilities\")\n",
    "print(model.state_action_successor_state_transition_probabilities)\n",
    "print(\"model.state_action_successor_state_rewards\")\n",
    "print(model.state_action_successor_state_rewards)\n",
    "print(\"model.state_action_successor_state_number_of_visits\")\n",
    "print(model.state_action_successor_state_number_of_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
