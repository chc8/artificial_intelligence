{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all environment properties in\n",
    "class Environment:\n",
    "    def __init__(self, number_of_states, number_of_non_terminal_states, max_number_of_actions):\n",
    "        # Create environment state-action successor state arrrays\n",
    "        self.number_of_state_action_successor_states = np.ones(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "\n",
    "        self.state_action_successor_state_indices = np.reshape(a= np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        self.state_action_successor_state_transition_probabilities = np.reshape(a = np.repeat(a = 1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        self.state_action_successor_state_rewards = np.reshape(a = np.repeat(a = -1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        \n",
    "environment = Environment(number_of_states, number_of_non_terminal_states, max_number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all model properties in\n",
    "class Model:\n",
    "    def __init__(self, number_of_states, number_of_non_terminal_states, max_number_of_actions):\n",
    "        # Create model state visit counters\n",
    "        self.number_of_seen_non_terminal_states = 0\n",
    "        self.seen_non_terminal_states_stack = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_stack_reverse_lookup = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        \n",
    "        # Create model state-action visit counters\n",
    "        self.number_of_seen_non_terminal_states_actions = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_actions_stack = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_actions_stack_reverse_lookup = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        self.state_action_time_since_last_visit = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        \n",
    "        # Create model state-action successor state arrrays\n",
    "        self.number_of_state_action_successor_states = np.zeros(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "\n",
    "        self.state_action_successor_state_indices = np.array(object = [[[0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_transition_probabilities = np.array(object = [[[0.0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_rewards = np.array(object = [[[0.0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_number_of_visits = np.array(object = [[[0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        del self.state_action_successor_state_indices[0, 0][0]\n",
    "        del self.state_action_successor_state_transition_probabilities[0, 0][0]\n",
    "        del self.state_action_successor_state_rewards[0, 0][0]\n",
    "        del self.state_action_successor_state_number_of_visits[0, 0][0]\n",
    "        \n",
    "model = Model(number_of_states, number_of_non_terminal_states, max_number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes\n",
    "number_of_episodes = 40000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 2000\n",
    "# Set the number of steps for the planning stage\n",
    "number_of_planning_steps = 5\n",
    "# Set learning rate alpha\n",
    "alpha = 0.001\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.05\n",
    "# Set discounting factor gamma\n",
    "discounting_factor_gamma = 1.0\n",
    "# Set the time weight factor for bonus reward for states not visited in a long time from the environment\n",
    "kappa = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_value_function = np.repeat(a = 0.0, repeats = number_of_states * max_number_of_actions)\n",
    "state_action_value_function = np.reshape(a = state_action_value_function, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(a = 1.0 / max_number_of_actions, repeats = number_of_non_terminal_states * max_number_of_actions)\n",
    "policy = np.reshape(a = policy, newshape = (number_of_non_terminal_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(number_of_non_terminal_states):\n",
    "    # Initial state\n",
    "    initial_state_index = np.random.randint(low = 0, high = number_of_non_terminal_states, dtype = np.int64) # randomly choose an initial state from all non-terminal states\n",
    "\n",
    "    return initial_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy):\n",
    "    # Save max state-action value and find the number of actions that have the same max state-action value\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function[state_index, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if max_action_count == max_number_of_actions:\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remaining_apportioned_probability_per_action = epsilon / (max_number_of_actions - max_action_count)\n",
    "\n",
    "    policy[state_index, :] = np.where(state_action_value_function[state_index, :] == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, kappa, maximum_episode_length, number_of_planning_steps, state_index):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "        policy = epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy)\n",
    "\n",
    "        # Get epsilon-greedy action\n",
    "        action_index = np.random.choice(a = max_number_of_actions, p = policy[state_index, :])\n",
    "        \n",
    "        # Update what state and actions the model has seen\n",
    "        model = update_model_seen_state_actions(state_index, action_index, model)\n",
    "        \n",
    "        # Get reward\n",
    "        successor_state_transition_index = np.random.choice(a = environment.number_of_state_action_successor_states[state_index, action_index], p = environment.state_action_successor_state_transition_probabilities[state_index, action_index, :])\n",
    "\n",
    "        reward = environment.state_action_successor_state_rewards[state_index, action_index, successor_state_transition_index]\n",
    "\n",
    "        # Get next state\n",
    "        next_state_index = environment.state_action_successor_state_indices[state_index, action_index, successor_state_transition_index]\n",
    "            \n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            # Update state-action value function\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward - state_action_value_function[state_index, action_index])\n",
    "            \n",
    "            # Update model from environment experience\n",
    "            model= update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model)\n",
    "            \n",
    "            # Use updated model to simulate experience in planning phase\n",
    "            state_action_value_function = model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, kappa, state_action_value_function)\n",
    "            \n",
    "            break; # episode terminated since we ended up in a terminal state\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "            max_action_stack = np.extract(condition = state_action_value_function[state_index, :] == max_action_value, arr = np.arange(max_number_of_actions))\n",
    "            \n",
    "            next_action_index = np.random.choice(a = max_action_stack)\n",
    "\n",
    "            # Calculate state-action-function using quintuple SARSA\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward + discounting_factor_gamma * state_action_value_function[next_state_index, next_action_index] - state_action_value_function[state_index, action_index])\n",
    "            \n",
    "            # Update model from environment experience\n",
    "            model = update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model)\n",
    "            \n",
    "            # Use updated model to simulate experience in planning phase\n",
    "            state_action_value_function = model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, kappa, state_action_value_function)\n",
    "\n",
    "            # Update state and action to next state and action\n",
    "            state_index = next_state_index\n",
    "            action_index = next_action_index\n",
    "\n",
    "    return state_action_value_function, policy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates what state and actions the model has seen\n",
    "def update_model_seen_state_actions(state_index, action_index, model):\n",
    "    # Check to see if state has already been visited\n",
    "    if model.number_of_seen_non_terminal_states == 0 or (model.seen_non_terminal_states_stack_reverse_lookup[state_index] == 0 and model.seen_non_terminal_states_stack[0] != state_index): # if new state\n",
    "        # Add to state stack\n",
    "        model.seen_non_terminal_states_stack[model.number_of_seen_non_terminal_states] = state_index # 1, 3, 2, 0, 4\n",
    "        model.seen_non_terminal_states_stack_reverse_lookup[state_index] = model.number_of_seen_non_terminal_states # 3, 0, 2, 1, 4\n",
    "\n",
    "        # Add to action stack\n",
    "        model.seen_non_terminal_states_actions_stack[state_index][model.number_of_seen_non_terminal_states_actions[state_index]] = action_index # 2, 0, 3, 1\n",
    "        model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] = model.number_of_seen_non_terminal_states_actions[state_index] # 1, 3, 0, 2\n",
    "\n",
    "        # Initialize time since last real state-action pair visit\n",
    "        model.state_action_time_since_last_visit[state_index, model.number_of_seen_non_terminal_states_actions[state_index]] = 0\n",
    "\n",
    "        # Increment counters\n",
    "        model.number_of_seen_non_terminal_states_actions[state_index] += 1\n",
    "        model.number_of_seen_non_terminal_states += 1\n",
    "    else: # if already visited state\n",
    "        # Check to see if action has already been visited\n",
    "        if model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] == 0 and model.seen_non_terminal_states_actions_stack[state_index][0] != action_index:\n",
    "            # Add to action stack\n",
    "            model.seen_non_terminal_states_actions_stack[state_index][model.number_of_seen_non_terminal_states_actions[state_index]] = action_index # 2, 0, 3, 1\n",
    "            model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] = model.number_of_seen_non_terminal_states_actions[state_index] # 1, 3, 0, 2\n",
    "\n",
    "            # Initialize time since last real state-action pair visit\n",
    "            model.state_action_time_since_last_visit[state_index, model.number_of_seen_non_terminal_states_actions[state_index]] = 0\n",
    "\n",
    "            # Increment counters\n",
    "            model.number_of_seen_non_terminal_states_actions[state_index] += 1\n",
    "        else: # if already visited state-action pair\n",
    "            model.state_action_time_since_last_visit[state_index, model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index, action_index]] += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the model from environment experience\n",
    "def update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model):\n",
    "    # Update model successor arrays\n",
    "    if next_state_index in model.state_action_successor_state_indices[state_index, action_index]:\n",
    "        model.successor_index = model.state_action_successor_state_indices[state_index, action_index].index(next_state_index)\n",
    "        model.state_action_successor_state_number_of_visits[state_index, action_index][model.successor_index] += 1\n",
    "    else:\n",
    "        model.number_of_state_action_successor_states[state_index, action_index] += 1\n",
    "        model.state_action_successor_state_indices[state_index, action_index].append(next_state_index)\n",
    "        model.state_action_successor_state_rewards[state_index, action_index].append(reward)\n",
    "        model.state_action_successor_state_number_of_visits[state_index, action_index].append(1)\n",
    "\n",
    "    model.state_action_successor_state_number_of_visits_sum = np.sum(a = np.asarray(a = model.state_action_successor_state_number_of_visits[state_index, action_index]))\n",
    "    model.state_action_successor_state_transition_probabilities[state_index, action_index] = [float(model.state_action_successor_state_number_of_visits[state_index, action_index][successor_index]) / model.state_action_successor_state_number_of_visits_sum for successor_index in range(0, model.number_of_state_action_successor_states[state_index, action_index])]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, kappa, state_action_value_function):\n",
    "    for i in range(0, number_of_planning_steps):\n",
    "        # Randomly choose state indices from previously seen states\n",
    "        state_index = model.seen_non_terminal_states_stack[np.random.randint(low = 0, high = model.number_of_seen_non_terminal_states, dtype = np.int64)]\n",
    "        \n",
    "        # Randomly choose action indices from previously seen actions in previously seen states\n",
    "        action_index = model.seen_non_terminal_states_actions_stack[state_index, np.random.randint(low = 0, high = model.number_of_seen_non_terminal_states_actions[state_index], dtype = np.int64)]\n",
    "        \n",
    "        # Get reward\n",
    "        successor_state_transition_index = np.random.choice(a = np.arange(model.number_of_state_action_successor_states[state_index, action_index]), p = np.asarray(a = model.state_action_successor_state_transition_probabilities[state_index, action_index], dtype = np.float64))\n",
    "    \n",
    "        # Get reward from state and action */\n",
    "        reward = model.state_action_successor_state_rewards[state_index, action_index][successor_state_transition_index]  + kappa * np.sqrt(model.state_action_time_since_last_visit[state_index, model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index, action_index]])\n",
    "\n",
    "        # Get next state */\n",
    "        next_state_index = model.state_action_successor_state_indices[state_index, action_index][successor_state_transition_index]\n",
    "\n",
    "        # Check to see if we actioned into a terminal state */\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward - state_action_value_function[state_index, action_index])\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "            max_action_stack = np.extract(condition = state_action_value_function[state_index, :] == max_action_value, arr = np.arange(max_number_of_actions))\n",
    "            \n",
    "            next_action_index = np.random.choice(a = max_action_stack)\n",
    "\n",
    "            # Calculate state-action-function using quintuple SARSargmax(a,Q) */\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward + discounting_factor_gamma * state_action_value_function[next_state_index, next_action_index] - state_action_value_function[state_index, action_index]);\n",
    "            \n",
    "    return state_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_planning_and_learning_tabular_dyna_q_plus(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, kappa, maximum_episode_length, number_of_planning_steps):\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        # Initialize episode to get initial state\n",
    "        initial_state_index = initialize_epsiode(number_of_non_terminal_states)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        state_action_value_function, policy, model  = loop_through_episode(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, kappa, maximum_episode_length, number_of_planning_steps, initial_state_index)\n",
    "    \n",
    "    return state_action_value_function, policy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-2.98850926 -1.99398164 -0.99560389 -3.55405873]\n",
      " [-8.48065369 -2.98785684 -1.99172433 -8.4920954 ]\n",
      " [-3.97920848 -3.97989512 -8.46068897 -2.9885757 ]\n",
      " [-3.55390533 -0.99562818 -1.99415767 -2.98886289]\n",
      " [-8.47656011 -2.56032137 -2.56039251 -8.48002029]\n",
      " [-3.61277612 -8.47063291 -8.51217348 -3.61264321]\n",
      " [-2.98883937 -3.98039885 -4.58381433 -1.99201217]\n",
      " [-8.49107366 -1.99198947 -2.98802377 -8.4886857 ]\n",
      " [-3.61258486 -8.4786327  -8.49572685 -3.612296  ]\n",
      " [-2.62182313 -4.58366031 -4.58317275 -2.62217784]\n",
      " [-1.99417691 -2.98928347 -3.61166335 -0.99586206]\n",
      " [-2.98860602 -8.48385614 -3.97964526 -3.98001333]\n",
      " [-1.99214504 -4.58302332 -3.98116039 -2.98897373]\n",
      " [-0.99607567 -3.61125166 -2.98906267 -1.99422843]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Final policy\n",
      "[[0.01666667 0.01666667 0.95       0.01666667]\n",
      " [0.01666667 0.01666667 0.95       0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.95       0.01666667 0.01666667]\n",
      " [0.01666667 0.01666667 0.95       0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.95       0.01666667 0.01666667]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run off policy planning and learning tabular dyna-Q+\n",
    "state_action_value_function, policy, model = off_policy_planning_and_learning_tabular_dyna_q_plus(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, kappa, maximum_episode_length, number_of_planning_steps)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.number_of_seen_non_terminal_states\n",
      "14\n",
      "model.seen_non_terminal_states_stack\n",
      "[12 11  7  8 13  4  9  3  0  5  1  6 10  2]\n",
      "model.seen_non_terminal_states_stack_reverse_lookup\n",
      "[ 8 10 13  7  5  9 11  2  3  6 12  1  0  4]\n",
      "model.number_of_seen_non_terminal_states_actions\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "model.seen_non_terminal_states_actions_stack\n",
      "[[3 0 2 1]\n",
      " [3 0 2 1]\n",
      " [1 3 2 0]\n",
      " [3 2 0 1]\n",
      " [3 1 0 2]\n",
      " [1 0 3 2]\n",
      " [3 0 1 2]\n",
      " [3 1 2 0]\n",
      " [2 1 0 3]\n",
      " [3 1 0 2]\n",
      " [3 2 0 1]\n",
      " [3 2 1 0]\n",
      " [2 3 1 0]\n",
      " [2 3 0 1]]\n",
      "model.seen_non_terminal_states_actions_stack_reverse_lookup\n",
      "[[1 3 2 0]\n",
      " [1 3 2 0]\n",
      " [3 0 2 1]\n",
      " [2 3 1 0]\n",
      " [2 1 3 0]\n",
      " [1 0 3 2]\n",
      " [1 2 3 0]\n",
      " [3 1 2 0]\n",
      " [2 1 0 3]\n",
      " [2 1 3 0]\n",
      " [2 3 1 0]\n",
      " [3 2 1 0]\n",
      " [3 2 0 1]\n",
      " [2 3 0 1]]\n",
      "model.state_action_time_since_last_visit\n",
      "[[  164   168  7636   163]\n",
      " [  251   211  3399   300]\n",
      " [  279  2880   460   331]\n",
      " [  168   141   162  7556]\n",
      " [  160  1629   213  1739]\n",
      " [  339  1664  1469   279]\n",
      " [ 7056   308   264   236]\n",
      " [  231  3311   282   196]\n",
      " [  302   349  1450  1735]\n",
      " [ 2927   250  2936   236]\n",
      " [12409   266   251   210]\n",
      " [  352   358   416  2958]\n",
      " [  248   373   349  7136]\n",
      " [  240   248 12395   259]]\n"
     ]
    }
   ],
   "source": [
    "# Print model seen arrays\n",
    "print(\"model.number_of_seen_non_terminal_states\")\n",
    "print(model.number_of_seen_non_terminal_states)\n",
    "print(\"model.seen_non_terminal_states_stack\")\n",
    "print(model.seen_non_terminal_states_stack)\n",
    "print(\"model.seen_non_terminal_states_stack_reverse_lookup\")\n",
    "print(model.seen_non_terminal_states_stack_reverse_lookup)\n",
    "print(\"model.number_of_seen_non_terminal_states_actions\")\n",
    "print(model.number_of_seen_non_terminal_states_actions)\n",
    "print(\"model.seen_non_terminal_states_actions_stack\")\n",
    "print(model.seen_non_terminal_states_actions_stack)\n",
    "print(\"model.seen_non_terminal_states_actions_stack_reverse_lookup\")\n",
    "print(model.seen_non_terminal_states_actions_stack_reverse_lookup)\n",
    "print(\"model.state_action_time_since_last_visit\")\n",
    "print(model.state_action_time_since_last_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.number_of_state_action_successor_states\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "model.state_action_successor_state_indices\n",
      "[[list([1]) list([0]) list([14]) list([4])]\n",
      " [list([2]) list([1]) list([0]) list([5])]\n",
      " [list([2]) list([2]) list([1]) list([6])]\n",
      " [list([4]) list([14]) list([3]) list([7])]\n",
      " [list([5]) list([0]) list([3]) list([8])]\n",
      " [list([6]) list([1]) list([4]) list([9])]\n",
      " [list([6]) list([2]) list([5]) list([10])]\n",
      " [list([8]) list([3]) list([7]) list([11])]\n",
      " [list([9]) list([4]) list([7]) list([12])]\n",
      " [list([10]) list([5]) list([8]) list([13])]\n",
      " [list([10]) list([6]) list([9]) list([15])]\n",
      " [list([12]) list([7]) list([11]) list([11])]\n",
      " [list([13]) list([8]) list([11]) list([12])]\n",
      " [list([15]) list([9]) list([12]) list([13])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_transition_probabilities\n",
      "[[list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_rewards\n",
      "[[list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_number_of_visits\n",
      "[[list([169]) list([164]) list([7637]) list([165])]\n",
      " [list([212]) list([301]) list([3400]) list([252])]\n",
      " [list([332]) list([280]) list([461]) list([2881])]\n",
      " [list([163]) list([7557]) list([142]) list([169])]\n",
      " [list([214]) list([1630]) list([1740]) list([161])]\n",
      " [list([1665]) list([340]) list([280]) list([1470])]\n",
      " [list([309]) list([265]) list([237]) list([7057])]\n",
      " [list([197]) list([3312]) list([283]) list([232])]\n",
      " [list([1451]) list([350]) list([303]) list([1736])]\n",
      " [list([2937]) list([251]) list([237]) list([2928])]\n",
      " [list([252]) list([211]) list([267]) list([12410])]\n",
      " [list([2959]) list([417]) list([359]) list([353])]\n",
      " [list([7137]) list([350]) list([249]) list([374])]\n",
      " [list([12396]) list([260]) list([241]) list([249])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n"
     ]
    }
   ],
   "source": [
    "# Print model successor arrays\n",
    "print(\"model.number_of_state_action_successor_states\")\n",
    "print(model.number_of_state_action_successor_states)\n",
    "print(\"model.state_action_successor_state_indices\")\n",
    "print(model.state_action_successor_state_indices)\n",
    "print(\"model.state_action_successor_state_transition_probabilities\")\n",
    "print(model.state_action_successor_state_transition_probabilities)\n",
    "print(\"model.state_action_successor_state_rewards\")\n",
    "print(model.state_action_successor_state_rewards)\n",
    "print(\"model.state_action_successor_state_number_of_visits\")\n",
    "print(model.state_action_successor_state_number_of_visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
