{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all environment properties in\n",
    "class Environment:\n",
    "    def __init__(self, number_of_states, number_of_non_terminal_states, max_number_of_actions):\n",
    "        # Create environment state-action successor state arrrays\n",
    "        self.number_of_state_action_successor_states = np.ones(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "\n",
    "        self.state_action_successor_state_indices = np.reshape(a= np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        self.state_action_successor_state_transition_probabilities = np.reshape(a = np.repeat(a = 1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        self.state_action_successor_state_rewards = np.reshape(a = np.repeat(a = -1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1), newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "        \n",
    "environment = Environment(number_of_states, number_of_non_terminal_states, max_number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to hold all model properties in\n",
    "class Model:\n",
    "    def __init__(self, number_of_states, number_of_non_terminal_states, max_number_of_actions):\n",
    "        # Create model state visit counters\n",
    "        self.number_of_seen_non_terminal_states = 0\n",
    "        self.seen_non_terminal_states_stack = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_stack_reverse_lookup = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        \n",
    "        # Create model state-action visit counters\n",
    "        self.number_of_seen_non_terminal_states_actions = np.zeros(shape = [number_of_non_terminal_states], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_actions_stack = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        self.seen_non_terminal_states_actions_stack_reverse_lookup = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        self.state_action_time_since_last_visit = np.zeros(shape = [number_of_non_terminal_states, max_number_of_actions], dtype = np.int64)\n",
    "        \n",
    "        # Create model state-action successor state arrrays\n",
    "        self.number_of_state_action_successor_states = np.zeros(shape = [number_of_states, max_number_of_actions], dtype = np.int64)\n",
    "\n",
    "        self.state_action_successor_state_indices = np.array(object = [[[0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_transition_probabilities = np.array(object = [[[0.0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_rewards = np.array(object = [[[0.0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        self.state_action_successor_state_number_of_visits = np.array(object = [[[0] if state_index == 0 and action_index == 0 else [] for action_index in range(0, max_number_of_actions)] for state_index in range(0, number_of_states)], dtype = np.object)\n",
    "        del self.state_action_successor_state_indices[0, 0][0]\n",
    "        del self.state_action_successor_state_transition_probabilities[0, 0][0]\n",
    "        del self.state_action_successor_state_rewards[0, 0][0]\n",
    "        del self.state_action_successor_state_number_of_visits[0, 0][0]\n",
    "        \n",
    "model = Model(number_of_states, number_of_non_terminal_states, max_number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes\n",
    "number_of_episodes = 40000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 2000\n",
    "# Set the number of steps for the planning stage\n",
    "number_of_planning_steps = 5\n",
    "# Set learning rate alpha\n",
    "alpha = 0.001\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.05\n",
    "# Set discounting factor gamma\n",
    "discounting_factor_gamma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_value_function = np.repeat(a = 0.0, repeats = number_of_states * max_number_of_actions)\n",
    "state_action_value_function = np.reshape(a = state_action_value_function, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(a = 1.0 / max_number_of_actions, repeats = number_of_non_terminal_states * max_number_of_actions)\n",
    "policy = np.reshape(a = policy, newshape = (number_of_non_terminal_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(number_of_non_terminal_states):\n",
    "    # Initial state\n",
    "    initial_state_index = np.random.randint(low = 0, high = number_of_non_terminal_states, dtype = np.int64) # randomly choose an initial state from all non-terminal states\n",
    "\n",
    "    return initial_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy):\n",
    "    # Save max state-action value and find the number of actions that have the same max state-action value\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function[state_index, :] == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if max_action_count == max_number_of_actions:\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remaining_apportioned_probability_per_action = epsilon / (max_number_of_actions - max_action_count)\n",
    "\n",
    "    policy[state_index, :] = np.where(state_action_value_function[state_index, :] == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length, number_of_planning_steps, state_index):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "        policy = epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy)\n",
    "\n",
    "        # Get epsilon-greedy action\n",
    "        action_index = np.random.choice(a = max_number_of_actions, p = policy[state_index, :])\n",
    "        \n",
    "        # Update what state and actions the model has seen\n",
    "        model = update_model_seen_state_actions(state_index, action_index, model)\n",
    "        \n",
    "        # Get reward\n",
    "        successor_state_transition_index = np.random.choice(a = environment.number_of_state_action_successor_states[state_index, action_index], p = environment.state_action_successor_state_transition_probabilities[state_index, action_index, :])\n",
    "\n",
    "        reward = environment.state_action_successor_state_rewards[state_index, action_index, successor_state_transition_index]\n",
    "\n",
    "        # Get next state\n",
    "        next_state_index = environment.state_action_successor_state_indices[state_index, action_index, successor_state_transition_index]\n",
    "            \n",
    "        # Check to see if we actioned into a terminal state\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            # Update state-action value function\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward - state_action_value_function[state_index, action_index])\n",
    "            \n",
    "            # Update model from environment experience\n",
    "            model = update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model)\n",
    "            \n",
    "            # Use updated model to simulate experience in planning phase\n",
    "            state_action_value_function = model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, state_action_value_function)\n",
    "            \n",
    "            break; # episode terminated since we ended up in a terminal state\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "            max_action_stack = np.extract(condition = state_action_value_function[state_index, :] == max_action_value, arr = np.arange(max_number_of_actions))\n",
    "            \n",
    "            next_action_index = np.random.choice(a = max_action_stack)\n",
    "\n",
    "            # Calculate state-action-function using quintuple SARSA\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward + discounting_factor_gamma * state_action_value_function[next_state_index, next_action_index] - state_action_value_function[state_index, action_index])\n",
    "            \n",
    "            # Update model from environment experience\n",
    "            model = update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model)\n",
    "            \n",
    "            # Use updated model to simulate experience in planning phase\n",
    "            state_action_value_function = model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, state_action_value_function)\n",
    "\n",
    "            # Update state and action to next state and action\n",
    "            state_index = next_state_index\n",
    "            action_index = next_action_index\n",
    "\n",
    "    return state_action_value_function, policy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates what state and actions the model has seen\n",
    "def update_model_seen_state_actions(state_index, action_index, model):\n",
    "    # Check to see if state has already been visited\n",
    "    if model.number_of_seen_non_terminal_states == 0 or (model.seen_non_terminal_states_stack_reverse_lookup[state_index] == 0 and model.seen_non_terminal_states_stack[0] != state_index): # if new state\n",
    "        # Add to state stack\n",
    "        model.seen_non_terminal_states_stack[model.number_of_seen_non_terminal_states] = state_index # 1, 3, 2, 0, 4\n",
    "        model.seen_non_terminal_states_stack_reverse_lookup[state_index] = model.number_of_seen_non_terminal_states # 3, 0, 2, 1, 4\n",
    "\n",
    "        # Add to action stack\n",
    "        model.seen_non_terminal_states_actions_stack[state_index][model.number_of_seen_non_terminal_states_actions[state_index]] = action_index # 2, 0, 3, 1\n",
    "        model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] = model.number_of_seen_non_terminal_states_actions[state_index] # 1, 3, 0, 2\n",
    "\n",
    "        # Increment counters\n",
    "        model.number_of_seen_non_terminal_states_actions[state_index] += 1\n",
    "        model.number_of_seen_non_terminal_states += 1\n",
    "    else: # if already visited state\n",
    "        # Check to see if action has already been visited\n",
    "        if model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] == 0 and model.seen_non_terminal_states_actions_stack[state_index][0] != action_index:\n",
    "            # Add to action stack\n",
    "            model.seen_non_terminal_states_actions_stack[state_index][model.number_of_seen_non_terminal_states_actions[state_index]] = action_index # 2, 0, 3, 1\n",
    "            model.seen_non_terminal_states_actions_stack_reverse_lookup[state_index][action_index] = model.number_of_seen_non_terminal_states_actions[state_index] # 1, 3, 0, 2\n",
    "\n",
    "            # Increment counters\n",
    "            model.number_of_seen_non_terminal_states_actions[state_index] += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the model from environment experience\n",
    "def update_model_of_environment_from_experience(state_index, action_index, reward, next_state_index, model):\n",
    "    if next_state_index in model.state_action_successor_state_indices[state_index, action_index]:\n",
    "        model.successor_index = model.state_action_successor_state_indices[state_index, action_index].index(next_state_index)\n",
    "        model.state_action_successor_state_number_of_visits[state_index, action_index][model.successor_index] += 1\n",
    "    else:\n",
    "        model.number_of_state_action_successor_states[state_index, action_index] += 1\n",
    "        model.state_action_successor_state_indices[state_index, action_index].append(next_state_index)\n",
    "        model.state_action_successor_state_rewards[state_index, action_index].append(reward)\n",
    "        model.state_action_successor_state_number_of_visits[state_index, action_index].append(1)\n",
    "\n",
    "    model.state_action_successor_state_number_of_visits_sum = np.sum(a = np.asarray(a = model.state_action_successor_state_number_of_visits[state_index, action_index]))\n",
    "    model.state_action_successor_state_transition_probabilities[state_index, action_index] = [float(model.state_action_successor_state_number_of_visits[state_index, action_index][successor_index]) / model.state_action_successor_state_number_of_visits_sum for successor_index in range(0, model.number_of_state_action_successor_states[state_index, action_index])]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simualate_planning(number_of_planning_steps, number_of_non_terminal_states, max_number_of_actions, model, alpha, discounting_factor_gamma, state_action_value_function):\n",
    "    for i in range(0, number_of_planning_steps):\n",
    "        # Randomly choose state indices from previously seen states\n",
    "        state_index = model.seen_non_terminal_states_stack[np.random.randint(low = 0, high = model.number_of_seen_non_terminal_states, dtype = np.int64)]\n",
    "        \n",
    "        # Randomly choose action indices from previously seen actions in previously seen states\n",
    "        action_index = model.seen_non_terminal_states_actions_stack[state_index, np.random.randint(low = 0, high = model.number_of_seen_non_terminal_states_actions[state_index], dtype = np.int64)]\n",
    "        \n",
    "        # Get reward\n",
    "        successor_state_transition_index = np.random.choice(a = np.arange(model.number_of_state_action_successor_states[state_index, action_index]), p = np.asarray(a = model.state_action_successor_state_transition_probabilities[state_index, action_index], dtype = np.float64))\n",
    "    \n",
    "        # Get reward from state and action */\n",
    "        reward = model.state_action_successor_state_rewards[state_index, action_index][successor_state_transition_index]\n",
    "\n",
    "        # Get next state */\n",
    "        next_state_index = model.state_action_successor_state_indices[state_index, action_index][successor_state_transition_index]\n",
    "\n",
    "        # Check to see if we actioned into a terminal state */\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward - state_action_value_function[state_index, action_index])\n",
    "        else:\n",
    "            # Get next action, max action of next state\n",
    "            max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "            max_action_stack = np.extract(condition = state_action_value_function[state_index, :] == max_action_value, arr = np.arange(max_number_of_actions))\n",
    "            \n",
    "            next_action_index = np.random.choice(a = max_action_stack)\n",
    "\n",
    "            # Calculate state-action-function using quintuple SARSargmax(a,Q) */\n",
    "            state_action_value_function[state_index, action_index] += alpha * (reward + discounting_factor_gamma * state_action_value_function[next_state_index, next_action_index] - state_action_value_function[state_index, action_index]);\n",
    "            \n",
    "    return state_action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_planning_and_learning_tabular_dyna_q(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length, number_of_planning_steps):\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        # Initialize episode to get initial state\n",
    "        initial_state_index = initialize_epsiode(number_of_non_terminal_states)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        state_action_value_function, policy, model = loop_through_episode(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length, number_of_planning_steps, initial_state_index)\n",
    "    \n",
    "    return state_action_value_function, policy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-2.99735237 -1.99925958 -0.9999999  -3.57418531]\n",
      " [-8.44248666 -2.9973631  -1.99995784 -8.35601611]\n",
      " [-3.99217238 -3.9916204  -8.41330405 -2.99978016]\n",
      " [-3.57407501 -0.99999999 -1.99918769 -2.99769535]\n",
      " [-8.4001543  -2.59300282 -2.5932972  -7.70072548]\n",
      " [-3.6297768  -8.43155974 -8.19430757 -3.62943777]\n",
      " [-2.99806178 -3.99270458 -4.60211943 -1.99999887]\n",
      " [-8.29448783 -1.99999582 -2.99787481 -3.99228778]\n",
      " [-3.62975922 -8.21771992 -6.49049505 -3.62958679]\n",
      " [-2.63357078 -4.601051   -4.60164717 -2.63365203]\n",
      " [-1.99928383 -2.99791987 -3.62632805 -1.        ]\n",
      " [-5.51422466 -2.99972733 -3.99158593 -3.99129679]\n",
      " [-1.99998794 -4.59929297 -6.33077103 -2.99782855]\n",
      " [-0.99999999 -3.62573684 -2.99742388 -1.99912683]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Final policy\n",
      "[[0.01666667 0.01666667 0.95       0.01666667]\n",
      " [0.01666667 0.01666667 0.95       0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.95       0.01666667 0.01666667]\n",
      " [0.01666667 0.95       0.01666667 0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.95       0.01666667 0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]\n",
      " [0.01666667 0.01666667 0.01666667 0.95      ]\n",
      " [0.01666667 0.95       0.01666667 0.01666667]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]\n",
      " [0.95       0.01666667 0.01666667 0.01666667]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run off policy planning and learning tabular dyna-Q\n",
    "state_action_value_function, policy, model = off_policy_planning_and_learning_tabular_dyna_q(number_of_non_terminal_states, max_number_of_actions, environment, model, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length, number_of_planning_steps)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.number_of_seen_non_terminal_states\n",
      "14\n",
      "model.seen_non_terminal_states_stack\n",
      "[12 11  7  8 13  9  5  4  1  2  6 10  0  3]\n",
      "model.seen_non_terminal_states_stack_reverse_lookup\n",
      "[12  8  9 13  7  6 10  2  3  5 11  1  0  4]\n",
      "model.number_of_seen_non_terminal_states_actions\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "model.seen_non_terminal_states_actions_stack\n",
      "[[1 2 3 0]\n",
      " [0 3 2 1]\n",
      " [1 2 3 0]\n",
      " [1 3 0 2]\n",
      " [3 0 1 2]\n",
      " [2 1 0 3]\n",
      " [0 1 3 2]\n",
      " [3 0 2 1]\n",
      " [2 0 3 1]\n",
      " [1 2 3 0]\n",
      " [3 0 1 2]\n",
      " [3 2 1 0]\n",
      " [2 3 1 0]\n",
      " [2 1 3 0]]\n",
      "model.seen_non_terminal_states_actions_stack_reverse_lookup\n",
      "[[3 0 1 2]\n",
      " [0 3 2 1]\n",
      " [3 0 1 2]\n",
      " [2 0 3 1]\n",
      " [1 2 3 0]\n",
      " [2 1 0 3]\n",
      " [0 1 3 2]\n",
      " [1 3 2 0]\n",
      " [1 3 0 2]\n",
      " [3 0 1 2]\n",
      " [1 2 3 0]\n",
      " [3 2 1 0]\n",
      " [3 2 0 1]\n",
      " [3 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Print model seen arrays\n",
    "print(\"model.number_of_seen_non_terminal_states\")\n",
    "print(model.number_of_seen_non_terminal_states)\n",
    "print(\"model.seen_non_terminal_states_stack\")\n",
    "print(model.seen_non_terminal_states_stack)\n",
    "print(\"model.seen_non_terminal_states_stack_reverse_lookup\")\n",
    "print(model.seen_non_terminal_states_stack_reverse_lookup)\n",
    "print(\"model.number_of_seen_non_terminal_states_actions\")\n",
    "print(model.number_of_seen_non_terminal_states_actions)\n",
    "print(\"model.seen_non_terminal_states_actions_stack\")\n",
    "print(model.seen_non_terminal_states_actions_stack)\n",
    "print(\"model.seen_non_terminal_states_actions_stack_reverse_lookup\")\n",
    "print(model.seen_non_terminal_states_actions_stack_reverse_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.number_of_state_action_successor_states\n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "model.state_action_successor_state_indices\n",
      "[[list([1]) list([0]) list([14]) list([4])]\n",
      " [list([2]) list([1]) list([0]) list([5])]\n",
      " [list([2]) list([2]) list([1]) list([6])]\n",
      " [list([4]) list([14]) list([3]) list([7])]\n",
      " [list([5]) list([0]) list([3]) list([8])]\n",
      " [list([6]) list([1]) list([4]) list([9])]\n",
      " [list([6]) list([2]) list([5]) list([10])]\n",
      " [list([8]) list([3]) list([7]) list([11])]\n",
      " [list([9]) list([4]) list([7]) list([12])]\n",
      " [list([10]) list([5]) list([8]) list([13])]\n",
      " [list([10]) list([6]) list([9]) list([15])]\n",
      " [list([12]) list([7]) list([11]) list([11])]\n",
      " [list([13]) list([8]) list([11]) list([12])]\n",
      " [list([15]) list([9]) list([12]) list([13])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_transition_probabilities\n",
      "[[list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([1.0]) list([1.0]) list([1.0]) list([1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_rewards\n",
      "[[list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([-1.0]) list([-1.0]) list([-1.0]) list([-1.0])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n",
      "model.state_action_successor_state_number_of_visits\n",
      "[[list([178]) list([203]) list([7766]) list([147])]\n",
      " [list([203]) list([276]) list([3393]) list([220])]\n",
      " [list([289]) list([333]) list([394]) list([2984])]\n",
      " [list([210]) list([9931]) list([233]) list([225])]\n",
      " [list([212]) list([1791]) list([1763]) list([161])]\n",
      " [list([1699]) list([334]) list([313]) list([1462])]\n",
      " [list([293]) list([256]) list([348]) list([7121])]\n",
      " [list([282]) list([5765]) list([306]) list([254])]\n",
      " [list([1402]) list([415]) list([303]) list([1662])]\n",
      " [list([2811]) list([213]) list([236]) list([3021])]\n",
      " [list([288]) list([215]) list([259]) list([12315])]\n",
      " [list([400]) list([2912]) list([320]) list([329])]\n",
      " [list([4600]) list([250]) list([200]) list([289])]\n",
      " [list([9988]) list([254]) list([202]) list([226])]\n",
      " [list([]) list([]) list([]) list([])]\n",
      " [list([]) list([]) list([]) list([])]]\n"
     ]
    }
   ],
   "source": [
    "# Print model successor arrays\n",
    "print(\"model.number_of_state_action_successor_states\")\n",
    "print(model.number_of_state_action_successor_states)\n",
    "print(\"model.state_action_successor_state_indices\")\n",
    "print(model.state_action_successor_state_indices)\n",
    "print(\"model.state_action_successor_state_transition_probabilities\")\n",
    "print(model.state_action_successor_state_transition_probabilities)\n",
    "print(\"model.state_action_successor_state_rewards\")\n",
    "print(model.state_action_successor_state_rewards)\n",
    "print(\"model.state_action_successor_state_number_of_visits\")\n",
    "print(model.state_action_successor_state_number_of_visits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
