{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.repeat(a = 1, repeats = number_of_states * max_number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.reshape(a = number_of_state_action_successor_states, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_transition_probabilities = np.repeat(a = 1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_rewards = np.repeat(a = -1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.reshape(a = state_action_successor_state_indices, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_transition_probabilities = np.reshape(a = state_action_successor_state_transition_probabilities, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_rewards = np.reshape(a = state_action_successor_state_rewards, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of episodes\n",
    "number_of_episodes = 10000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 200\n",
    "# Set the number of state-action-value functions\n",
    "number_of_state_action_value_functions = 3\n",
    "# Set learning rate alpha\n",
    "alpha = 0.1\n",
    "# Set epsilon for our epsilon level of exploration\n",
    "epsilon = 0.1\n",
    "# Set discounting factor gamma\n",
    "discounting_factor_gamma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_value_function = np.repeat(a = 0.0, repeats = number_of_state_action_value_functions * number_of_states * max_number_of_actions)\n",
    "state_action_value_function = np.reshape(a = state_action_value_function, newshape = (number_of_state_action_value_functions, number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(a = 1.0 / max_number_of_actions, repeats = number_of_non_terminal_states * max_number_of_actions)\n",
    "policy = np.reshape(a = policy, newshape = (number_of_non_terminal_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(number_of_non_terminal_states):\n",
    "    step_count = 0\n",
    "\n",
    "    # Initial state\n",
    "    initial_state_index = np.random.randint(low = 0, high = number_of_non_terminal_states, dtype = np.int64) # randomly choose an initial state from all non-terminal states\n",
    "\n",
    "    return initial_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy):\n",
    "    # Combine state-action value functions\n",
    "    state_action_value_function = np.sum(a = state_action_value_function[:, state_index, :], axis = 0)\n",
    "    \n",
    "    # Save max state-action value and find the number of actions that have the same max state-action value\n",
    "    max_action_value = np.max(a = state_action_value_function)\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function == max_action_value)\n",
    "\n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    if max_action_count == max_number_of_actions:\n",
    "        max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "        remaining_apportioned_probability_per_action = 0.0\n",
    "    else:\n",
    "        max_policy_apportioned_probability_per_action = (1.0 - epsilon) / max_action_count\n",
    "        remaining_apportioned_probability_per_action = epsilon / (max_number_of_actions - max_action_count)\n",
    "\n",
    "    policy[state_index, :] = np.where(state_action_value_function == max_action_value, max_policy_apportioned_probability_per_action, remaining_apportioned_probability_per_action)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, number_of_state_action_value_functions, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length, state_index):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "        policy = epsilon_greedy_policy_from_state_action_function(max_number_of_actions, state_action_value_function, epsilon, state_index, policy)\n",
    "\n",
    "        # Get epsilon-greedy action\n",
    "        action_index = np.random.choice(a = max_number_of_actions, p = policy[state_index, :])\n",
    "        \n",
    "        # Get reward\n",
    "        successor_state_transition_index = np.random.choice(a = number_of_state_action_successor_states[state_index, action_index], p = state_action_successor_state_transition_probabilities[state_index, action_index, :])\n",
    "\n",
    "        reward = state_action_successor_state_rewards[state_index, action_index, successor_state_transition_index]\n",
    "\n",
    "        # Get next state\n",
    "        next_state_index = state_action_successor_state_indices[state_index, action_index, successor_state_transition_index]\n",
    "\n",
    "        # Update state action value equally randomly selecting from the state-action-value functions\n",
    "        updating_state_action_value_function_index = np.random.randint(low = 0, high = number_of_state_action_value_functions, dtype = np.int64)\n",
    "        \n",
    "        state_action_value_function, policy, state_index = update_state_action_value_function(number_of_non_terminal_states, max_number_of_actions, policy, alpha, epsilon, discounting_factor_gamma, state_index, action_index, reward, next_state_index, updating_state_action_value_function_index, number_of_state_action_value_functions, state_action_value_function);\n",
    "\n",
    "        if next_state_index >= number_of_non_terminal_states:\n",
    "            break; # episode terminated since we ended up in a terminal state\n",
    "\n",
    "    return state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function updates the state-action-value function\n",
    "def update_state_action_value_function(number_of_non_terminal_states, max_number_of_actions, policy, alpha, epsilon, discounting_factor_gamma, state_index, action_index, reward, next_state_index, updating_state_action_value_function_index, number_of_state_action_value_functions, state_action_value_function):\n",
    "    # Check to see if we actioned into a terminal state\n",
    "    if next_state_index >= number_of_non_terminal_states:\n",
    "        state_action_value_function[updating_state_action_value_function_index, state_index, action_index] += alpha * (reward - state_action_value_function[updating_state_action_value_function_index, state_index, action_index])\n",
    "    else:\n",
    "        # Get next action, max action of next state\n",
    "        max_action_value = np.max(a = state_action_value_function[updating_state_action_value_function_index, state_index, :])\n",
    "        max_action_stack = np.extract(condition = state_action_value_function[updating_state_action_value_function_index, state_index, :] == max_action_value, arr = np.arange(max_number_of_actions))\n",
    "\n",
    "        next_action_index = np.random.choice(a = max_action_stack)\n",
    "\n",
    "        # Calculate state-action-function expectation\n",
    "        state_action_value_function_indices = np.arange(number_of_state_action_value_functions)\n",
    "        \n",
    "        not_updating_state_action_value_function_index = np.random.choice(a = np.extract(condition = state_action_value_function_indices != updating_state_action_value_function_index, arr = state_action_value_function_indices))\n",
    "        \n",
    "        state_action_value_function[updating_state_action_value_function_index, state_index, action_index] += alpha * (reward + discounting_factor_gamma * state_action_value_function[not_updating_state_action_value_function_index, next_state_index, next_action_index] - state_action_value_function[updating_state_action_value_function_index, state_index, action_index])\n",
    "\n",
    "        # Update state and action to next state and action\n",
    "        state_index = next_state_index\n",
    "        \n",
    "    return state_action_value_function, policy, state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_temporal_difference_n_tuple_q_learning(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, number_of_state_action_value_functions, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length):\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        # Initialize episode to get initial state\n",
    "        initial_state_index = initialize_epsiode(number_of_non_terminal_states)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        state_action_value_function, policy = loop_through_episode(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, number_of_state_action_value_functions, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length, initial_state_index)\n",
    "    \n",
    "    return state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[[-3.85926601 -1.97434338 -1.         -3.68091528]\n",
      "  [-4.31854122 -3.56918336 -2.00000003 -4.53398963]\n",
      "  [-4.46002641 -4.81412218 -3.87944707 -5.80142117]\n",
      "  [-4.599681   -1.         -1.96309243 -2.93408263]\n",
      "  [-4.44130879 -4.48730847 -3.27633618 -4.35708968]\n",
      "  [-3.92805994 -4.56674553 -4.92638826 -4.66132764]\n",
      "  [-2.99607317 -4.31992999 -5.05244941 -2.        ]\n",
      "  [-4.0196938  -2.         -2.94916141 -4.4938994 ]\n",
      "  [-4.64435367 -4.09664497 -3.88535982 -4.49491687]\n",
      "  [-2.65848813 -3.80405394 -4.10868499 -2.59679294]\n",
      "  [-1.97827973 -3.00853677 -4.64901428 -1.        ]\n",
      "  [-4.85310242 -3.00047795 -3.87423939 -4.24488599]\n",
      "  [-2.         -4.37675412 -4.08790564 -3.61349136]\n",
      "  [-1.         -4.67702774 -4.76585684 -1.94209087]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-3.81901233 -1.8746518  -1.         -4.34650746]\n",
      "  [-4.08409085 -4.00815888 -2.         -4.48029922]\n",
      "  [-4.11081296 -4.14875379 -4.40770927 -3.13360131]\n",
      "  [-4.99092891 -1.         -1.96953367 -2.95568322]\n",
      "  [-4.58672631 -2.56093757 -2.56032806 -4.40069202]\n",
      "  [-5.8339202  -4.13936427 -4.39848614 -4.7097195 ]\n",
      "  [-2.86089629 -4.84135841 -4.69783487 -2.        ]\n",
      "  [-4.43838045 -2.         -2.90183779 -3.78684423]\n",
      "  [-3.5331084  -4.85588672 -5.03269487 -3.66421697]\n",
      "  [-5.59167645 -4.28253479 -3.94098891 -5.5807523 ]\n",
      "  [-1.90155072 -3.010322   -3.37101347 -1.        ]\n",
      "  [-2.99999956 -5.11887221 -4.84011406 -4.90702307]\n",
      "  [-5.66080048 -4.3347787  -3.7624079  -3.93077545]\n",
      "  [-1.         -3.3916926  -2.79766338 -1.95013279]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-2.91524091 -1.9624335  -1.         -2.96934744]\n",
      "  [-3.55087899 -3.53374319 -3.73850386 -4.04819346]\n",
      "  [-4.54573988 -4.63599407 -4.90958964 -3.        ]\n",
      "  [-3.62928994 -1.         -1.92916461 -2.90909329]\n",
      "  [-4.23860224 -4.48018788 -3.42697395 -4.03587957]\n",
      "  [-3.90991997 -5.04225457 -4.78428932 -4.81498462]\n",
      "  [-2.98332669 -5.60176904 -5.02776937 -2.        ]\n",
      "  [-4.36614837 -2.         -2.90132169 -4.30049742]\n",
      "  [-4.75509815 -3.93156777 -3.91502923 -4.78336626]\n",
      "  [-2.72320509 -4.38744952 -4.27889027 -2.66678446]\n",
      "  [-1.97616192 -2.94832013 -4.72646879 -1.        ]\n",
      "  [-4.68280228 -3.00928145 -4.44973311 -4.78166048]\n",
      "  [-2.         -3.5653551  -4.16533304 -4.3107488 ]\n",
      "  [-1.         -4.3009123  -4.59099491 -1.8480722 ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "\n",
      "Final policy\n",
      "[[0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.03333333 0.03333333 0.9        0.03333333]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.03333333 0.03333333 0.9       ]\n",
      " [0.03333333 0.9        0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]\n",
      " [0.9        0.03333333 0.03333333 0.03333333]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run off policy temporal difference n-tuple q learning\n",
    "state_action_value_function, policy = off_policy_temporal_difference_n_tuple_q_learning(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, number_of_state_action_value_functions, state_action_value_function, policy, alpha, epsilon, discounting_factor_gamma, maximum_episode_length)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
