{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 16\n",
    "number_of_terminal_states = 2\n",
    "number_of_non_terminal_states = number_of_states - number_of_terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions_per_non_terminal_state = np.repeat(a = max_number_of_actions, repeats = number_of_non_terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.repeat(a = 1, repeats = number_of_states * max_number_of_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_state_action_successor_states = np.reshape(a = number_of_state_action_successor_states, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.array([1, 0, 14, 4, 2, 1, 0, 5, 2, 2, 1, 6, 4, 14, 3, 7, 5, 0, 3, 8, 6, 1, 4, 9, 6, 2, 5, 10, 8, 3, 7, 11, 9, 4, 7, 12, 10, 5, 8, 13, 10, 6, 9, 15, 12, 7, 11, 11, 13, 8, 11, 12, 15, 9, 12, 13], dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_transition_probabilities = np.repeat(a = 1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_rewards = np.repeat(a = -1.0, repeats = number_of_non_terminal_states * max_number_of_actions * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_successor_state_indices = np.reshape(a = state_action_successor_state_indices, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_transition_probabilities = np.reshape(a = state_action_successor_state_transition_probabilities, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))\n",
    "state_action_successor_state_rewards = np.reshape(a = state_action_successor_state_rewards, newshape = (number_of_non_terminal_states, max_number_of_actions, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the n steps\n",
    "n_steps = 4\n",
    "# Set the number of episodes\n",
    "number_of_episodes = 200000\n",
    "# Set the maximum episode length\n",
    "maximum_episode_length = 2000\n",
    "# Set learning rate alpha\n",
    "alpha = 0.05\n",
    "# Set discounting factor gamma\n",
    "discounting_factor_gamma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create value function and policy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epsiode log\n",
    "episode_log = {\"state_index\": np.repeat(a = -1, repeats = maximum_episode_length), \n",
    "               \"action_index\": np.repeat(a = -1, repeats = maximum_episode_length), \n",
    "               \"reward\": np.repeat(a = 0.0, repeats = maximum_episode_length)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_value_function = np.repeat(a = 0.0, repeats = number_of_states * max_number_of_actions)\n",
    "state_action_value_function = np.reshape(a = state_action_value_function, newshape = (number_of_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.repeat(a = 1.0 / max_number_of_actions, repeats = number_of_non_terminal_states * max_number_of_actions)\n",
    "policy = np.reshape(a = policy, newshape = (number_of_non_terminal_states, max_number_of_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed so that everything is reproducible\n",
    "np.random.seed(seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initializes episodes\n",
    "def initialize_epsiode(number_of_non_terminal_states, max_number_of_actions, maximum_episode_length, policy, episode_log):\n",
    "    # Initial state\n",
    "    episode_log[\"state_index\"][0] = np.random.randint(low = 0, high = number_of_non_terminal_states, dtype = np.int64) # randomly choose an initial state from all non-terminal states\n",
    "\n",
    "    # Get initial action\n",
    "    episode_log[\"action_index\"][0] = np.random.choice(a = max_number_of_actions, p = policy[episode_log[\"state_index\"][0], :])\n",
    "\n",
    "    return maximum_episode_length, episode_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function selects a policy greedily from the state-action-value function\n",
    "def greedy_policy_from_state_action_function(state_action_value_function, state_index, policy):\n",
    "    # Save max state-action value and find the number of actions that have the same max state-action value\n",
    "    max_action_value = np.max(a = state_action_value_function[state_index, :])\n",
    "    max_action_count = np.count_nonzero(a = state_action_value_function[state_index, :] == max_action_value)\n",
    "    \n",
    "    # Apportion policy probability across ties equally for state-action pairs that have the same value and zero otherwise\n",
    "    max_policy_apportioned_probability_per_action = 1.0 / max_action_count\n",
    "    policy[state_index, :] = np.where(state_action_value_function[state_index, :] == max_action_value, max_policy_apportioned_probability_per_action, 0.0)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loops through episodes and updates the policy\n",
    "def loop_through_episode(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, alpha, discounting_factor_gamma, maximum_episode_length, max_timestep, episode_log, n_steps):\n",
    "    # Loop through episode steps until termination\n",
    "    for t in range(0, maximum_episode_length):\n",
    "        # Spend a little memory to save computation time\n",
    "        t_mod_n_plus_1 = t % (n_steps + 1);\n",
    "        t_plus_1_mod_n_plus_1 = (t + 1) % (n_steps + 1);\n",
    "        \n",
    "        if t < max_timestep:\n",
    "            # Get reward\n",
    "            successor_state_transition_index = np.random.choice(a = number_of_state_action_successor_states[episode_log[\"state_index\"][t_mod_n_plus_1], episode_log[\"action_index\"][t_mod_n_plus_1]], p = state_action_successor_state_transition_probabilities[episode_log[\"state_index\"][t_mod_n_plus_1], episode_log[\"action_index\"][t_mod_n_plus_1], :])\n",
    "\n",
    "            episode_log[\"reward\"][t_plus_1_mod_n_plus_1] = state_action_successor_state_rewards[episode_log[\"state_index\"][t_mod_n_plus_1], episode_log[\"action_index\"][t_mod_n_plus_1], successor_state_transition_index]\n",
    "\n",
    "            # Get next state\n",
    "            episode_log[\"state_index\"][t_plus_1_mod_n_plus_1] = state_action_successor_state_indices[episode_log[\"state_index\"][t_mod_n_plus_1], episode_log[\"action_index\"][t_mod_n_plus_1], successor_state_transition_index]\n",
    "\n",
    "            # Check to see if we actioned into a terminal state\n",
    "            if episode_log[\"state_index\"][t_plus_1_mod_n_plus_1] >= number_of_non_terminal_states:\n",
    "                max_timestep = t + 1\n",
    "            else:\n",
    "                # Get next action\n",
    "                episode_log[\"action_index\"][t_plus_1_mod_n_plus_1] = np.random.randint(low = 0, high = max_number_of_actions, dtype = np.int64) # randomly choose next action from next state\n",
    "                \n",
    "        tau = t - n_steps + 1 # tau is the time whose estimate is being updated\n",
    "        \n",
    "        if tau >= 0:\n",
    "            # Calculate expected return\n",
    "            if t + 1 >= max_timestep:\n",
    "                # Calculate expected return\n",
    "                expected_return = episode_log[\"reward\"][max_timestep % (n_steps + 1)]\n",
    "            else:\n",
    "                # Calculate expected state value function from policy\n",
    "                state_value_function_expected_value_on_policy = np.sum(a = policy[episode_log[\"state_index\"][t_plus_1_mod_n_plus_1], :] * state_action_value_function[episode_log[\"state_index\"][t_plus_1_mod_n_plus_1], :])\n",
    "\n",
    "                # Calculate expected return\n",
    "                expected_return = episode_log[\"reward\"][t_plus_1_mod_n_plus_1] + discounting_factor_gamma * state_value_function_expected_value_on_policy\n",
    "\n",
    "            for k in range(min(t, max_timestep - 1), tau, -1):\n",
    "                # Spend a little memory to save computation time\n",
    "                k_mod_n_plus_1 = k % (n_steps + 1)\n",
    "                \n",
    "                # Calculate expected state value function from policy, however without including kth chosen action\n",
    "                not_action_taken_mask = np.arange(max_number_of_actions) != episode_log[\"action_index\"][k_mod_n_plus_1]\n",
    "                not_action_taken_policy = np.extract(condition = not_action_taken_mask, arr = policy[episode_log[\"state_index\"][k_mod_n_plus_1], :])\n",
    "                not_action_taken_state_action_value = np.extract(condition = not_action_taken_mask, arr = state_action_value_function[episode_log[\"state_index\"][k_mod_n_plus_1], :])\n",
    "                \n",
    "                state_value_function_expected_value_on_policy = np.sum(a = not_action_taken_policy * not_action_taken_state_action_value)\n",
    "\n",
    "            # Spend a little memory to save computation time\n",
    "            tau_mod_n_plus_1 = tau % (n_steps + 1)\n",
    "\n",
    "            # Calculate state-action-function at tau timestep\n",
    "            state_action_value_function[episode_log[\"state_index\"][tau_mod_n_plus_1], episode_log[\"action_index\"][tau_mod_n_plus_1]] += alpha * (expected_return - state_action_value_function[episode_log[\"state_index\"][tau_mod_n_plus_1], episode_log[\"action_index\"][tau_mod_n_plus_1]])\n",
    "\n",
    "            # Choose policy for chosen state by epsilon-greedy choosing from the state-action-value function\n",
    "            policy = greedy_policy_from_state_action_function(state_action_value_function, episode_log[\"state_index\"][tau_mod_n_plus_1], policy)\n",
    "\n",
    "        if tau == max_timestep - 1:\n",
    "            break # break episode step loop, move on to next episode\n",
    "\n",
    "    return state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_n_step_bootstrapping_tree_backup(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, alpha, discounting_factor_gamma, maximum_episode_length, episode_log, n_steps):\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        # Initialize episode to get initial state and action\n",
    "        max_timestep, episode_log = initialize_epsiode(number_of_non_terminal_states, max_number_of_actions, maximum_episode_length, policy, episode_log)\n",
    "\n",
    "        # Loop through episode and update the policy\n",
    "        state_action_value_function, policy = loop_through_episode(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, alpha, discounting_factor_gamma, maximum_episode_length, max_timestep, episode_log, n_steps)\n",
    "    \n",
    "    return state_action_value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial state-action value function\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Initial policy\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "\n",
      "Final state-action value function\n",
      "[[-2.99774574 -2.44179948 -1.         -2.48279976]\n",
      " [-3.13165697 -2.90680095 -2.31512945 -3.00587791]\n",
      " [-3.32875676 -3.37928397 -2.70049158 -3.15115737]\n",
      " [-2.9657913  -1.         -2.45394261 -3.18192262]\n",
      " [-3.11894451 -2.66316626 -2.32846724 -3.03710149]\n",
      " [-3.05239319 -3.07262312 -2.99328998 -2.73403838]\n",
      " [-3.06047893 -3.30568351 -2.98893872 -2.63668917]\n",
      " [-3.2535225  -2.47066298 -3.04265421 -3.41552079]\n",
      " [-2.76470967 -2.98041862 -3.2023205  -3.13542246]\n",
      " [-2.53601088 -2.9693113  -3.08764737 -2.45149598]\n",
      " [-2.65443997 -3.14413555 -2.74091821 -1.        ]\n",
      " [-3.15932584 -3.11367181 -3.50313248 -3.44873964]\n",
      " [-2.3699663  -3.33588312 -3.60160633 -3.10579696]\n",
      " [-1.         -2.90876368 -3.10223645 -2.44498004]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "\n",
      "Final policy\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Print initial arrays\n",
    "print(\"\\nInitial state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nInitial policy\")\n",
    "print(policy)\n",
    "\n",
    "# Run on policy n-step bootstrapping sarsa\n",
    "state_action_value_function, policy = off_policy_n_step_bootstrapping_tree_backup(number_of_non_terminal_states, max_number_of_actions, number_of_state_action_successor_states, state_action_successor_state_indices, state_action_successor_state_transition_probabilities, state_action_successor_state_rewards, state_action_value_function, policy, alpha, discounting_factor_gamma, maximum_episode_length, episode_log, n_steps)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal state-action value function\")\n",
    "print(state_action_value_function)\n",
    "\n",
    "print(\"\\nFinal policy\")\n",
    "print(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
