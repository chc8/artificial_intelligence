{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# PROJECT = \"PROJECT\" # REPLACE WITH YOUR PROJECT ID\n",
    "# BUCKET = \"BUCKET\" # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "# REGION = \"us-central1\" # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "PROJECT = \"qwiklabs-gcp-cbc8684b07fc2dbd\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"qwiklabs-gcp-cbc8684b07fc2dbd-bucket\" # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "REGION = \"us-east1\" # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# Import os environment variables\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] =  BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now write into a python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_anomaly_detection_module/trainer/model.py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set logging to be level of INFO\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Determine CSV and label columns\n",
    "number_of_tags = 5\n",
    "tag_columns = [\"tag_{0}\".format(tag) for tag in range(0, number_of_tags)]\n",
    "UNLABELED_CSV_COLUMNS = tag_columns\n",
    "\n",
    "LABEL_COLUMN = \"anomalous_sequence_flag\"\n",
    "LABELED_CSV_COLUMNS = UNLABELED_CSV_COLUMNS + [LABEL_COLUMN]\n",
    "\n",
    "# Set default values for each CSV column\n",
    "UNLABELED_DEFAULTS = [[\"\"] for _ in UNLABELED_CSV_COLUMNS]\n",
    "\n",
    "LABELED_DEFAULTS = UNLABELED_DEFAULTS + [[0.0]]\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column, seq_len):\n",
    "      def convert_sequences_from_strings_to_floats(features, column_list):\n",
    "        def split_and_convert_string(string_tensor):\n",
    "          # Split string tensor into a sparse tensor based on delimiter\n",
    "          split_string = tf.string_split(source = tf.expand_dims(\n",
    "            input = string_tensor, axis = 0), delimiter = \",\")\n",
    "\n",
    "          # Converts the values of the sparse tensor to floats\n",
    "          converted_tensor = tf.string_to_number(\n",
    "            string_tensor = split_string.values, \n",
    "            out_type = tf.float64)\n",
    "\n",
    "          # Create a new sparse tensor with the new converted values, \n",
    "          # because the original sparse tensor values are immutable\n",
    "          new_sparse_tensor = tf.SparseTensor(\n",
    "            indices = split_string.indices, \n",
    "            values = converted_tensor, \n",
    "            dense_shape = split_string.dense_shape)\n",
    "\n",
    "          # Create a dense tensor of the float values that were converted from text csv\n",
    "          dense_floats = tf.sparse_tensor_to_dense(\n",
    "            sp_input = new_sparse_tensor, default_value = 0.0)\n",
    "\n",
    "          dense_floats_vector = tf.squeeze(input = dense_floats, axis = 0)\n",
    "\n",
    "          return dense_floats_vector\n",
    "          \n",
    "        for column in column_list:\n",
    "          features[column] = split_and_convert_string(features[column])\n",
    "          features[column].set_shape([seq_len])\n",
    "\n",
    "        return features\n",
    "        \n",
    "      if mode == tf.estimator.ModeKeys.TRAIN or (mode == tf.estimator.ModeKeys.EVAL and params[\"evaluation_mode\"] != \"tune_anomaly_thresholds\"):\n",
    "        columns = tf.decode_csv(\n",
    "          records = value_column, \n",
    "          record_defaults = UNLABELED_DEFAULTS, \n",
    "          field_delim = \";\")\n",
    "        features = dict(zip(UNLABELED_CSV_COLUMNS, columns))\n",
    "        features = convert_sequences_from_strings_to_floats(\n",
    "          features, UNLABELED_CSV_COLUMNS)\n",
    "        return features\n",
    "      else:\n",
    "        columns = tf.decode_csv(\n",
    "          records = value_column, \n",
    "          record_defaults = LABELED_DEFAULTS, \n",
    "          field_delim = \";\")\n",
    "        features = dict(zip(LABELED_CSV_COLUMNS, columns))\n",
    "        labels = tf.cast(x = features.pop(LABEL_COLUMN), dtype = tf.float64)\n",
    "        features = convert_sequences_from_strings_to_floats(\n",
    "          features, LABELED_CSV_COLUMNS[0:-1])\n",
    "        return features, labels\n",
    "    \n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename = filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.TextLineDataset(filenames = file_list)  # Read text file\n",
    "\n",
    "    # Decode the CSV file into a features dictionary of tensors\n",
    "    dataset = dataset.map(map_func = lambda x: decode_csv(x, params[\"seq_len\"]))\n",
    "    \n",
    "    # Determine amount of times to repeat file based on if we are training or evaluating\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      num_epochs = None # indefinitely\n",
    "    else:\n",
    "      num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    # Repeat files num_epoch times\n",
    "    dataset = dataset.repeat(count = num_epochs)\n",
    "\n",
    "    # Group the data into batches\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    \n",
    "    # Determine if we should shuffle based on if we are training or evaluating\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "\n",
    "    # Create a iterator and then pull the next batch of features from the example queue\n",
    "    batched_dataset = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return batched_dataset\n",
    "  return _input_fn\n",
    "\n",
    "# This function updates the count of records used\n",
    "def update_count(count_a, count_b):\n",
    "  return count_a + count_b\n",
    "\n",
    "# This function updates the mahalanobis distance variables when number_of_rows equals 1\n",
    "def singleton_batch_cov_variable_updating(\n",
    "  inner_size, \n",
    "  X, \n",
    "  count_variable, \n",
    "  mean_variable, \n",
    "  cov_variable, \n",
    "  eps):\n",
    "  # This function updates the mean vector incrementally\n",
    "  def update_mean_incremental(count_a, mean_a, value_b):\n",
    "    mean_ab = (mean_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "           tf.squeeze(input = value_b, axis = 0)) / tf.cast(x = count_a + 1, dtype = tf.float64)\n",
    "    return mean_ab\n",
    "\n",
    "  # This function updates the covariance matrix incrementally\n",
    "  def update_cov_incremental(count_a, mean_a, cov_a, value_b, mean_ab, sample_cov):\n",
    "    if sample_cov == True:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a - 1, dtype = tf.float64) + \\\n",
    "            tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, transpose_a = True)) \\\n",
    "        / tf.cast(x = count_a, dtype = tf.float64)\n",
    "    else:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "            tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, transpose_a = True)) \\\n",
    "        / tf.cast(x = count_a + 1, dtype = tf.float64)\n",
    "    return cov_ab\n",
    "\n",
    "  # Calculate new combined mean to use for incremental covariance matrix calculation\n",
    "  mean_ab = update_mean_incremental(\n",
    "    count_a = count_variable, \n",
    "    mean_a = mean_variable, \n",
    "    value_b = X) # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "\n",
    "  # Update running variables from single example\n",
    "  count_tensor = update_count(\n",
    "    count_a = count_variable, \n",
    "    count_b = 1) # time_shape = (), features_shape = ()\n",
    "\n",
    "  mean_tensor = mean_ab # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "\n",
    "  if inner_size == 1:\n",
    "    cov_tensor = tf.zeros_like(\n",
    "      tensor = cov_variable, dtype = tf.float64)\n",
    "  else:\n",
    "    # time_shape = (num_features, num_features)\n",
    "    # features_shape = (sequence_length, sequence_length)\n",
    "    cov_tensor = update_cov_incremental(\n",
    "      count_a = count_variable, \n",
    "      mean_a = mean_variable, \n",
    "      cov_a = cov_variable, \n",
    "      value_b = X, \n",
    "      mean_ab = mean_ab, \n",
    "      sample_cov = True)\n",
    "\n",
    "  # Assign values to variables, use control dependencies around return to enforce the mahalanobis \n",
    "  # variables to be assigned, the control order matters, hence the separate contexts\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [tf.assign(\n",
    "      ref = cov_variable, \n",
    "      value = cov_tensor)]):\n",
    "    with tf.control_dependencies(\n",
    "      control_inputs = [tf.assign(\n",
    "        ref = mean_variable, \n",
    "        value = mean_tensor)]):\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [tf.assign(\n",
    "          ref = count_variable, \n",
    "          value = count_tensor)]):\n",
    "        return tf.identity(input = cov_variable), tf.identity(input = mean_variable), tf.identity(input = count_variable)\n",
    "\n",
    "# This function updates the mahalanobis distance variables when number_of_rows does NOT equal 1\n",
    "def non_singleton_batch_cov_variable_updating(\n",
    "  cur_batch_size, \n",
    "  inner_size, \n",
    "  X, \n",
    "  count_variable, \n",
    "  mean_variable, \n",
    "  cov_variable, \n",
    "  eps):\n",
    "  # This function updates the mean vector using a batch of data\n",
    "  def update_mean_batch(count_a, mean_a, count_b, mean_b):\n",
    "    mean_ab = (mean_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "               mean_b * tf.cast(x = count_b, dtype = tf.float64)) \\\n",
    "               / tf.cast(x = count_a + count_b, dtype = tf.float64)\n",
    "    return mean_ab\n",
    "\n",
    "  # This function updates the covariance matrix using a batch of data\n",
    "  def update_cov_batch(count_a, mean_a, cov_a, count_b, mean_b, cov_b, sample_cov):\n",
    "    mean_diff = tf.expand_dims(input = mean_a - mean_b, axis = 0)\n",
    "\n",
    "    if sample_cov == True:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a - 1, dtype = tf.float64) + \\\n",
    "                cov_b * tf.cast(x = count_b - 1, dtype = tf.float64) + \\\n",
    "                tf.matmul(a = mean_diff, b = mean_diff, transpose_a = True) * \\\n",
    "                tf.cast(x = count_a * count_b, dtype = tf.float64) \\\n",
    "                / tf.cast(x = count_a + count_b, dtype = tf.float64)) \\\n",
    "                / tf.cast(x = count_a + count_b - 1, dtype = tf.float64)\n",
    "    else:\n",
    "      cov_ab = (cov_a * tf.cast(x = count_a, dtype = tf.float64) + \\\n",
    "                cov_b * tf.cast(x = count_b, dtype = tf.float64) + \\\n",
    "                tf.matmul(a = mean_diff, b = mean_diff, transpose_a = True) * \\\n",
    "                tf.cast(x = count_a * count_b, dtype = tf.float64) \\\n",
    "                / tf.cast(x = count_a + count_b, dtype = tf.float64)) \\\n",
    "                / tf.cast(x = count_a + count_b, dtype = tf.float64)\n",
    "    return cov_ab          \n",
    "\n",
    "  # Find statistics of batch\n",
    "  number_of_rows = cur_batch_size * inner_size\n",
    "\n",
    "  # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "  X_mean = tf.reduce_mean(input_tensor = X, axis = 0)\n",
    "\n",
    "  # time_shape = (cur_batch_size * sequence_length, num_features)\n",
    "  # features_shape = (cur_batch_size * num_features, sequence_length)\n",
    "  X_centered = X - X_mean\n",
    "\n",
    "  if inner_size > 1:\n",
    "    # time_shape = (num_features, num_features)\n",
    "    # features_shape = (sequence_length, sequence_length)\n",
    "    X_cov = tf.matmul(\n",
    "      a = X_centered,\n",
    "      b = X_centered, \n",
    "      transpose_a = True) / tf.cast(x = number_of_rows - 1, dtype = tf.float64)\n",
    "\n",
    "  # Update running variables from batch statistics\n",
    "  count_tensor = update_count(\n",
    "    count_a = count_variable, \n",
    "    count_b = number_of_rows) # time_shape = (), features_shape = ()\n",
    "\n",
    "  mean_tensor = update_mean_batch(\n",
    "    count_a = count_variable, \n",
    "    mean_a = mean_variable, \n",
    "    count_b = number_of_rows, \n",
    "    mean_b = X_mean) # time_shape = (num_features,), features_shape = (sequence_length,)\n",
    "\n",
    "  if inner_size == 1:\n",
    "    cov_tensor = tf.zeros_like(\n",
    "      tensor = cov_variable, dtype = tf.float64)\n",
    "  else:\n",
    "    # time_shape = (num_features, num_features)\n",
    "    # features_shape = (sequence_length, sequence_length)\n",
    "    cov_tensor = update_cov_batch(\n",
    "      count_a = count_variable, \n",
    "      mean_a = mean_variable, \n",
    "      cov_a = cov_variable, \n",
    "      count_b = number_of_rows, \n",
    "      mean_b = X_mean, \n",
    "      cov_b = X_cov, \n",
    "      sample_cov = True)\n",
    "\n",
    "  # Assign values to variables, use control dependencies around return to enforce the mahalanobis \n",
    "  # variables to be assigned, the control order matters, hence the separate contexts\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [tf.assign(ref = cov_variable, value = cov_tensor)]):\n",
    "    with tf.control_dependencies(\n",
    "      control_inputs = [tf.assign(ref = mean_variable, value = mean_tensor)]):\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [tf.assign(ref = count_variable, value = count_tensor)]):\n",
    "        return tf.identity(input = cov_variable), tf.identity(input = mean_variable), tf.identity(input = count_variable)\n",
    "\n",
    "def mahalanobis_distance(error_vectors_reshaped, mean_vector, inv_covariance, final_shape):\n",
    "  # time_shape = (current_batch_size * seq_len, num_features)\n",
    "  # features_shape = (current_batch_size * num_features, seq_len)\n",
    "  error_vectors_reshaped_centered = error_vectors_reshaped - mean_vector\n",
    "\n",
    "  # time_shape = (num_features, current_batch_size * seq_len)\n",
    "  # features_shape = (seq_len, current_batch_size * num_features)\n",
    "  mahalanobis_right_product = tf.matmul(\n",
    "    a = inv_covariance,\n",
    "    b = error_vectors_reshaped_centered,\n",
    "    transpose_b = True)\n",
    "\n",
    "  # time_shape = (current_batch_size * seq_len, current_batch_size * seq_len)\n",
    "  # features_shape = (current_batch_size * num_features, current_batch_size * num_features)\n",
    "  mahalanobis_distance_vectorized = tf.matmul(\n",
    "    a = error_vectors_reshaped_centered,\n",
    "    b = mahalanobis_right_product)\n",
    "\n",
    "  # time_shape = (current_batch_size * seq_len,)\n",
    "  # features_shape = (current_batch_size * num_features,)\n",
    "  mahalanobis_distance_flat = tf.diag_part(input = mahalanobis_distance_vectorized)\n",
    "\n",
    "  # time_shape = (current_batch_size, seq_len)\n",
    "  # features_shape = (current_batch_size, num_features)\n",
    "  mahalanobis_distance_final_shaped = tf.reshape(\n",
    "    tensor = mahalanobis_distance_flat, \n",
    "    shape = [-1, final_shape])\n",
    "\n",
    "  # time_shape = (current_batch_size, seq_len)\n",
    "  # features_shape = (current_batch_size, num_features)\n",
    "  mahalanobis_distance_final_shaped_abs = tf.abs(x = mahalanobis_distance_final_shaped)\n",
    "\n",
    "  return mahalanobis_distance_final_shaped_abs\n",
    "\n",
    "def update_anomaly_threshold_variables(\n",
    "  labels_normal_mask, \n",
    "  labels_anomalous_mask, \n",
    "  num_thresholds, \n",
    "  anomaly_thresholds, \n",
    "  mahalanobis_distance, \n",
    "  tp_at_thresholds_variable, \n",
    "  fn_at_thresholds_variable, \n",
    "  fp_at_thresholds_variable, \n",
    "  tn_at_thresholds_variable,\n",
    "  mode):\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # time_shape = (num_time_anomaly_thresholds, current_batch_size, sequence_length)\n",
    "    # features_shape = (num_features_anomaly_thresholds, current_batch_size, number_of_features)\n",
    "    mahalanobis_distance_over_thresholds = tf.map_fn(\n",
    "      fn = lambda anomaly_threshold: mahalanobis_distance > anomaly_threshold, \n",
    "      elems = anomaly_thresholds, \n",
    "      dtype = tf.bool)\n",
    "  else:\n",
    "    # time_shape = (current_batch_size, sequence_length)\n",
    "    # features_shape = (current_batch_size, number_of_features)\n",
    "    mahalanobis_distance_over_thresholds = mahalanobis_distance > anomaly_thresholds\n",
    "\n",
    "  # time_shape = (num_time_anomaly_thresholds, current_batch_size)\n",
    "  # features_shape = (num_features_anomaly_thresholds, current_batch_size)    \n",
    "  mahalanobis_distance_any_over_thresholds = tf.reduce_any(\n",
    "    input_tensor = mahalanobis_distance_over_thresholds, \n",
    "    axis = -1)\n",
    "    \n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    # time_shape = (1, current_batch_size)\n",
    "    # features_shape = (1, current_batch_size)\n",
    "    mahalanobis_distance_any_over_thresholds = tf.expand_dims(\n",
    "      input = mahalanobis_distance_any_over_thresholds, axis = 0)\n",
    "\n",
    "  # time_shape = (num_time_anomaly_thresholds, current_batch_size)\n",
    "  # features_shape = (num_features_anomaly_thresholds, current_batch_size)\n",
    "  predicted_normals = tf.equal(\n",
    "    x = mahalanobis_distance_any_over_thresholds, \n",
    "    y = False)\n",
    "\n",
    "  # time_shape = (num_time_anomaly_thresholds, current_batch_size)\n",
    "  # features_shape = (num_features_anomaly_thresholds, current_batch_size)\n",
    "  predicted_anomalies = tf.equal(\n",
    "    x = mahalanobis_distance_any_over_thresholds, \n",
    "    y = True)\n",
    "  \n",
    "  # Calculate confusion matrix of current batch\n",
    "  # time_shape = (num_time_anomaly_thresholds,)\n",
    "  # features_shape = (num_features_anomaly_thresholds,)\n",
    "  tp = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_anomalous_mask, \n",
    "          y = predicted_anomalies[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "\n",
    "  fn = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_anomalous_mask, \n",
    "          y = predicted_normals[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "\n",
    "  fp = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_normal_mask, \n",
    "          y = predicted_anomalies[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "\n",
    "  tn = tf.reduce_sum(\n",
    "    input_tensor = tf.cast(\n",
    "      x = tf.map_fn(\n",
    "        fn = lambda threshold: tf.logical_and(\n",
    "          x = labels_normal_mask, \n",
    "          y = predicted_normals[threshold, :]), \n",
    "        elems = tf.range(start = 0, limit = num_thresholds, dtype = tf.int64), \n",
    "        dtype = tf.bool), \n",
    "      dtype = tf.int64), \n",
    "    axis = 1)\n",
    "  \n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    # shape = ()\n",
    "    tp = tf.squeeze(input = tp)\n",
    "    fn = tf.squeeze(input = fn)\n",
    "    fp = tf.squeeze(input = fp)\n",
    "    tn = tf.squeeze(input = tn)\n",
    "\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [tf.assign_add(ref = tp_at_thresholds_variable, value = tp), \n",
    "                      tf.assign_add(ref = fn_at_thresholds_variable, value = fn), \n",
    "                      tf.assign_add(ref = fp_at_thresholds_variable, value = fp), \n",
    "                      tf.assign_add(ref = tn_at_thresholds_variable, value = tn)]):\n",
    "    return tf.identity(input = tp_at_thresholds_variable), tf.identity(input = fn_at_thresholds_variable), tf.identity(input = fp_at_thresholds_variable), tf.identity(input = tn_at_thresholds_variable)\n",
    "\n",
    "def calculate_composite_classification_metrics(anomaly_thresholds, tp, fn, fp, tn, f_score_beta):\n",
    "  # time_shape = (num_time_anomaly_thresholds,)\n",
    "  # features_shape = (num_features_anomaly_thresholds,)\n",
    "  acc = tf.cast(x = tp + tn, dtype = tf.float64) \\\n",
    "    / tf.cast(x = tp + fn + fp + tn, dtype = tf.float64)\n",
    "  pre = tf.cast(x = tp, dtype = tf.float64) / tf.cast(x = tp + fp, dtype = tf.float64)\n",
    "  rec = tf.cast(x = tp, dtype = tf.float64) / tf.cast(x = tp + fn, dtype = tf.float64)\n",
    "  f_beta_score = (1.0 + f_score_beta ** 2) * (pre * rec) / (f_score_beta ** 2 * pre + rec)\n",
    "\n",
    "  return acc, pre, rec, f_beta_score\n",
    "\n",
    "def find_best_anomaly_threshold(\n",
    "  anomaly_thresholds, f_beta_score, user_passed_anomaly_threshold, anomaly_threshold_variable):\n",
    "  if user_passed_anomaly_threshold == None:\n",
    "    best_anomaly_threshold = tf.gather(\n",
    "      params = anomaly_thresholds, \n",
    "      indices = tf.argmax(input = f_beta_score, \n",
    "      axis = 0)) # shape = ()\n",
    "  else:\n",
    "    best_anomaly_threshold = user_passed_anomaly_threshold # shape = ()\n",
    "\n",
    "  with tf.control_dependencies(\n",
    "    control_inputs = [\n",
    "      tf.assign(ref = anomaly_threshold_variable, value = best_anomaly_threshold)]):\n",
    "    return tf.identity(input = anomaly_threshold_variable)\n",
    "\n",
    "# Create our model function to be used in our custom estimator\n",
    "def pca_anomaly_detection(features, labels, mode, params):\n",
    "  print(\"\\npca_anomaly_detection: features = \\n{}\".format(features))\n",
    "  print(\"pca_anomaly_detection: labels = \\n{}\".format(labels))\n",
    "  print(\"pca_anomaly_detection: mode = \\n{}\".format(mode))\n",
    "  print(\"pca_anomaly_detection: params = \\n{}\".format(params))\n",
    "\n",
    "  # 0. Get input sequence tensor into correct shape\n",
    "  # Get dynamic batch size in case there was a partially filled batch\n",
    "  cur_batch_size = tf.shape(\n",
    "    input = features[UNLABELED_CSV_COLUMNS[0]], out_type = tf.int64)[0]\n",
    "\n",
    "  # Get the number of features \n",
    "  num_features = len(UNLABELED_CSV_COLUMNS)\n",
    "\n",
    "  # Stack all of the features into a 3-D tensor\n",
    "  X = tf.stack(\n",
    "    values = [features[key] for key in UNLABELED_CSV_COLUMNS], \n",
    "    axis = 2) # shape = (cur_batch_size, seq_len, num_features)\n",
    "\n",
    "  # Reshape into a 2-D tensors\n",
    "  # Time based\n",
    "  # shape = (cur_batch_size * seq_len, num_features)\n",
    "  X_time = tf.reshape(\n",
    "    tensor = X, \n",
    "    shape = [cur_batch_size * params[\"seq_len\"], num_features])\n",
    "  \n",
    "  # Features based\n",
    "  # shape = (cur_batch_size, num_features, seq_len)\n",
    "  X_transposed = tf.transpose(a = X, perm = [0, 2, 1])\n",
    "  # shape = (cur_batch_size * num_features, seq_len)\n",
    "  X_features = tf.reshape(\n",
    "    tensor = X_transposed, \n",
    "    shape = [cur_batch_size * num_features, params[\"seq_len\"]])\n",
    "\n",
    "  ################################################################################\n",
    "  \n",
    "  # Variables for calculating error distribution statistics\n",
    "  with tf.variable_scope(name_or_scope = \"pca_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    pca_time_count_variable = tf.get_variable(\n",
    "      name = \"pca_time_count_variable\", # shape = ()\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_mean_variable = tf.get_variable(\n",
    "      name = \"pca_time_mean_variable\", # shape = (num_features,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features],  dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_cov_variable = tf.get_variable(\n",
    "      name = \"pca_time_cov_variable\", # shape = (num_features, num_features)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_eigenvalues_variable = tf.get_variable(\n",
    "      name = \"pca_time_eigenvalues_variable\", # shape = (num_features,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_time_eigenvectors_variable = tf.get_variable(\n",
    "      name = \"pca_time_eigenvectors_variable\", # shape = (num_features, num_features)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    # Features based\n",
    "    pca_features_count_variable = tf.get_variable(\n",
    "      name = \"pca_features_count_variable\", # shape = ()\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_mean_variable = tf.get_variable(\n",
    "      name = \"pca_features_mean_variable\", # shape = (seq_len,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_cov_variable = tf.get_variable(\n",
    "      name = \"pca_features_cov_variable\", # shape = (seq_len, seq_len)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_eigenvalues_variable = tf.get_variable(\n",
    "      name = \"pca_features_eigenvalues_variable\", # shape = (seq_len,)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "    pca_features_eigenvectors_variable = tf.get_variable(\n",
    "      name = \"pca_features_eigenvectors_variable\", # shape = (seq_len, seq_len)\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "  \n",
    "  # Variables for calculating error distribution statistics\n",
    "  with tf.variable_scope(\n",
    "    name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    abs_err_count_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_count_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    abs_err_mean_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_mean_time_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features], dtype = tf.float64),\n",
    "      trainable = False) # shape = (num_features,)\n",
    "\n",
    "    abs_err_cov_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_cov_time_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False) # shape = (num_features, num_features)\n",
    "\n",
    "    abs_err_inv_cov_time_variable = tf.get_variable(\n",
    "      name = \"abs_err_inv_cov_time_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [num_features, num_features], dtype = tf.float64),\n",
    "      trainable = False) # shape = (num_features, num_features)\n",
    "\n",
    "    # Features based\n",
    "    abs_err_count_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_count_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    abs_err_mean_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_mean_features_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False) # shape = (seq_len,)\n",
    "\n",
    "    abs_err_cov_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_cov_features_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False) # shape = (seq_len, seq_len)\n",
    "\n",
    "    abs_err_inv_cov_features_variable = tf.get_variable(\n",
    "      name = \"abs_err_inv_cov_features_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [params[\"seq_len\"], params[\"seq_len\"]], dtype = tf.float64),\n",
    "      trainable = False) # shape = (seq_len, seq_len)\n",
    "  \n",
    "  # Variables for automatically tuning anomaly thresholds\n",
    "  with tf.variable_scope(\n",
    "    name_or_scope = \"mahalanobis_distance_threshold_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    tp_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"tp_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    fn_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"fn_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    fp_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"fp_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    tn_at_thresholds_time_variable = tf.get_variable(\n",
    "      name = \"tn_at_thresholds_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_time_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_time_anomaly_thresholds,)\n",
    "\n",
    "    time_anomaly_threshold_variable = tf.get_variable(\n",
    "      name = \"time_anomaly_threshold_variable\",\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.float64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    # Features based\n",
    "    tp_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"tp_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    fn_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"fn_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    fp_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"fp_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    tn_at_thresholds_features_variable = tf.get_variable(\n",
    "      name = \"tn_at_thresholds_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [params[\"num_features_anomaly_thresholds\"]], dtype = tf.int64),\n",
    "      trainable = False) # shape = (num_features_anomaly_thresholds,)\n",
    "\n",
    "    features_anomaly_threshold_variable = tf.get_variable(\n",
    "      name = \"features_anomaly_threshold_variable\", # shape = ()\n",
    "      dtype = tf.float64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.float64),\n",
    "      trainable = False)\n",
    "\n",
    "  # Variables for automatically tuning anomaly thresholds\n",
    "  with tf.variable_scope(\n",
    "    name_or_scope = \"anomaly_threshold_eval_variables\", reuse = tf.AUTO_REUSE):\n",
    "    # Time based\n",
    "    tp_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"tp_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fn_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"fn_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fp_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"fp_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    tn_at_threshold_eval_time_variable = tf.get_variable(\n",
    "      name = \"tn_at_threshold_eval_time_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    # Features based\n",
    "    tp_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"tp_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fn_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"fn_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    fp_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"fp_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "    tn_at_threshold_eval_features_variable = tf.get_variable(\n",
    "      name = \"tn_at_threshold_eval_features_variable\",\n",
    "      dtype = tf.int64,\n",
    "      initializer = tf.zeros(shape = [], dtype = tf.int64),\n",
    "      trainable = False) # shape = ()\n",
    "\n",
    "  dummy_variable = tf.get_variable(\n",
    "    name = \"dummy_variable\",\n",
    "    dtype = tf.float64,\n",
    "    initializer = tf.zeros(shape = [], dtype = tf.float64),\n",
    "    trainable = True) # shape = ()\n",
    "  \n",
    "  # Now branch off based on which mode we are in\n",
    "  predictions_dict = None\n",
    "  loss = None\n",
    "  train_op = None\n",
    "  eval_metric_ops = None\n",
    "  export_outputs = None\n",
    "  \n",
    "  # 3. Loss function, training/eval ops\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    with tf.variable_scope(name_or_scope = \"pca_variables\", reuse = tf.AUTO_REUSE):\n",
    "      # Check if batch is a singleton or not, very important for covariance math\n",
    "\n",
    "      # Time based ########################################\n",
    "      singleton_condition = tf.equal(\n",
    "        x = cur_batch_size * params[\"seq_len\"], y = 1) # shape = ()\n",
    "\n",
    "      pca_time_cov_variable, pca_time_mean_variable, pca_time_count_variable = tf.cond(\n",
    "        pred = singleton_condition, \n",
    "        true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "          params[\"seq_len\"], \n",
    "          X_time, \n",
    "          pca_time_count_variable, \n",
    "          pca_time_mean_variable, \n",
    "          pca_time_cov_variable,\n",
    "          params[\"eps\"]), \n",
    "        false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "          cur_batch_size, \n",
    "          params[\"seq_len\"], \n",
    "          X_time, \n",
    "          pca_time_count_variable, \n",
    "          pca_time_mean_variable, \n",
    "          pca_time_cov_variable,\n",
    "          params[\"eps\"]))\n",
    "\n",
    "      pca_time_eigenvalues_tensor, pca_time_eigenvectors_tensor = tf.linalg.eigh(\n",
    "        tensor = pca_time_cov_variable) # shape = (num_features,) & (num_features, num_features)\n",
    "\n",
    "      # Features based ########################################\n",
    "      singleton_features_condition = tf.equal(\n",
    "        x = cur_batch_size * num_features, y = 1) # shape = ()\n",
    "\n",
    "      pca_features_cov_variable, pca_features_mean_variable, pca_features_count_variable = tf.cond(\n",
    "        pred = singleton_features_condition, \n",
    "        true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "          num_features, \n",
    "          X_features, \n",
    "          pca_features_count_variable, pca_features_mean_variable, \n",
    "          pca_features_cov_variable,\n",
    "          params[\"eps\"]), \n",
    "        false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "          cur_batch_size, \n",
    "          num_features, \n",
    "          X_features, \n",
    "          pca_features_count_variable, \n",
    "          pca_features_mean_variable, \n",
    "          pca_features_cov_variable,\n",
    "          params[\"eps\"]))\n",
    "\n",
    "      pca_features_eigenvalues_tensor, pca_features_eigenvectors_tensor = tf.linalg.eigh(\n",
    "        tensor = pca_features_cov_variable) # shape = (seq_len,) & (seq_len, seq_len)\n",
    "\n",
    "    # Lastly use control dependencies around loss to enforce the mahalanobis variables to be assigned, the control order matters, hence the separate contexts\n",
    "    with tf.control_dependencies(\n",
    "      control_inputs = [pca_time_cov_variable, pca_features_cov_variable]):\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [pca_time_mean_variable, pca_features_mean_variable]):\n",
    "        with tf.control_dependencies(\n",
    "          control_inputs = [pca_time_count_variable, pca_features_count_variable]):\n",
    "          with tf.control_dependencies(\n",
    "            control_inputs = [tf.assign(ref = pca_time_eigenvalues_variable, value = pca_time_eigenvalues_tensor), \n",
    "                              tf.assign(ref = pca_time_eigenvectors_variable, value = pca_time_eigenvectors_tensor),\n",
    "                              tf.assign(ref = pca_features_eigenvalues_variable, value = pca_features_eigenvalues_tensor), \n",
    "                              tf.assign(ref = pca_features_eigenvectors_variable, value = pca_features_eigenvectors_tensor)]):\n",
    "            loss = tf.reduce_sum(input_tensor = tf.zeros(shape = (), dtype = tf.float64) * dummy_variable)\n",
    "\n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss = loss,\n",
    "              global_step = tf.train.get_global_step(),\n",
    "              learning_rate = params[\"learning_rate\"],\n",
    "              optimizer = \"SGD\")\n",
    "  else:\n",
    "    # Time based\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time_centered = X_time - pca_time_mean_variable\n",
    "    # shape = (cur_batch_size * seq_len, params[\"k_principal_components\"])\n",
    "    X_time_projected = tf.matmul(\n",
    "      a = X_time_centered, \n",
    "      b = pca_time_eigenvectors_variable[:, -params[\"k_principal_components\"]:])\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time_reconstructed = tf.matmul(\n",
    "      a = X_time_projected, \n",
    "      b = pca_time_eigenvectors_variable[:, -params[\"k_principal_components\"]:], \n",
    "      transpose_b = True)\n",
    "    # shape = (cur_batch_size * seq_len, num_features)\n",
    "    X_time_abs_reconstruction_error = tf.abs(\n",
    "      x = X_time_centered - X_time_reconstructed)\n",
    "\n",
    "    # Features based\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_centered = X_features - pca_features_mean_variable\n",
    "    # shape = (cur_batch_size * num_features, params[\"k_principal_components\"])\n",
    "    X_features_projected = tf.matmul(\n",
    "      a = X_features_centered, \n",
    "      b = pca_features_eigenvectors_variable[:, -params[\"k_principal_components\"]:])\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_reconstructed = tf.matmul(\n",
    "      a = X_features_projected, \n",
    "      b = pca_features_eigenvectors_variable[:, -params[\"k_principal_components\"]:], \n",
    "      transpose_b = True)\n",
    "    # shape = (cur_batch_size * num_features, seq_len)\n",
    "    X_features_abs_reconstruction_error = tf.abs(\n",
    "      x = X_features_centered - X_features_reconstructed)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN and params[\"evaluation_mode\"] == \"calculate_error_distribution_statistics\":\n",
    "      ################################################################################\n",
    "\n",
    "      with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        # Time based ########################################\n",
    "        singleton_time_condition = tf.equal(\n",
    "          x = cur_batch_size * params[\"seq_len\"], y = 1) # shape = ()\n",
    "        \n",
    "        cov_time_variable, mean_time_variable, count_time_variable = tf.cond(\n",
    "          pred = singleton_time_condition, \n",
    "          true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "            params[\"seq_len\"], \n",
    "            X_time_abs_reconstruction_error, \n",
    "            abs_err_count_time_variable, \n",
    "            abs_err_mean_time_variable, \n",
    "            abs_err_cov_time_variable,\n",
    "            params[\"eps\"]), \n",
    "          false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "            cur_batch_size, \n",
    "            params[\"seq_len\"], \n",
    "            X_time_abs_reconstruction_error, \n",
    "            abs_err_count_time_variable, \n",
    "            abs_err_mean_time_variable, \n",
    "            abs_err_cov_time_variable,\n",
    "            params[\"eps\"]))\n",
    "\n",
    "        # Features based ########################################\n",
    "        singleton_features_condition = tf.equal(\n",
    "          x = cur_batch_size * num_features, y = 1) # shape = ()\n",
    "        \n",
    "        cov_features_variable, mean_features_variable, count_features_variable = tf.cond(\n",
    "          pred = singleton_features_condition, \n",
    "          true_fn = lambda: singleton_batch_cov_variable_updating(\n",
    "            num_features, \n",
    "            X_features_abs_reconstruction_error, \n",
    "            abs_err_count_features_variable, \n",
    "            abs_err_mean_features_variable, \n",
    "            abs_err_cov_features_variable,\n",
    "            params[\"eps\"]), \n",
    "          false_fn = lambda: non_singleton_batch_cov_variable_updating(\n",
    "            cur_batch_size, \n",
    "            num_features, \n",
    "            X_features_abs_reconstruction_error, \n",
    "            abs_err_count_features_variable, \n",
    "            abs_err_mean_features_variable, \n",
    "            abs_err_cov_features_variable,\n",
    "            params[\"eps\"]))\n",
    "\n",
    "      # Lastly use control dependencies around loss to enforce the mahalanobis variables to be assigned, the control order matters, hence the separate contexts\n",
    "      with tf.control_dependencies(\n",
    "        control_inputs = [cov_time_variable, cov_features_variable]):\n",
    "        with tf.control_dependencies(\n",
    "          control_inputs = [mean_time_variable, mean_features_variable]):\n",
    "          with tf.control_dependencies(\n",
    "            control_inputs = [count_time_variable, count_features_variable]):\n",
    "            # Time based\n",
    "            # shape = (num_features, num_features)\n",
    "            abs_err_inv_cov_time_tensor = \\\n",
    "              tf.matrix_inverse(input = cov_time_variable + \\\n",
    "                tf.eye(num_rows = tf.shape(input = cov_time_variable)[0], \n",
    "                     dtype = tf.float64) * params[\"eps\"])\n",
    "            # Features based\n",
    "            # shape = (seq_len, seq_len)\n",
    "            abs_err_inv_cov_features_tensor = \\\n",
    "              tf.matrix_inverse(input = cov_features_variable + \\\n",
    "                tf.eye(num_rows = tf.shape(input = cov_features_variable)[0], \n",
    "                     dtype = tf.float64) * params[\"eps\"])\n",
    "            \n",
    "            with tf.control_dependencies(\n",
    "              control_inputs = [tf.assign(ref = abs_err_inv_cov_time_variable, value = abs_err_inv_cov_time_tensor), \n",
    "                                tf.assign(ref = abs_err_inv_cov_features_variable, value = abs_err_inv_cov_features_tensor)]):\n",
    "              loss = tf.reduce_sum(input_tensor = tf.zeros(shape = (), dtype = tf.float64) * dummy_variable)\n",
    "\n",
    "              train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss = loss,\n",
    "                global_step = tf.train.get_global_step(),\n",
    "                learning_rate = params[\"learning_rate\"],\n",
    "                optimizer = \"SGD\")\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL and params[\"evaluation_mode\"] != \"tune_anomaly_thresholds\":\n",
    "      # Reconstruction loss on evaluation set\n",
    "      loss = tf.losses.mean_squared_error(labels = X_time_centered, predictions = X_time_abs_reconstruction_error)\n",
    "\n",
    "      if params[\"evaluation_mode\"] == \"reconstruction\":\n",
    "        # Reconstruction eval metrics\n",
    "        eval_metric_ops = {\n",
    "          \"rmse\": tf.metrics.root_mean_squared_error(labels = X_time_centered, predictions = X_time_abs_reconstruction_error),\n",
    "          \"mae\": tf.metrics.mean_absolute_error(labels = X_time_centered, predictions = X_time_abs_reconstruction_error)\n",
    "        }\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT or ((mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL) and params[\"evaluation_mode\"] == \"tune_anomaly_thresholds\"):\n",
    "      with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        # Time based\n",
    "        mahalanobis_distance_time = mahalanobis_distance(\n",
    "          error_vectors_reshaped = X_time_abs_reconstruction_error,\n",
    "          mean_vector = abs_err_mean_time_variable, \n",
    "          inv_covariance = abs_err_inv_cov_time_variable, \n",
    "          final_shape = params[\"seq_len\"]) # shape = (cur_batch_size, seq_len)\n",
    "        \n",
    "        # Features based\n",
    "        mahalanobis_distance_features = mahalanobis_distance(\n",
    "          error_vectors_reshaped = X_features_abs_reconstruction_error,\n",
    "          mean_vector = abs_err_mean_features_variable, \n",
    "          inv_covariance = abs_err_inv_cov_features_variable,\n",
    "          final_shape = num_features) # shape = (cur_batch_size, num_features)\n",
    "\n",
    "      if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        labels_normal_mask = tf.equal(x = labels, y = 0)\n",
    "        labels_anomalous_mask = tf.equal(x = labels, y = 1)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "          with tf.variable_scope(\n",
    "            name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            # shape = (num_time_anomaly_thresholds,)\n",
    "            time_anomaly_thresholds = tf.linspace(\n",
    "              start = tf.constant(value = params[\"min_time_anomaly_threshold\"], dtype = tf.float64),\n",
    "              stop = tf.constant(value = params[\"max_time_anomaly_threshold\"], dtype = tf.float64), \n",
    "              num = params[\"num_time_anomaly_thresholds\"])\n",
    "\n",
    "            tp_time_update_op, fn_time_update_op, fp_time_update_op, tn_time_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                params[\"num_time_anomaly_thresholds\"], \n",
    "                time_anomaly_thresholds, \n",
    "                mahalanobis_distance_time, \n",
    "                tp_at_thresholds_time_variable, \n",
    "                fn_at_thresholds_time_variable, \n",
    "                fp_at_thresholds_time_variable, \n",
    "                tn_at_thresholds_time_variable,\n",
    "                mode)\n",
    "\n",
    "            # Features based\n",
    "            # shape = (num_features_anomaly_thresholds,)\n",
    "            features_anomaly_thresholds = tf.linspace(\n",
    "              start = tf.constant(value = params[\"min_features_anomaly_threshold\"], dtype = tf.float64),\n",
    "              stop = tf.constant(value = params[\"max_features_anomaly_threshold\"], dtype = tf.float64), \n",
    "              num = params[\"num_features_anomaly_thresholds\"])\n",
    "\n",
    "            tp_features_update_op, fn_features_update_op, fp_features_update_op, tn_features_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                params[\"num_features_anomaly_thresholds\"], \n",
    "                features_anomaly_thresholds, \n",
    "                mahalanobis_distance_features, \n",
    "                tp_at_thresholds_features_variable, \n",
    "                fn_at_thresholds_features_variable, \n",
    "                fp_at_thresholds_features_variable, \n",
    "                tn_at_thresholds_features_variable, \n",
    "                mode)\n",
    "\n",
    "          # Reconstruction loss on evaluation set\n",
    "          with tf.control_dependencies(\n",
    "            control_inputs = [\n",
    "              tp_time_update_op, \n",
    "              fn_time_update_op, \n",
    "              fp_time_update_op, \n",
    "              tn_time_update_op, \n",
    "              tp_features_update_op, \n",
    "              fn_features_update_op, \n",
    "              fp_features_update_op, \n",
    "              tn_features_update_op]):\n",
    "            # Time based\n",
    "            acc_time, pre_time, rec_time, f_beta_score_time = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                time_anomaly_thresholds, \n",
    "                tp_at_thresholds_time_variable, \n",
    "                fn_at_thresholds_time_variable, \n",
    "                fp_at_thresholds_time_variable, \n",
    "                tn_at_thresholds_time_variable,\n",
    "                params[\"f_score_beta\"])\n",
    "\n",
    "            # Features based\n",
    "            acc_features, pre_features, rec_features, f_beta_score_features = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                features_anomaly_thresholds, \n",
    "                tp_at_thresholds_features_variable, \n",
    "                fn_at_thresholds_features_variable, \n",
    "                fp_at_thresholds_features_variable, \n",
    "                tn_at_thresholds_features_variable,\n",
    "                params[\"f_score_beta\"])\n",
    "\n",
    "            with tf.control_dependencies(\n",
    "              control_inputs = [pre_time, pre_features]):\n",
    "              with tf.control_dependencies(\n",
    "                control_inputs = [rec_time, rec_features]):\n",
    "                with tf.control_dependencies(\n",
    "                  control_inputs = [f_beta_score_time, f_beta_score_features]):\n",
    "                  # Time based\n",
    "                  best_anomaly_threshold_time = find_best_anomaly_threshold(\n",
    "                    time_anomaly_thresholds, \n",
    "                    f_beta_score_time, \n",
    "                    params[\"time_anomaly_threshold\"], \n",
    "                    time_anomaly_threshold_variable)\n",
    "\n",
    "                  # Features based\n",
    "                  best_anomaly_threshold_features = find_best_anomaly_threshold(\n",
    "                    features_anomaly_thresholds, \n",
    "                    f_beta_score_features, \n",
    "                    params[\"features_anomaly_threshold\"], \n",
    "                    features_anomaly_threshold_variable)\n",
    "\n",
    "                  with tf.control_dependencies(\n",
    "                    control_inputs = [\n",
    "                      tf.assign(\n",
    "                        ref = time_anomaly_threshold_variable, \n",
    "                        value = best_anomaly_threshold_time), \n",
    "                      tf.assign(ref = \n",
    "                                features_anomaly_threshold_variable, \n",
    "                                value = best_anomaly_threshold_features)]):\n",
    "\n",
    "                    loss = tf.reduce_sum(\n",
    "                      input_tensor = tf.zeros(shape = (), dtype = tf.float64) * dummy_variable)\n",
    "\n",
    "                    train_op = tf.contrib.layers.optimize_loss(\n",
    "                      loss = loss,\n",
    "                      global_step = tf.train.get_global_step(),\n",
    "                      learning_rate = params[\"learning_rate\"],\n",
    "                      optimizer = \"SGD\")\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "          with tf.variable_scope(\n",
    "            name_or_scope = \"anomaly_threshold_eval_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            tp_time_update_op, fn_time_update_op, fp_time_update_op, tn_time_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                1,\n",
    "                time_anomaly_threshold_variable, \n",
    "                mahalanobis_distance_time, \n",
    "                tp_at_threshold_eval_time_variable, \n",
    "                fn_at_threshold_eval_time_variable, \n",
    "                fp_at_threshold_eval_time_variable, \n",
    "                tn_at_threshold_eval_time_variable,\n",
    "                mode)\n",
    "\n",
    "            # Features based\n",
    "            tp_features_update_op, fn_features_update_op, fp_features_update_op, tn_features_update_op = \\\n",
    "              update_anomaly_threshold_variables(\n",
    "                labels_normal_mask, \n",
    "                labels_anomalous_mask, \n",
    "                1,\n",
    "                features_anomaly_threshold_variable, \n",
    "                mahalanobis_distance_features, \n",
    "                tp_at_threshold_eval_features_variable, \n",
    "                fn_at_threshold_eval_features_variable, \n",
    "                fp_at_threshold_eval_features_variable, \n",
    "                tn_at_threshold_eval_features_variable,\n",
    "                mode)\n",
    "\n",
    "          with tf.variable_scope(\n",
    "            name_or_scope = \"anomaly_threshold_eval_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            acc_time_update_op, pre_time_update_op, rec_time_update_op, f_beta_score_time_update_op = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                time_anomaly_threshold_variable, \n",
    "                tp_at_threshold_eval_time_variable, \n",
    "                fn_at_threshold_eval_time_variable, \n",
    "                fp_at_threshold_eval_time_variable, \n",
    "                tn_at_threshold_eval_time_variable,\n",
    "                params[\"f_score_beta\"]) \n",
    "\n",
    "            # Features based\n",
    "            acc_features_update_op, pre_features_update_op, rec_features_update_op, f_beta_score_features_update_op = \\\n",
    "              calculate_composite_classification_metrics(\n",
    "                features_anomaly_threshold_variable, \n",
    "                tp_at_threshold_eval_features_variable, \n",
    "                fn_at_threshold_eval_features_variable, \n",
    "                fp_at_threshold_eval_features_variable, \n",
    "                tn_at_threshold_eval_features_variable,\n",
    "                params[\"f_score_beta\"]) \n",
    "\n",
    "          loss = tf.losses.mean_squared_error(labels = X_time_centered, predictions = X_time_reconstructed)\n",
    "\n",
    "          acc_at_threshold_eval_time_variable = (tp_at_threshold_eval_time_variable + tn_at_threshold_eval_time_variable) / (tp_at_threshold_eval_time_variable + fn_at_threshold_eval_time_variable + fp_at_threshold_eval_time_variable + tn_at_threshold_eval_time_variable)\n",
    "          pre_at_threshold_eval_time_variable = tp_at_threshold_eval_time_variable / (tp_at_threshold_eval_time_variable + fp_at_threshold_eval_time_variable)\n",
    "          rec_at_threshold_eval_time_variable = tp_at_threshold_eval_time_variable / (tp_at_threshold_eval_time_variable + fn_at_threshold_eval_time_variable)\n",
    "          f_beta_score_at_threshold_eval_time_variable = (1.0 + params[\"f_score_beta\"] ** 2) * pre_at_threshold_eval_time_variable * rec_at_threshold_eval_time_variable / (params[\"f_score_beta\"] ** 2 * pre_at_threshold_eval_time_variable + rec_at_threshold_eval_time_variable)\n",
    "\n",
    "          acc_at_threshold_eval_features_variable = (tp_at_threshold_eval_features_variable + tn_at_threshold_eval_features_variable) / (tp_at_threshold_eval_features_variable + fn_at_threshold_eval_features_variable + fp_at_threshold_eval_features_variable + tn_at_threshold_eval_features_variable)\n",
    "          pre_at_threshold_eval_features_variable = tp_at_threshold_eval_features_variable / (tp_at_threshold_eval_features_variable + fp_at_threshold_eval_features_variable)\n",
    "          rec_at_threshold_eval_features_variable = tp_at_threshold_eval_features_variable / (tp_at_threshold_eval_features_variable + fn_at_threshold_eval_features_variable)\n",
    "          f_beta_score_at_threshold_eval_features_variable = (1.0 + params[\"f_score_beta\"] ** 2) * pre_at_threshold_eval_features_variable * rec_at_threshold_eval_features_variable / (params[\"f_score_beta\"] ** 2 * pre_at_threshold_eval_features_variable + rec_at_threshold_eval_features_variable)\n",
    "\n",
    "          # Anomaly detection eval metrics\n",
    "          eval_metric_ops = {\n",
    "            # Time based\n",
    "            \"time_anomaly_tp\": (tp_at_threshold_eval_time_variable, tp_time_update_op),\n",
    "            \"time_anomaly_fn\": (fn_at_threshold_eval_time_variable, fn_time_update_op),\n",
    "            \"time_anomaly_fp\": (fp_at_threshold_eval_time_variable, fp_time_update_op),\n",
    "            \"time_anomaly_tn\": (tn_at_threshold_eval_time_variable, tn_time_update_op),\n",
    "\n",
    "            \"time_anomaly_acc\": (acc_at_threshold_eval_time_variable, acc_time_update_op),\n",
    "            \"time_anomaly_pre\": (pre_at_threshold_eval_time_variable, pre_time_update_op),\n",
    "            \"time_anomaly_rec\": (rec_at_threshold_eval_time_variable, rec_time_update_op),\n",
    "            \"time_anomaly_f_beta_score\": (f_beta_score_at_threshold_eval_time_variable, f_beta_score_time_update_op),\n",
    "\n",
    "             # Features based\n",
    "            \"features_anomaly_tp\": (tp_at_threshold_eval_features_variable, tp_features_update_op),\n",
    "            \"features_anomaly_fn\": (fn_at_threshold_eval_features_variable, fn_features_update_op),\n",
    "            \"features_anomaly_fp\": (fp_at_threshold_eval_features_variable, fp_features_update_op),\n",
    "            \"features_anomaly_tn\": (tn_at_threshold_eval_features_variable, tn_features_update_op),\n",
    "\n",
    "            \"features_anomaly_acc\": (acc_at_threshold_eval_features_variable, acc_features_update_op),\n",
    "            \"features_anomaly_pre\": (pre_at_threshold_eval_features_variable, pre_features_update_op),\n",
    "            \"features_anomaly_rec\": (rec_at_threshold_eval_features_variable, rec_features_update_op),\n",
    "            \"features_anomaly_f_beta_score\": (f_beta_score_at_threshold_eval_features_variable, f_beta_score_features_update_op)\n",
    "          }\n",
    "      else: # mode == tf.estimator.ModeKeys.PREDICT\n",
    "        # Flag predictions as either normal or anomalous\n",
    "        time_anomaly_flags = tf.where(\n",
    "          condition = tf.reduce_any(\n",
    "            input_tensor = tf.greater(\n",
    "              x = tf.abs(x = mahalanobis_distance_time),\n",
    "              y = time_anomaly_threshold_variable), \n",
    "            axis = 1), \n",
    "          x = tf.ones(shape = [cur_batch_size], dtype = tf.int64), \n",
    "          y = tf.zeros(shape = [cur_batch_size], dtype = tf.int64)) # shape = (cur_batch_size,)\n",
    "\n",
    "        features_anomaly_flags = tf.where(\n",
    "          condition = tf.reduce_any(\n",
    "            input_tensor = tf.greater(\n",
    "              x = tf.abs(x = mahalanobis_distance_features),\n",
    "              y = features_anomaly_threshold_variable), \n",
    "            axis = 1), \n",
    "          x = tf.ones(shape = [cur_batch_size], dtype = tf.int64), \n",
    "          y = tf.zeros(shape = [cur_batch_size], dtype = tf.int64)) # shape = (cur_batch_size,)\n",
    "\n",
    "        # Create predictions dictionary\n",
    "        predictions_dict = {\n",
    "          \"X_time_abs_reconstruction_error\": tf.reshape(\n",
    "            tensor = X_time_abs_reconstruction_error, \n",
    "            shape = [cur_batch_size, params[\"seq_len\"], num_features]), \n",
    "          \"X_features_abs_reconstruction_error\": tf.transpose(\n",
    "            a = tf.reshape(\n",
    "              tensor = X_features_abs_reconstruction_error, \n",
    "              shape = [cur_batch_size, num_features, params[\"seq_len\"]]), \n",
    "            perm = [0, 2, 1]),\n",
    "          \"mahalanobis_distance_time\": mahalanobis_distance_time, \n",
    "          \"mahalanobis_distance_features\": mahalanobis_distance_features, \n",
    "          \"time_anomaly_flags\": time_anomaly_flags, \n",
    "          \"features_anomaly_flags\": features_anomaly_flags}\n",
    "\n",
    "        # Create export outputs\n",
    "        export_outputs = {\n",
    "          \"predict_export_outputs\": tf.estimator.export.PredictOutput(\n",
    "            outputs = predictions_dict)}\n",
    "\n",
    "  # Return EstimatorSpec\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode = mode,\n",
    "    predictions = predictions_dict,\n",
    "    loss = loss,\n",
    "    train_op = train_op,\n",
    "    eval_metric_ops = eval_metric_ops,\n",
    "    export_outputs = export_outputs)\n",
    "\n",
    "# Create our serving input function to accept the data at serving and send it in the \n",
    "# right format to our custom estimator\n",
    "def serving_input_fn(seq_len):\n",
    "    # This function fixes the shape and type of our input strings\n",
    "    def fix_shape_and_type_for_serving(placeholder):\n",
    "        current_batch_size = tf.shape(input = placeholder, out_type = tf.int64)[0]\n",
    "        \n",
    "        # String split each string in batch and output values from the resulting SparseTensors\n",
    "        split_string = tf.stack(values = tf.map_fn( # shape = (batch_size, seq_len)\n",
    "            fn = lambda x: tf.string_split(source = [placeholder[x]], delimiter = ',').values, \n",
    "            elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "            dtype = tf.string), axis = 0)\n",
    "        \n",
    "        # Convert each string in the split tensor to float\n",
    "        # shape = (batch_size, seq_len)\n",
    "        feature_tensor = tf.string_to_number(string_tensor = split_string, out_type = tf.float64)\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    # This function fixes dynamic shape ambiguity of last dimension so that we will be able to \n",
    "    # use it in our DNN (since tf.layers.dense require the last dimension to be known)\n",
    "    def get_shape_and_set_modified_shape_2D(tensor, additional_dimension_sizes):\n",
    "        # Get static shape for tensor and convert it to list\n",
    "        shape = tensor.get_shape().as_list()\n",
    "        # Set outer shape to additional_dimension_sizes[0] since know this is the correct size\n",
    "        shape[1] = additional_dimension_sizes[0]\n",
    "        # Set the shape of tensor to our modified shape\n",
    "        tensor.set_shape(shape = shape) # shape = (batch_size, additional_dimension_sizes[0])\n",
    "\n",
    "        return tensor\n",
    "            \n",
    "    # Create placeholders to accept the data sent to the model at serving time\n",
    "    # All features come in as a batch of strings, shape = (batch_size,), \n",
    "    # this was so because of passing the arrays to online ml-engine prediction\n",
    "    feature_placeholders = {\n",
    "        feature: tf.placeholder(\n",
    "          dtype = tf.string, shape = [None]) for feature in UNLABELED_CSV_COLUMNS\n",
    "    }\n",
    "    \n",
    "    # Create feature tensors\n",
    "    features = {key: fix_shape_and_type_for_serving(placeholder = tensor) \n",
    "      for key, tensor in feature_placeholders.items()}\n",
    "    \n",
    "    # Fix dynamic shape ambiguity of feature tensors for our DNN\n",
    "    features = {key: get_shape_and_set_modified_shape_2D(\n",
    "      tensor = tensor, additional_dimension_sizes = [seq_len]) for key, tensor in features.items()}\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "      features = features, receiver_tensors = feature_placeholders)\n",
    "\n",
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(args):\n",
    "  # Create our custom estimator using our model function\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn = pca_anomaly_detection,\n",
    "    model_dir = args[\"output_dir\"],\n",
    "    params = {\n",
    "      \"seq_len\": args[\"seq_len\"],\n",
    "      \"learning_rate\": args[\"learning_rate\"],\n",
    "      \"evaluation_mode\": args[\"evaluation_mode\"],\n",
    "      \"k_principal_components\": args[\"k_principal_components\"],\n",
    "      \"num_time_anomaly_thresholds\": args[\"num_time_anomaly_thresholds\"],\n",
    "      \"num_features_anomaly_thresholds\": args[\"num_features_anomaly_thresholds\"],\n",
    "      \"min_time_anomaly_threshold\": args[\"min_time_anomaly_threshold\"],\n",
    "      \"max_time_anomaly_threshold\": args[\"max_time_anomaly_threshold\"],\n",
    "      \"min_features_anomaly_threshold\": args[\"min_features_anomaly_threshold\"],\n",
    "      \"max_features_anomaly_threshold\": args[\"max_features_anomaly_threshold\"],\n",
    "      \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "      \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"],\n",
    "      \"eps\": args[\"eps\"],\n",
    "      \"f_score_beta\": args[\"f_score_beta\"]})\n",
    "  \n",
    "  if args[\"evaluation_mode\"] == \"reconstruction\":\n",
    "    # Create train spec to read in our training data\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "      input_fn = read_dataset(\n",
    "        filename = args[\"train_file_pattern\"],\n",
    "        mode = tf.estimator.ModeKeys.TRAIN, \n",
    "        batch_size = args[\"train_batch_size\"],\n",
    "        params = args),\n",
    "      max_steps = args[\"train_steps\"]), \n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "      input_fn = read_dataset(\n",
    "        filename = args[\"eval_file_pattern\"], \n",
    "        mode = tf.estimator.ModeKeys.EVAL, \n",
    "        batch_size = args[\"eval_batch_size\"],\n",
    "        params = args),\n",
    "      steps = None,\n",
    "      start_delay_secs = args[\"start_delay_secs\"], # start evaluating after N seconds\n",
    "      throttle_secs = args[\"throttle_secs\"])  # evaluate every N seconds\n",
    "\n",
    "    # Create train and evaluate loop to train and evaluate our estimator\n",
    "    tf.estimator.train_and_evaluate(\n",
    "      estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
    "  else:\n",
    "    if args[\"evaluation_mode\"] == \"calculate_error_distribution_statistics\":\n",
    "      # Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "      train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"],\n",
    "          mode = tf.estimator.ModeKeys.EVAL, # only read through validation dataset once\n",
    "          batch_size = args[\"train_batch_size\"],\n",
    "          params = args),\n",
    "        max_steps = args[\"train_steps\"])\n",
    "\n",
    "      # Don't create exporter for serving yet since anomaly thresholds aren't trained yet\n",
    "      exporter = None\n",
    "    elif args[\"evaluation_mode\"] == \"tune_anomaly_thresholds\":\n",
    "      # Tune anomaly thresholds using valdiation_2 and validation_anomaly datasets\n",
    "      train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"],\n",
    "          mode = tf.estimator.ModeKeys.EVAL, # only read through validation dataset once\n",
    "          batch_size = args[\"train_batch_size\"],\n",
    "          params = args),\n",
    "        max_steps = args[\"train_steps\"])\n",
    "      \n",
    "      # Create exporter that uses serving_input_fn to create saved_model for serving\n",
    "      exporter = tf.estimator.LatestExporter(\n",
    "        name = \"exporter\", serving_input_receiver_fn = lambda: serving_input_fn(args[\"seq_len\"]))\n",
    "\n",
    "    # Create eval spec to read in our validation data and export our model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "      input_fn = read_dataset(\n",
    "        filename = args[\"eval_file_pattern\"], \n",
    "        mode = tf.estimator.ModeKeys.EVAL, \n",
    "        batch_size = args[\"eval_batch_size\"],\n",
    "        params = args),\n",
    "      steps = None,\n",
    "      exporters = exporter,\n",
    "      start_delay_secs = args[\"start_delay_secs\"], # start evaluating after N seconds\n",
    "      throttle_secs = args[\"throttle_secs\"])  # evaluate every N seconds\n",
    "    \n",
    "    # Create train and evaluate loop to train and evaluate our estimator\n",
    "    tf.estimator.train_and_evaluate(\n",
    "      estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pca_anomaly_detection_module/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # File arguments\n",
    "  parser.add_argument(\n",
    "    \"--train_file_pattern\",\n",
    "    help = \"GCS location to read training data\",\n",
    "    required = True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--eval_file_pattern\",\n",
    "    help = \"GCS location to read evaluation data\",\n",
    "    required = True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    help = \"GCS location to write checkpoints and export models\",\n",
    "    required = True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--job-dir\",\n",
    "    help = \"this model ignores this field, but it is required by gcloud\",\n",
    "    default = \"junk\"\n",
    "  )\n",
    "  \n",
    "  # Sequence shape hyperparameters\n",
    "  parser.add_argument(\n",
    "    \"--seq_len\",\n",
    "    help = \"Number of timesteps to include in each example\",\n",
    "    type = int,\n",
    "    default = 32\n",
    "  )\n",
    "  \n",
    "  # Anomaly detection\n",
    "  parser.add_argument(\n",
    "    \"--evaluation_mode\",\n",
    "    help = \"Which evaluation mode we are in (reconstruction, calculate_error_distribution_statistics, tune_anomaly_thresholds)\",\n",
    "    type = str,\n",
    "    default = \"reconstruction\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--k_principal_components\",\n",
    "    help = \"The top k principal components to keep after eigendecomposition\",\n",
    "    type = int,\n",
    "    default = 3\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--num_time_anomaly_thresholds\",\n",
    "    help = \"Number of anomaly thresholds to evaluate in the time dimension\",\n",
    "    type = int,\n",
    "    default = 120\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--num_features_anomaly_thresholds\",\n",
    "    help = \"Number of anomaly thresholds to evaluate in the features dimension\",\n",
    "    type = int,\n",
    "    default = 120\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--min_time_anomaly_threshold\",\n",
    "    help = \"The minimum anomaly threshold to evaluate in the time dimension\",\n",
    "    type = float,\n",
    "    default = 100.0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--max_time_anomaly_threshold\",\n",
    "    help = \"The maximum anomaly threshold to evaluate in the time dimension\",\n",
    "    type = float,\n",
    "    default = 2000.0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--min_features_anomaly_threshold\",\n",
    "    help = \"The minimum anomaly threshold to evaluate in the time dimension\",\n",
    "    type = float,\n",
    "    default = 100.0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--max_features_anomaly_threshold\",\n",
    "    help = \"The maximum anomaly threshold to evaluate in the time dimension\",\n",
    "    type = float,\n",
    "    default = 2000.0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--time_anomaly_threshold\",\n",
    "    help = \"The anomaly threshold in the time dimension\",\n",
    "    type = float,\n",
    "    default = None\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--features_anomaly_threshold\",\n",
    "    help = \"The anomaly threshold in the features dimension\",\n",
    "    type = float,\n",
    "    default = None\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--eps\",\n",
    "    help = \"The precision value to add to the covariance matrix before inversion to avoid being singular\",\n",
    "    type = str,\n",
    "    default = \"1e-12\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "    \"--f_score_beta\",\n",
    "    help = \"The value of beta of the f-beta score\",\n",
    "    type = float,\n",
    "    default = 0.05\n",
    "  )\n",
    "  \n",
    "  # Parse all arguments\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "\n",
    "  # Unused args provided by service\n",
    "  arguments.pop(\"job_dir\", None)\n",
    "  arguments.pop(\"job-dir\", None)\n",
    "  \n",
    "  # Fix eps argument\n",
    "  arguments[\"eps\"] = float(arguments[\"eps\"])\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  arguments[\"output_dir\"] = os.path.join(\n",
    "    arguments[\"output_dir\"],\n",
    "    json.loads(\n",
    "      os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "    ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "  )\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow==1.13.1\n",
    "numpy==1.15.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train reconstruction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf trained_model\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/pca_anomaly_detection_module\n",
    "python3 -m trainer.task \\\n",
    "    --train_file_pattern=\"data/training_normal_sequences.csv\" \\\n",
    "    --eval_file_pattern=\"data/validation_normal_1_sequences.csv\" \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --seq_len=30 \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2000 \\\n",
    "    --learning_rate=0.1 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120 \\\n",
    "    --evaluation_mode=\"reconstruction\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train error distribution statistics variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/pca_anomaly_detection_module\n",
    "python -m trainer.task \\\n",
    "    --train_file_pattern=\"data/validation_normal_1_sequences.csv\" \\\n",
    "    --eval_file_pattern=\"data/validation_normal_1_sequences.csv\" \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --seq_len=30 \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2200 \\\n",
    "    --evaluation_mode=\"calculate_error_distribution_statistics\" \\\n",
    "    --k_principal_components=3 \\\n",
    "    --eps=\"1e-12\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune anomaly thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/pca_anomaly_detection_module\n",
    "python -m trainer.task \\\n",
    "    --train_file_pattern=\"data/labeled_validation_mixed_sequences.csv\" \\\n",
    "    --eval_file_pattern=\"data/labeled_validation_mixed_sequences.csv\" \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --seq_len=30 \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2400 \\\n",
    "    --evaluation_mode=\"tune_anomaly_thresholds\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300 \\\n",
    "    --min_time_anomaly_threshold=1.0 \\\n",
    "    --max_time_anomaly_threshold=20.0 \\\n",
    "    --min_features_anomaly_threshold=20.0 \\\n",
    "    --max_features_anomaly_threshold=80.0 \\\n",
    "    --f_score_beta=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy data over to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r data/* gs://$BUCKET/pca_anomaly_detection/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train reconstruction variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://$BUCKET/pca_anomaly_detection/trained_model\n",
    "JOBNAME=job_pca_anomaly_detection_reconstruction_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$PWD/pca_anomaly_detection_module/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=1.13 \\\n",
    "    -- \\\n",
    "    --train_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/training_normal_sequences.csv \\\n",
    "    --eval_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/validation_normal_1_sequences.csv \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --seq_len=30 \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2000 \\\n",
    "    --learning_rate=0.1 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120 \\\n",
    "    --evaluation_mode=\"reconstruction\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning of reconstruction hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam_reconstruction.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: rmse\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 30\n",
    "        maxParallelTrials: 1\n",
    "        params:\n",
    "        - parameterName: train_batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE\n",
    "        - parameterName: learning_rate\n",
    "          type: DOUBLE\n",
    "          minValue: 0.001\n",
    "          maxValue: 0.1\n",
    "          scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://$BUCKET/pca_anomaly_detection/hyperparam_reconstruction\n",
    "JOBNAME=job_pca_anomaly_detection_hyperparam_reconstruction_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$PWD/pca_anomaly_detection_module/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --config=hyperparam_reconstruction.yaml \\\n",
    "    --runtime-version=1.13 \\\n",
    "    -- \\\n",
    "    --train_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/training_normal_sequences.csv \\\n",
    "    --eval_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/validation_normal_1_sequences.csv \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --seq_len=30 \\\n",
    "    --horizon=0 \\\n",
    "    --reverse_labels_sequence=True \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2000 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120 \\\n",
    "    --evaluation_mode=\"reconstruction\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train error distribution variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://$BUCKET/pca_anomaly_detection/trained_model\n",
    "JOBNAME=job_pca_anomaly_detection_calculate_error_distribution_statistics_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$PWD/pca_anomaly_detection_module/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=1.13 \\\n",
    "    -- \\\n",
    "    --train_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/validation_normal_1_sequences.csv \\\n",
    "    --eval_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/validation_normal_1_sequences.csv \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --seq_len=30 \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2200 \\\n",
    "    --evaluation_mode=\"calculate_error_distribution_statistics\" \\\n",
    "    --k_principal_components=3 \\\n",
    "    --eps=\"1e-12\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune anomaly thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://$BUCKET/pca_anomaly_detection/trained_model\n",
    "JOBNAME=job_pca_anomaly_detection_tune_anomaly_thresholds_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$PWD/pca_anomaly_detection_module/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=1.13 \\\n",
    "    -- \\\n",
    "    --train_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/labeled_validation_mixed_sequences.csv \\\n",
    "    --eval_file_pattern=gs://$BUCKET/pca_anomaly_detection/data/labeled_validation_mixed_sequences.csv \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --seq_len=30 \\\n",
    "    --train_batch_size=32 \\\n",
    "    --eval_batch_size=32 \\\n",
    "    --train_steps=2400 \\\n",
    "    --evaluation_mode=\"tune_anomaly_thresholds\" \\\n",
    "    --num_time_anomaly_thresholds=300 \\\n",
    "    --num_features_anomaly_thresholds=300 \\\n",
    "    --min_time_anomaly_threshold=2.0 \\\n",
    "    --max_time_anomaly_threshold=15.0 \\\n",
    "    --min_features_anomaly_threshold=20 \\\n",
    "    --max_features_anomaly_threshold=60 \\\n",
    "    --f_score_beta=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"pca_anomaly_detection\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://$BUCKET/pca_anomaly_detection/trained_model/export/exporter/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create $MODEL_NAME --regions $REGION\n",
    "gcloud ml-engine versions create $MODEL_VERSION --model $MODEL_NAME --origin $MODEL_LOCATION --runtime-version 1.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNLABELED_CSV_COLUMNS = [\"tag_{0}\".format(tag) for tag in range(0, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labeled_test_mixed_sequences_array = np.loadtxt(fname = \"data/labeled_test_mixed_sequences.csv\", dtype = str, delimiter = \";\")\n",
    "print(\"labeled_test_mixed_sequences_array.shape = {}\".format(labeled_test_mixed_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_instances = 10\n",
    "print(\"labels = {}\".format(labeled_test_mixed_sequences_array[0:number_of_prediction_instances, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local prediction from local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_sequences.json', 'w') as outfile:\n",
    "  test_data_normal_string_list = labeled_test_mixed_sequences_array.tolist()[0:number_of_prediction_instances]\n",
    "  json_string = \"\"\n",
    "  for example in test_data_normal_string_list:\n",
    "    json_string += \"{\" + ','.join([\"{0}: \\\"{1}\\\"\".format('\\\"' + UNLABELED_CSV_COLUMNS[i] + '\\\"', example[i]) \n",
    "                                   for i in range(len(UNLABELED_CSV_COLUMNS))]) + \"}\\n\"\n",
    "  json_string = json_string.replace(' ', '').replace(':', ': ').replace(',', ', ')\n",
    "  print(json_string)\n",
    "  outfile.write(\"%s\" % json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/trained_model/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "  --model-dir=${PWD}/trained_model/export/exporter/${model_dir} \\\n",
    "  --json-instances=./test_sequences.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud ML-Engine prediction from deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_normal_string_list = labeled_test_mixed_sequences_array.tolist()[0:number_of_prediction_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataframe to instances list to get sent to ML-Engine\n",
    "instances = [{UNLABELED_CSV_COLUMNS[i]: example[i]\n",
    "              for i in range(len(UNLABELED_CSV_COLUMNS))} \n",
    "             for example in labeled_test_mixed_sequences_array.tolist()[0:number_of_prediction_instances]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send instance dictionary to receive response from ML-Engine for online prediction\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials = credentials)\n",
    "\n",
    "request_data = {\"instances\": instances}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'lstm_autoencoder_anomaly_detection', 'v1')\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
