{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "1.15.4\n",
      "0.23.4\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "np.set_printoptions(threshold = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()\n",
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "BUCKET = 'qwiklabs-gcp-8923d4964bfbd247-bucket' # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "PROJECT = 'qwiklabs-gcp-8923d4964bfbd247' # REPLACE WITH YOUR PROJECT ID\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# Import os environment variables\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] =  BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_sequence_before_anomaly = 80.0\n",
    "percent_sequence_after_anomaly = 0.0\n",
    "\n",
    "def create_time_series_clean_parameters():\n",
    "    clean_frequency_noise_scale = 0.1\n",
    "    clean_frequence_noise_shift = 0.0\n",
    "\n",
    "    clean_amplitude_noise_scale = 1.0\n",
    "    clean_amplitude_noise_shift = 1.0\n",
    "\n",
    "    clean_noise_noise_scale = 0.2\n",
    "\n",
    "    clean_freq = (np.random.random() * clean_frequency_noise_scale) + clean_frequence_noise_shift\n",
    "    clean_ampl = np.random.random() * clean_amplitude_noise_scale + clean_amplitude_noise_shift\n",
    "\n",
    "    return {\"clean_freq\": clean_freq, \"clean_ampl\": clean_ampl, \"clean_noise_noise_scale\": clean_noise_noise_scale}\n",
    "  \n",
    "\n",
    "def create_time_series_clean(number_of_sequences, sequence_length, clean_freq, clean_ampl, clean_noise_noise_scale):\n",
    "    # Clean parameters\n",
    "    clean_noise = [np.random.random() * clean_noise_noise_scale for i in range(0, number_of_sequences * sequence_length)]\n",
    "\n",
    "    sequence = np.sin(np.arange(0, number_of_sequences * sequence_length) * clean_freq) * clean_ampl + clean_noise\n",
    "\n",
    "    sequence = sequence.reshape(number_of_sequences, sequence_length)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def create_time_series_with_anomaly(number_of_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, clean_freq, clean_ampl, clean_noise_noise_scale):\n",
    "    sequence_length_before_anomaly = int(sequence_length * percent_sequence_before_anomaly / 100.0)\n",
    "    sequence_length_after_anomaly = int(sequence_length * percent_sequence_after_anomaly / 100.0)\n",
    "    sequence_length_anomaly = sequence_length - sequence_length_before_anomaly - sequence_length_after_anomaly\n",
    "\n",
    "    # Clean parameters\n",
    "    clean_noise_before = [np.random.random() * clean_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_before_anomaly)]\n",
    "    clean_noise_after = [np.random.random() * clean_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_after_anomaly)]\n",
    "\n",
    "    # Anomalous parameters\n",
    "    anomalous_frequency_noise_scale = 2.0\n",
    "    anomalous_frequence_noise_shift = 1.0\n",
    "\n",
    "    anomalous_amplitude_noise_scale = 1.0\n",
    "    anomalous_amplitude_noise_shift = 1.0\n",
    "\n",
    "    anomalous_noise_noise_scale = 1.0\n",
    "\n",
    "    anomalous_freq = (np.random.random() * anomalous_frequency_noise_scale) + anomalous_frequence_noise_shift\n",
    "    anomalous_ampl = np.random.random() * anomalous_amplitude_noise_scale + anomalous_amplitude_noise_shift\n",
    "    anomalous_noise = [np.random.random() * anomalous_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_anomaly)]\n",
    "\n",
    "    sequence_before_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_before_anomaly) * clean_freq) * clean_ampl + clean_noise_before\n",
    "    sequence_before_anomaly = sequence_before_anomaly.reshape(number_of_sequences, sequence_length_before_anomaly)\n",
    "\n",
    "    sequence_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_anomaly) * anomalous_freq) * anomalous_ampl + anomalous_noise\n",
    "    sequence_anomaly = sequence_anomaly.reshape(number_of_sequences, sequence_length_anomaly)\n",
    "\n",
    "    sequence_after_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_after_anomaly) * clean_freq) * clean_ampl + clean_noise_after\n",
    "    sequence_after_anomaly = sequence_after_anomaly.reshape(number_of_sequences, sequence_length_after_anomaly)\n",
    "\n",
    "    return np.concatenate(seq = [sequence_before_anomaly, sequence_anomaly, sequence_after_anomaly], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryangillard/anaconda3/lib/python3.6/site-packages/seaborn/timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/ryangillard/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_clean_parameters = create_time_series_clean_parameters()\n",
    "\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_clean(5, 50, test_clean_parameters[\"clean_freq\"], test_clean_parameters[\"clean_ampl\"], test_clean_parameters[\"clean_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryangillard/anaconda3/lib/python3.6/site-packages/seaborn/timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_with_anomaly(5, 50, percent_sequence_before_anomaly, percent_sequence_after_anomaly, test_clean_parameters[\"clean_freq\"], test_clean_parameters[\"clean_ampl\"], test_clean_parameters[\"clean_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_clean_sequences = 8000\n",
    "number_of_evaluation_clean_sequences = 1980\n",
    "number_of_evaluation_anomalous_sequences = 20\n",
    "\n",
    "sequence_length = 13\n",
    "number_of_tags = 3\n",
    "tag_columns = [\"tag_{0}\".format(tag) for tag in range(0, number_of_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clean_freq': 0.005652984975311248,\n",
       "  'clean_ampl': 1.2525322122856308,\n",
       "  'clean_noise_noise_scale': 0.2},\n",
       " {'clean_freq': 0.0253522469717702,\n",
       "  'clean_ampl': 1.9311673797768711,\n",
       "  'clean_noise_noise_scale': 0.2},\n",
       " {'clean_freq': 0.05417740548196111,\n",
       "  'clean_ampl': 1.9354380953730974,\n",
       "  'clean_noise_noise_scale': 0.2}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_data_list = [create_time_series_clean_parameters() for tag in range(0, number_of_tags)]\n",
    "tag_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_array.shape = \n",
      "(8000, 3)\n"
     ]
    }
   ],
   "source": [
    "training_data_list = [create_time_series_clean(number_of_training_clean_sequences, sequence_length, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "training_data_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = training_data_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_training_clean_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "np.random.shuffle(training_data_array)\n",
    "print(\"training_data_array.shape = \\n{}\".format(training_data_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_data_clean_array.shape = \n",
      "(1980, 3)\n",
      "evaluation_data_anomalous_array.shape = \n",
      "(20, 3)\n",
      "evaluation_data_array.shape = \n",
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "evaluation_data_clean_list = [create_time_series_clean(number_of_evaluation_clean_sequences, sequence_length, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "evaluation_data_clean_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = evaluation_data_clean_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_evaluation_clean_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"evaluation_data_clean_array.shape = \\n{}\".format(evaluation_data_clean_array.shape))\n",
    "\n",
    "evaluation_data_anomalous_list = [create_time_series_with_anomaly(number_of_evaluation_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "evaluation_data_anomalous_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = evaluation_data_anomalous_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_evaluation_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"evaluation_data_anomalous_array.shape = \\n{}\".format(evaluation_data_anomalous_array.shape))\n",
    "\n",
    "evaluation_data_array = np.concatenate(seq = [evaluation_data_clean_array, evaluation_data_anomalous_array], axis = 0)\n",
    "np.random.shuffle(evaluation_data_array)\n",
    "print(\"evaluation_data_array.shape = \\n{}\".format(evaluation_data_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = \"data/train.csv\", X = training_data_array, fmt = '%s', delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27182496,1.26886656,1.25824257,1.36379633,1.44000216,1.33771941,1.36160289,1.4466736,1.34015,1.29149616,1.30663379,1.39335665,1.3489749;-1.53604873,-1.33370597,-1.33450673,-1.3640746,-1.41601392,-1.22761391,-1.24550294,-1.14257848,-1.1278296,-1.07367443,-1.08043296,-1.04718101,-0.99484385;1.72829056,1.67063167,1.66266333,1.90903373,1.76552939,1.95689709,1.89005734,1.88540987,1.99579605,1.92503801,2.01603399,2.09163428,1.96038997\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = \"data/eval.csv\", X = evaluation_data_array, fmt = '%s', delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07229573,1.11356461,1.1496629,1.11679324,1.09413637,1.07022543,1.10139481,1.0605918,1.15039996,1.02650587,1.18494646,1.10955279,1.13574986;1.91032575,2.00681719,1.85259875,1.99238507,1.79538091,1.8942822,1.9136153,1.82254143,1.74263908,1.79410615,1.83865414,1.77622121,1.66456235;1.96078033,1.97276845,2.05785492,2.09943559,2.04348526,1.9384103,2.1307926,1.98211642,2.04365035,2.11450171,2.0526771,2.07860117,1.96068071\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/eval.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"train_file_pattern\"] = \"data/train.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/eval.csv\"\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Sequence shape hyperparameters\n",
    "arguments[\"batch_size\"] = 7\n",
    "arguments[\"sequence_length\"] = sequence_length\n",
    "arguments[\"horizon\"] = 0\n",
    "arguments[\"reverse_labels_sequence\"] = True\n",
    "\n",
    "# Architecture hyperparameters\n",
    "\n",
    "# LSTM hyperparameters\n",
    "arguments[\"encoder_lstm_hidden_units\"] = [64, 32, 16]\n",
    "arguments[\"decoder_lstm_hidden_units\"] = [12, 32, 64]\n",
    "arguments[\"lstm_dropout_output_keep_probs\"] = [1.0, 1.0, 1.0]\n",
    "\n",
    "# DNN hyperparameters\n",
    "arguments[\"dnn_hidden_units\"] = [1024, 256, 64]\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_steps\"] = 100\n",
    "arguments[\"learning_rate\"] = 0.01\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging to be level of INFO\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "CSV_COLUMNS = tag_columns\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[\"\"], [\"\"], [\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    def _input_fn():\n",
    "        print(\"\\nread_dataset: _input_fn: filename = \\n{}\".format(filename))\n",
    "        print(\"read_dataset: _input_fn: mode = \\n{}\".format(mode))\n",
    "        print(\"read_dataset: _input_fn: batch_size = \\n{}\".format(batch_size))\n",
    "        print(\"read_dataset: _input_fn: params = \\n{}\\n\".format(params))\n",
    "\n",
    "        def decode_csv(value_column, sequence_length):\n",
    "            def convert_sequences_from_strings_to_floats(features):\n",
    "                def split_and_convert_string(string_tensor):\n",
    "                    # Split string tensor into a sparse tensor based on delimiter\n",
    "                    split_string = tf.string_split(source = tf.expand_dims(input = string_tensor, axis = 0), delimiter = \",\")\n",
    "                    print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \\n{}\".format(split_string))\n",
    "\n",
    "                    # Converts the values of the sparse tensor to floats\n",
    "                    converted_tensor = tf.string_to_number(split_string.values, out_type = tf.float64)\n",
    "                    print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \\n{}\".format(converted_tensor))\n",
    "\n",
    "                    # Create a new sparse tensor with the new converted values, because the original sparse tensor values are immutable\n",
    "                    new_sparse_tensor = tf.SparseTensor(indices = split_string.indices, values = converted_tensor, dense_shape = split_string.dense_shape)\n",
    "                    print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \\n{}\".format(new_sparse_tensor))\n",
    "\n",
    "                    # Create a dense tensor of the float values that were converted from text csv\n",
    "                    dense_floats = tf.sparse_tensor_to_dense(sp_input = new_sparse_tensor, default_value = 0.0)\n",
    "                    print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \\n{}\".format(dense_floats))\n",
    "\n",
    "                    dense_floats_vector = tf.squeeze(input = dense_floats, axis = 0)\n",
    "                    print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \\n{}\\n\".format(dense_floats_vector))\n",
    "\n",
    "                    return dense_floats_vector\n",
    "                    \n",
    "                print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "                for column in CSV_COLUMNS:\n",
    "                    print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \\n{}\".format(column))\n",
    "                    features[column] = split_and_convert_string(features[column])\n",
    "                    features[column].set_shape([sequence_length])\n",
    "\n",
    "                print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "\n",
    "                return features\n",
    "                \n",
    "            print(\"\\nread_dataset: _input_fn: decode_csv: value_column = \\n{}\".format(value_column))\n",
    "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS, field_delim = \";\")\n",
    "            print(\"read_dataset: _input_fn: decode_csv: columns = \\n{}\".format(columns))\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            features = convert_sequences_from_strings_to_floats(features)\n",
    "            print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            return features\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename = filename)\n",
    "        print(\"\\nread_dataset: _input_fn: file_list = \\n{}\".format(file_list))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(filenames = file_list)    # Read text file\n",
    "        print(\"read_dataset: _input_fn: dataset.TextLineDataset(file_list) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Decode the CSV file into a features dictionary of tensors\n",
    "        dataset = dataset.map(map_func = lambda x: decode_csv(x, params[\"sequence_length\"]))\n",
    "        print(\"read_dataset: _input_fn: dataset.map(decode_csv) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine amount of times to repeat file based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        # Repeat files num_epoch times\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "        print(\"read_dataset: _input_fn: dataset.repeat(num_epochs) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Group the data into batches\n",
    "        dataset = dataset.batch(batch_size = batch_size)\n",
    "        print(\"read_dataset: _input_fn: dataset.batch(batch_size) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine if we should shuffle based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "            print(\"read_dataset: _input_fn: dataset.shuffle(buffer_size = 10 * batch_size) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Create a iterator and then pull the next batch of features from the example queue\n",
    "        batch_features = dataset.make_one_shot_iterator().get_next()\n",
    "        print(\"read_dataset: _input_fn: batch_features = \\n{}\".format(batch_features))\n",
    "\n",
    "        return batch_features\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out_input_function(args):\n",
    "    with tf.Session() as sess:\n",
    "        fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"],\n",
    "          mode = tf.estimator.ModeKeys.EVAL,\n",
    "          batch_size = args[\"batch_size\"],\n",
    "          params = args)\n",
    "\n",
    "        features = sess.run(fn())\n",
    "        print(\"try_out_input_function: features = {}\".format(features))\n",
    "\n",
    "        print(\"try_out_input_function: features[tag_0].shape = {}\".format(features[\"tag_0\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_out_input_function(args = arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model function to be used in our custom estimator\n",
    "def encoder_decoder_stacked_lstm_autoencoder(features, labels, mode, params):\n",
    "    print(\"\\nencoder_decoder_stacked_lstm_autoencoder: features = \\n{}\".format(features))\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: labels = \\n{}\".format(labels))\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: mode = \\n{}\".format(mode))\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: params = \\n{}\".format(params))\n",
    "\n",
    "    # 0. Get input sequence tensor into correct shape\n",
    "    # Get dynamic batch size in case there was a partially filled batch\n",
    "    current_batch_size = tf.shape(input = features[CSV_COLUMNS[0]], out_type = tf.int32)[0]\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: current_batch_size = \\n{}\".format(current_batch_size))\n",
    "\n",
    "    # Get the number of features \n",
    "    number_of_features = len(CSV_COLUMNS)\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: number_of_features = \\n{}\".format(number_of_features))\n",
    "\n",
    "    # Stack all of the features into a 3-D tensor\n",
    "    X = tf.stack(values = list(features.values()), axis = 2) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: X = \\n{}\".format(X))\n",
    "\n",
    "    # Unstack all of 3-D features tensor into a sequence(list) of 2-D tensors of shape = (current_batch_size, number_of_features)\n",
    "    X_sequence = tf.unstack(value = X, num = params[\"sequence_length\"], axis = 1)\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: X_sequence = \\n{}\".format(X_sequence))\n",
    "\n",
    "    # Since this is an autoencoder, the features are the labels. It works better though to have the labels in reverse order\n",
    "    if params[\"reverse_labels_sequence\"] == True:\n",
    "        Y = tf.reverse_sequence(input = X, \n",
    "                                seq_lengths = tf.tile(input = tf.constant(value = [params[\"sequence_length\"]], dtype = tf.int64), \n",
    "                                                      multiples = tf.expand_dims(input = current_batch_size, axis = 0)), \n",
    "                                seq_axis = 1, \n",
    "                                batch_axis = 0)\n",
    "    else:\n",
    "        Y = X\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: Y = \\n{}\".format(Y))\n",
    "  \n",
    "  ################################################################################\n",
    "  \n",
    "    # 1. Create encoder of encoder-decoder LSTM stacks\n",
    "    def create_LSTM_stack(lstm_hidden_units, lstm_dropout_output_keep_probs):\n",
    "        # First create a list of LSTM cells using our list of lstm hidden unit sizes\n",
    "        lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units = units, forget_bias = 1.0, state_is_tuple = True) for units in lstm_hidden_units] # list of LSTM cells\n",
    "        print(\"\\nencoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: lstm_cells = \\n{}\".format(lstm_cells))\n",
    "\n",
    "        # Next apply a dropout wrapper to our stack of LSTM cells, in this case just on the outputs\n",
    "        dropout_lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(cell = lstm_cells[cell_index], \n",
    "                                                            input_keep_prob = 1.0, \n",
    "                                                            output_keep_prob = lstm_dropout_output_keep_probs[cell_index], \n",
    "                                                            state_keep_prob = 1.0) for cell_index in range(len(lstm_cells))]\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: dropout_lstm_cells = \\n{}\".format(dropout_lstm_cells))\n",
    "\n",
    "        # Create a stack of layers of LSTM cells\n",
    "        stacked_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells = dropout_lstm_cells, state_is_tuple = True) # combines list into MultiRNNCell object\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: stacked_lstm_cells = \\n{}\\n\".format(stacked_lstm_cells))\n",
    "\n",
    "        return stacked_lstm_cells\n",
    "  \n",
    "    # Create our decoder now\n",
    "    decoder_stacked_lstm_cells = create_LSTM_stack(params[\"decoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_stacked_lstm_cells = \\n{}\".format(decoder_stacked_lstm_cells))\n",
    "  \n",
    "    # Create the encoder variable scope\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        # Create separate encoder cells with their own weights separate from decoder\n",
    "        encoder_stacked_lstm_cells = create_LSTM_stack(params[\"encoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_stacked_lstm_cells = {}\".format(encoder_stacked_lstm_cells))\n",
    "\n",
    "        # Encode the input sequence using our encoder stack of LSTMs\n",
    "        encoder_outputs, encoder_states = tf.nn.static_rnn(cell = encoder_stacked_lstm_cells, \n",
    "                                                           inputs = X_sequence, \n",
    "                                                           initial_state = encoder_stacked_lstm_cells.zero_state(batch_size = current_batch_size, dtype = tf.float64), \n",
    "                                                           dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_outputs = \\n{}\".format(encoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_states = \\n{}\".format(encoder_states)) # tuple of final encoder c_state and h_state for each layer\n",
    "\n",
    "        # We just pass on the final c and h states of the encoder\"s last layer, so extract that and drop the others\n",
    "        encoder_final_states = encoder_states[-1]\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_final_states = \\n{}\".format(encoder_final_states))\n",
    "\n",
    "        # Extract the c and h states from the tuple\n",
    "        encoder_final_c, encoder_final_h = encoder_final_states\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_final_c = \\n{}\".format(encoder_final_c))\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_final_h = \\n{}\".format(encoder_final_h))\n",
    "\n",
    "        # In case the decoder\"s first layer\"s number of units is different than encoder\"s last layer\"s number of units, use a dense layer to map to the correct shape\n",
    "        encoder_final_c_dense = tf.layers.dense(inputs = encoder_final_c, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "        encoder_final_h_dense = tf.layers.dense(inputs = encoder_final_h, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "\n",
    "        # The decoder\"s first layer\"s state comes from the encoder, the rest of the layers\" initial states are zero\n",
    "        decoder_intial_states = tuple([tf.contrib.rnn.LSTMStateTuple(c = encoder_final_c_dense, h = encoder_final_h_dense)] + [tf.contrib.rnn.LSTMStateTuple(c = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64), h = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64)) for units in params[\"decoder_lstm_hidden_units\"][1:]])\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_intial_states = \\n{}\".format(decoder_intial_states))\n",
    "    \n",
    "    ################################################################################\n",
    "\n",
    "    # 2. Create decoder of encoder-decoder LSTM stacks\n",
    "    # The rnn_decoder function takes labels during TRAIN/EVAL and a start token followed by its previous predictions during PREDICT\n",
    "    # Starts with an intial state of the final encoder states\n",
    "    def rnn_decoder(decoder_inputs, initial_state, cell, inference):\n",
    "        # Create the decoder variable scope\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # Load in our initial state from our encoder\n",
    "            state = initial_state # tuple of final encoder c_state and h_state of final encoder layer\n",
    "            print(\"\\nencoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \\n{}\".format(state))\n",
    "            \n",
    "            # Create an empty list to store our hidden state output for every timestep\n",
    "            outputs = []\n",
    "            \n",
    "            # Begin with no previous output\n",
    "            previous_output = None\n",
    "            \n",
    "            # Loop over all of our decoder_inputs which will be sequence_length long\n",
    "            for index, decoder_input in enumerate(decoder_inputs):\n",
    "                # If there has been a previous output then we will determine the next input\n",
    "                if previous_output is not None:\n",
    "                    # Create the input layer to our DNN\n",
    "                    network = previous_output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "                    print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \\n{}\".format(network))\n",
    "                    \n",
    "                    # Create our dnn variable scope\n",
    "                    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "                        # Add hidden layers with the given number of units/neurons per layer\n",
    "                        for units in params[\"dnn_hidden_units\"]:\n",
    "                            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size, dnn_hidden_units[i])\n",
    "                            print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = {}, units = {}\".format(network, units))\n",
    "                            \n",
    "                        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "                        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size, number_of_features)\n",
    "                        print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \\n{}\\n\".format(logits))\n",
    "                    \n",
    "                    # If we are in inference then we will overwrite our next decoder_input with the logits we just calculated.\n",
    "                    # Otherwise, we leave the decoder_input input as it was from the enumerated list\n",
    "                    # We have to calculate the logits even when not using them so that the correct dnn subgraph will be generated here and after the encoder-decoder for both training and inference\n",
    "                    if inference == True:\n",
    "                        decoder_input = logits # shape = (current_batch_size, number_of_features)\n",
    "\n",
    "                    print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \\n{}\\n\".format(decoder_input))\n",
    "                \n",
    "                # If this isn\"t our first time through the loop, just reuse(share) the same variables for each iteration within the current variable scope\n",
    "                if index > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                # Run the decoder input through the decoder stack picking up from the previous state\n",
    "                output, state = cell(decoder_input, state)\n",
    "                print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \\n{}\".format(output)) # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "                print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \\n{}\".format(state)) # tuple of final decoder c_state and h_state\n",
    "                \n",
    "                # Append the current decoder hidden state output to the outputs list\n",
    "                outputs.append(output) # growing list eventually sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "                \n",
    "                # Set the previous output to the output just calculated\n",
    "                previous_output = output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "        return outputs, state\n",
    "  \n",
    "    # Train our decoder now\n",
    "  \n",
    "    # Encoder-decoders work differently during training/evaluation and inference so we will have two separate subgraphs for each\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN    or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # Break 3-D labels tensor into a list of 2-D tensors\n",
    "        unstacked_labels = tf.unstack(value = Y, num = params[\"sequence_length\"], axis = 1) # list of sequence_length long of shape = (current_batch_size, number_of_features)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: unstacked_labels = \\n{}\".format(unstacked_labels))\n",
    "\n",
    "        # Call our decoder using the labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to false\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = unstacked_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = False)\n",
    "    else:\n",
    "        # Since this is inference create fake labels. The list length needs to be the output sequence length even though only the first element is actually used (as our go signal)\n",
    "        fake_labels = [tf.zeros(shape = [current_batch_size, number_of_features], dtype = tf.float64) for _ in range(params[\"sequence_length\"])]\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: fake_labels = \\n{}\".format(fake_labels))\n",
    "        \n",
    "        # Call our decoder using fake labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to true\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = fake_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = True)\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_outputs = \\n{}\".format(decoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_states = \\n{}\".format(decoder_states)) # tuple of final decoder c_state and h_state\n",
    "    \n",
    "    # Stack together the list of rank 2 decoder output tensors into one rank 3 tensor\n",
    "    stacked_decoder_outputs = tf.stack(values = decoder_outputs, axis = 1) # shape = (current_batch_size, sequence_length, lstm_hidden_units[-1])\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: stacked_decoder_outputs = \\n{}\".format(stacked_decoder_outputs))\n",
    "    \n",
    "    # Reshape rank 3 decoder outputs into rank 2 by folding sequence length into batch size\n",
    "    reshaped_stacked_decoder_outputs = tf.reshape(tensor = stacked_decoder_outputs, shape = [current_batch_size * params[\"sequence_length\"], params[\"decoder_lstm_hidden_units\"][-1]]) # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: reshaped_stacked_decoder_outputs = \\n{}\".format(reshaped_stacked_decoder_outputs))\n",
    "\n",
    "    ################################################################################\n",
    "    \n",
    "    # 3. Create the DNN structure now after the encoder-decoder LSTM stack\n",
    "    # Create the input layer to our DNN\n",
    "    network = reshaped_stacked_decoder_outputs # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: network = \\n{}\".format(network))\n",
    "    \n",
    "    # Reuse the same variable scope as we used within our decoder (for inference)\n",
    "    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "        # Add hidden layers with the given number of units/neurons per layer\n",
    "        for units in params[\"dnn_hidden_units\"]:\n",
    "            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size * sequence_length, dnn_hidden_units[i])\n",
    "            print(\"encoder_decoder_stacked_lstm_autoencoder: network = {}, units = {}\".format(network, units))\n",
    "\n",
    "        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: logits = \\n{}\".format(logits))\n",
    "    \n",
    "    # Now that we are through the final DNN for each sequence element for each example in the batch, reshape the predictions to match our labels\n",
    "    predictions = tf.reshape(tensor = logits, shape = [current_batch_size, params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "    print(\"encoder_decoder_stacked_lstm_autoencoder: predictions = \\n{}\".format(predictions))\n",
    "    \n",
    "    # 3. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step(),\n",
    "            learning_rate = params[\"learning_rate\"],\n",
    "            optimizer = \"Adam\")\n",
    "        eval_metric_ops = {\n",
    "            \"rmse\": tf.metrics.root_mean_squared_error(labels = Y, predictions = predictions),\n",
    "            \"mae\": tf.metrics.mean_absolute_error(labels = Y, predictions = predictions)\n",
    "        }\n",
    "    else:\n",
    "        # Calculate absolute error\n",
    "        absolute_error = tf.abs(x = Y - predictions)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error = \\n{}\".format(absolute_error))\n",
    "\n",
    "        # Calculate sequence anomaly scores shape = (current_batch_size, sequence_length)\n",
    "        mean_across_time = tf.reduce_mean(input_tensor = absolute_error, axis = 1)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_time = \\n{}\".format(mean_across_time))\n",
    "        mean_across_time_tiled = tf.map_fn(fn = lambda batch_index: \n",
    "                                               tf.tile(input = mean_across_time[batch_index], \n",
    "                                                       multiples = [params[\"sequence_length\"]]), \n",
    "                                           elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                           dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_time_tiled = \\n{}\".format(mean_across_time_tiled))\n",
    "        mean_across_time_tiled_reshaped = tf.reshape(tensor = mean_across_time_tiled, \n",
    "                                                     shape = [current_batch_size, params[\"sequence_length\"], number_of_features])\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_time_tiled_reshaped = \\n{}\".format(mean_across_time_tiled_reshaped))\n",
    "        centered_error_across_time = absolute_error - mean_across_time_tiled_reshaped # e(i, t, f) - mu(i, f)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: centered_error_across_time = \\n{}\".format(centered_error_across_time))\n",
    "        covariance_matrices_across_time = tf.map_fn(fn = lambda batch_index: \n",
    "                                                        tf.matmul(a = centered_error_across_time[batch_index], \n",
    "                                                                  b = centered_error_across_time[batch_index], \n",
    "                                                                  transpose_a = True) / (params[\"sequence_length\"] - 1), \n",
    "                                                    elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                    dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: covariance_matrices_across_time = \\n{}\".format(covariance_matrices_across_time))\n",
    "        inverse_covariance_matrices_across_time = tf.map_fn(fn = lambda batch_index: \n",
    "                                                                tf.matrix_inverse(input = covariance_matrices_across_time[batch_index]), \n",
    "                                                            elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                            dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: inverse_covariance_matrices_across_time = \\n{}\".format(inverse_covariance_matrices_across_time))\n",
    "        mahalanobis_right_matrix_product_across_time = tf.map_fn(fn = lambda batch_index: \n",
    "                                                                     tf.matmul(a = inverse_covariance_matrices_across_time[batch_index], \n",
    "                                                                               b = centered_error_across_time[batch_index], \n",
    "                                                                               transpose_b = True), \n",
    "                                                                 elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                                 dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_right_matrix_product = \\n{}\".format(mahalanobis_right_matrix_product_across_time))\n",
    "        mahalanobis_distance_vectorized_across_time = tf.map_fn(fn = lambda batch_index: \n",
    "                                                                    tf.matmul(a = centered_error_across_time[batch_index], \n",
    "                                                                              b = mahalanobis_right_matrix_product_across_time[batch_index]), \n",
    "                                                                elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                                dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_vectorized_across_time = \\n{}\".format(mahalanobis_distance_vectorized_across_time))\n",
    "        mahalanobis_distance_across_time = tf.map_fn(fn = lambda batch_index: \n",
    "                                                         tf.diag_part(input = mahalanobis_distance_vectorized_across_time[batch_index]), \n",
    "                                                     elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                     dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_across_time = \\n{}\".format(mahalanobis_distance_across_time))\n",
    "\n",
    "        # Calculate feature anomaly scores shape = (current_batch_size, number_of_features)\n",
    "        mean_across_features = tf.reduce_mean(input_tensor = absolute_error, axis = 2)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_features = \\n{}\".format(mean_across_features))\n",
    "        mean_across_features_tiled = tf.map_fn(fn = lambda batch_index: \n",
    "                                               tf.tile(input = mean_across_features[batch_index], \n",
    "                                                       multiples = [number_of_features]), \n",
    "                                               elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                               dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_features_tiled = \\n{}\".format(mean_across_features_tiled))\n",
    "        mean_across_features_tiled_reshaped = tf.map_fn(fn = lambda batch_index: \n",
    "                                                            tf.reshape(tensor = mean_across_features_tiled[batch_index], \n",
    "                                                                       shape = [number_of_features, params[\"sequence_length\"]]), \n",
    "                                                        elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                        dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_features_tiled_reshaped = \\n{}\".format(mean_across_features_tiled_reshaped))\n",
    "        mean_across_features_tiled_transposed = tf.map_fn(fn = lambda batch_index: \n",
    "                                                              tf.transpose(a = mean_across_features_tiled_reshaped[batch_index]), \n",
    "                                                          elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                          dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mean_across_features_tiled_transposed = \\n{}\".format(mean_across_features_tiled_transposed))\n",
    "        centered_error_across_features = absolute_error - mean_across_features_tiled_transposed # e(i, t, f) - mu(i, f)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: centered_error_across_features = \\n{}\".format(centered_error_across_features))\n",
    "        covariance_matrices_across_features = tf.map_fn(fn = lambda batch_index: \n",
    "                                                            tf.matmul(a = centered_error_across_features[batch_index], \n",
    "                                                                      b = centered_error_across_features[batch_index], \n",
    "                                                                      transpose_b = True) / (number_of_features - 1), \n",
    "                                                        elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                        dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: covariance_matrices_across_features = \\n{}\".format(covariance_matrices_across_features))\n",
    "        inverse_covariance_matrices_across_features = tf.map_fn(fn = lambda batch_index: \n",
    "                                                                    tf.matrix_inverse(input = covariance_matrices_across_features[batch_index]), \n",
    "                                                                elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                                dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: inverse_covariance_matrices_across_features = \\n{}\".format(inverse_covariance_matrices_across_features))\n",
    "        mahalanobis_right_matrix_product_across_features = tf.map_fn(fn = lambda batch_index: \n",
    "                                                                     tf.matmul(a = inverse_covariance_matrices_across_features[batch_index], \n",
    "                                                                               b = centered_error_across_features[batch_index]), \n",
    "                                                                     elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                                     dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_right_matrix_product = \\n{}\".format(mahalanobis_right_matrix_product_across_features))\n",
    "        mahalanobis_distance_vectorized_across_features = tf.map_fn(fn = lambda batch_index: \n",
    "                                                                    tf.matmul(a = centered_error_across_features[batch_index], \n",
    "                                                                              b = mahalanobis_right_matrix_product_across_features[batch_index],\n",
    "                                                                              transpose_a = True), \n",
    "                                                                    elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                                    dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_vectorized_across_features = \\n{}\".format(mahalanobis_distance_vectorized_across_features))\n",
    "        mahalanobis_distance_across_features = tf.map_fn(fn = lambda batch_index: \n",
    "                                                             tf.diag_part(input = mahalanobis_distance_vectorized_across_features[batch_index]), \n",
    "                                                         elems = tf.range(start = 0, limit = tf.cast(x = current_batch_size, dtype = tf.int64), dtype = tf.int64), \n",
    "                                                         dtype = tf.float64)\n",
    "        print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_across_features = \\n{}\".format(mahalanobis_distance_across_features))\n",
    "    \n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    "\n",
    "    # 4. Create predictions\n",
    "    predictions_dict = {\"predictions\": predictions, \n",
    "                        \"mahalanobis_distance_across_time\": mahalanobis_distance_across_time, \n",
    "                        \"mahalanobis_distance_across_features\": mahalanobis_distance_across_features}\n",
    "\n",
    "    # 5. Create export outputs\n",
    "    export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    # 6. Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = predictions_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our serving input function to accept the data at serving and send it in the right format to our custom estimator\n",
    "def serving_input_fn(sequence_length):\n",
    "    # This function fixes the shape and type of our input strings\n",
    "    def fix_shape_and_type_for_serving(placeholder):\n",
    "        current_batch_size = tf.shape(input = placeholder, out_type = tf.int64)[0]\n",
    "        \n",
    "        # String split each string in the batch and output the values from the resulting SparseTensors\n",
    "        split_string = tf.stack(values = tf.map_fn(\n",
    "            fn = lambda x: tf.string_split(source = [placeholder[x]], delimiter = ',').values, \n",
    "            elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "            dtype = tf.string), axis = 0) # shape = (batch_size, serving_sequence_length)\n",
    "        print(\"serving_input_fn: fix_shape_and_type_for_serving: split_string = {}\".format(split_string))\n",
    "        \n",
    "        # Convert each string in the split tensor to float\n",
    "        feature_tensor = tf.string_to_number(string_tensor = split_string, out_type = tf.float64) # shape = (batch_size, serving_sequence_length)\n",
    "        print(\"serving_input_fn: fix_shape_and_type_for_serving: feature_tensor = {}\".format(feature_tensor))\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    # This function fixes dynamic shape ambiguity of last dimension so that we will be able to use it in our DNN (since tf.layers.dense require the last dimension to be known)\n",
    "    def get_shape_and_set_modified_shape_2D(tensor, additional_dimension_sizes):\n",
    "        # Get static shape for tensor and convert it to list\n",
    "        shape = tensor.get_shape().as_list()\n",
    "        # Set outer shape to additional_dimension_sizes[0] since we know that this is the correct size\n",
    "        shape[1] = additional_dimension_sizes[0]\n",
    "        # Set the shape of tensor to our modified shape\n",
    "        tensor.set_shape(shape = shape) # shape = (batch_size, additional_dimension_sizes[0])\n",
    "        print(\"serving_input_fn: get_shape_and_set_modified_shape_2D: tensor = {}, additional_dimension_sizes = {}\".format(tensor, additional_dimension_sizes))\n",
    "        return tensor\n",
    "            \n",
    "    # Create placeholders to accept the data sent to the model at serving time\n",
    "    feature_placeholders = { # all features come in as a batch of strings, shape = (batch_size,), this was so because of passing the arrays to online ml-engine prediction\n",
    "        feature: tf.placeholder(dtype = tf.string, shape = [None]) for feature in CSV_COLUMNS\n",
    "    }\n",
    "    print(\"\\nserving_input_fn: feature_placeholders = {}\".format(feature_placeholders))\n",
    "    \n",
    "    # Create feature tensors\n",
    "    features = {key: fix_shape_and_type_for_serving(placeholder = tensor) for key, tensor in feature_placeholders.items()}\n",
    "    print(\"serving_input_fn: features = {}\".format(features))\n",
    "    \n",
    "    # Fix dynamic shape ambiguity of feature tensors for our DNN\n",
    "    features = {key: get_shape_and_set_modified_shape_2D(tensor = tensor, additional_dimension_sizes = [sequence_length]) for key, tensor in features.items()}\n",
    "    print(\"serving_input_fn: features = {}\".format(features))\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(args):\n",
    "    # Create our custome estimator using our model function\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn = encoder_decoder_stacked_lstm_autoencoder,\n",
    "        model_dir = args['output_dir'],\n",
    "        params = {\n",
    "            \"batch_size\": args['batch_size'], \n",
    "            \"sequence_length\": args[\"sequence_length\"],\n",
    "            \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "            \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "            \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "            \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "            \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "            \"learning_rate\": args['learning_rate']})\n",
    "    \n",
    "    # Create train spec to read in our training data\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args[\"train_file_pattern\"],\n",
    "            mode = tf.estimator.ModeKeys.TRAIN, \n",
    "            batch_size = args[\"batch_size\"],\n",
    "            params = args),\n",
    "        max_steps = args[\"train_steps\"])\n",
    "    \n",
    "    # Create exporter to save out the complete model to disk\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name = \"exporter\", \n",
    "        serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]))\n",
    "    \n",
    "    # Create eval spec to read in our validation data and export our model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args[\"eval_file_pattern\"], \n",
    "            mode = tf.estimator.ModeKeys.EVAL, \n",
    "            batch_size = args['batch_size'],\n",
    "            params = args),\n",
    "        steps = None,\n",
    "        start_delay_secs = args[\"start_delay_secs\"], # start evaluating after N seconds\n",
    "        throttle_secs = args[\"throttle_secs\"],    # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "    \n",
    "    # Create train and evaluate loop to train and evaluate our estimator\n",
    "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a32fadac8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "\n",
      "read_dataset: _input_fn: filename = \n",
      "data/train.csv\n",
      "read_dataset: _input_fn: mode = \n",
      "train\n",
      "read_dataset: _input_fn: batch_size = \n",
      "7\n",
      "read_dataset: _input_fn: params = \n",
      "{'train_file_pattern': 'data/train.csv', 'eval_file_pattern': 'data/eval.csv', 'output_dir': 'trained_model', 'batch_size': 7, 'sequence_length': 13, 'horizon': 0, 'reverse_labels_sequence': True, 'encoder_lstm_hidden_units': [64, 32, 16], 'decoder_lstm_hidden_units': [12, 32, 64], 'lstm_dropout_output_keep_probs': [1.0, 1.0, 1.0], 'dnn_hidden_units': [1024, 256, 64], 'train_steps': 100, 'learning_rate': 0.01, 'start_delay_secs': 60, 'throttle_secs': 120}\n",
      "\n",
      "\n",
      "read_dataset: _input_fn: file_list = \n",
      "['data/train.csv']\n",
      "read_dataset: _input_fn: dataset.TextLineDataset(file_list) = \n",
      "<TextLineDataset shapes: (), types: tf.string>\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: value_column = \n",
      "Tensor(\"arg0:0\", shape=(), dtype=string, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: columns = \n",
      "[<tf.Tensor 'DecodeCSV:0' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:1' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=string>]\n",
      "read_dataset: _input_fn: decode_csv: features = \n",
      "{'tag_0': <tf.Tensor 'DecodeCSV:0' shape=() dtype=string>, 'tag_1': <tf.Tensor 'DecodeCSV:1' shape=() dtype=string>, 'tag_2': <tf.Tensor 'DecodeCSV:2' shape=() dtype=string>}\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \n",
      "{'tag_0': <tf.Tensor 'DecodeCSV:0' shape=() dtype=string>, 'tag_1': <tf.Tensor 'DecodeCSV:1' shape=() dtype=string>, 'tag_2': <tf.Tensor 'DecodeCSV:2' shape=() dtype=string>}\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \n",
      "tag_0\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \n",
      "SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64, device=/device:CPU:0), values=Tensor(\"StringSplit:1\", shape=(?,), dtype=string, device=/device:CPU:0), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64, device=/device:CPU:0))\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \n",
      "Tensor(\"StringToNumber:0\", shape=(?,), dtype=float64, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \n",
      "SparseTensor(indices=Tensor(\"StringSplit:0\", shape=(?, 2), dtype=int64, device=/device:CPU:0), values=Tensor(\"StringToNumber:0\", shape=(?,), dtype=float64, device=/device:CPU:0), dense_shape=Tensor(\"StringSplit:2\", shape=(2,), dtype=int64, device=/device:CPU:0))\n",
      "WARNING:tensorflow:From /Users/ryangillard/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \n",
      "Tensor(\"SparseToDense:0\", shape=(?, ?), dtype=float64, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \n",
      "Tensor(\"Squeeze:0\", shape=(?,), dtype=float64, device=/device:CPU:0)\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \n",
      "tag_1\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \n",
      "SparseTensor(indices=Tensor(\"StringSplit_1:0\", shape=(?, 2), dtype=int64, device=/device:CPU:0), values=Tensor(\"StringSplit_1:1\", shape=(?,), dtype=string, device=/device:CPU:0), dense_shape=Tensor(\"StringSplit_1:2\", shape=(2,), dtype=int64, device=/device:CPU:0))\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \n",
      "Tensor(\"StringToNumber_1:0\", shape=(?,), dtype=float64, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \n",
      "SparseTensor(indices=Tensor(\"StringSplit_1:0\", shape=(?, 2), dtype=int64, device=/device:CPU:0), values=Tensor(\"StringToNumber_1:0\", shape=(?,), dtype=float64, device=/device:CPU:0), dense_shape=Tensor(\"StringSplit_1:2\", shape=(2,), dtype=int64, device=/device:CPU:0))\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \n",
      "Tensor(\"SparseToDense_1:0\", shape=(?, ?), dtype=float64, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \n",
      "Tensor(\"Squeeze_1:0\", shape=(?,), dtype=float64, device=/device:CPU:0)\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \n",
      "tag_2\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \n",
      "SparseTensor(indices=Tensor(\"StringSplit_2:0\", shape=(?, 2), dtype=int64, device=/device:CPU:0), values=Tensor(\"StringSplit_2:1\", shape=(?,), dtype=string, device=/device:CPU:0), dense_shape=Tensor(\"StringSplit_2:2\", shape=(2,), dtype=int64, device=/device:CPU:0))\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \n",
      "Tensor(\"StringToNumber_2:0\", shape=(?,), dtype=float64, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \n",
      "SparseTensor(indices=Tensor(\"StringSplit_2:0\", shape=(?, 2), dtype=int64, device=/device:CPU:0), values=Tensor(\"StringToNumber_2:0\", shape=(?,), dtype=float64, device=/device:CPU:0), dense_shape=Tensor(\"StringSplit_2:2\", shape=(2,), dtype=int64, device=/device:CPU:0))\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \n",
      "Tensor(\"SparseToDense_2:0\", shape=(?, ?), dtype=float64, device=/device:CPU:0)\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \n",
      "Tensor(\"Squeeze_2:0\", shape=(?,), dtype=float64, device=/device:CPU:0)\n",
      "\n",
      "read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \n",
      "{'tag_0': <tf.Tensor 'Squeeze:0' shape=(13,) dtype=float64>, 'tag_1': <tf.Tensor 'Squeeze_1:0' shape=(13,) dtype=float64>, 'tag_2': <tf.Tensor 'Squeeze_2:0' shape=(13,) dtype=float64>}\n",
      "read_dataset: _input_fn: decode_csv: features = \n",
      "{'tag_0': <tf.Tensor 'Squeeze:0' shape=(13,) dtype=float64>, 'tag_1': <tf.Tensor 'Squeeze_1:0' shape=(13,) dtype=float64>, 'tag_2': <tf.Tensor 'Squeeze_2:0' shape=(13,) dtype=float64>}\n",
      "read_dataset: _input_fn: dataset.map(decode_csv) = \n",
      "<MapDataset shapes: {tag_0: (13,), tag_1: (13,), tag_2: (13,)}, types: {tag_0: tf.float64, tag_1: tf.float64, tag_2: tf.float64}>\n",
      "read_dataset: _input_fn: dataset.repeat(num_epochs) = \n",
      "<RepeatDataset shapes: {tag_0: (13,), tag_1: (13,), tag_2: (13,)}, types: {tag_0: tf.float64, tag_1: tf.float64, tag_2: tf.float64}>\n",
      "read_dataset: _input_fn: dataset.batch(batch_size) = \n",
      "<BatchDataset shapes: {tag_0: (?, 13), tag_1: (?, 13), tag_2: (?, 13)}, types: {tag_0: tf.float64, tag_1: tf.float64, tag_2: tf.float64}>\n",
      "read_dataset: _input_fn: dataset.shuffle(buffer_size = 10 * batch_size) = \n",
      "<ShuffleDataset shapes: {tag_0: (?, 13), tag_1: (?, 13), tag_2: (?, 13)}, types: {tag_0: tf.float64, tag_1: tf.float64, tag_2: tf.float64}>\n",
      "read_dataset: _input_fn: batch_features = \n",
      "{'tag_0': <tf.Tensor 'IteratorGetNext:0' shape=(?, 13) dtype=float64>, 'tag_1': <tf.Tensor 'IteratorGetNext:1' shape=(?, 13) dtype=float64>, 'tag_2': <tf.Tensor 'IteratorGetNext:2' shape=(?, 13) dtype=float64>}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: features = \n",
      "{'tag_0': <tf.Tensor 'IteratorGetNext:0' shape=(?, 13) dtype=float64>, 'tag_1': <tf.Tensor 'IteratorGetNext:1' shape=(?, 13) dtype=float64>, 'tag_2': <tf.Tensor 'IteratorGetNext:2' shape=(?, 13) dtype=float64>}\n",
      "encoder_decoder_stacked_lstm_autoencoder: labels = \n",
      "None\n",
      "encoder_decoder_stacked_lstm_autoencoder: mode = \n",
      "train\n",
      "encoder_decoder_stacked_lstm_autoencoder: params = \n",
      "{'batch_size': 7, 'sequence_length': 13, 'reverse_labels_sequence': True, 'encoder_lstm_hidden_units': [64, 32, 16], 'decoder_lstm_hidden_units': [12, 32, 64], 'lstm_dropout_output_keep_probs': [1.0, 1.0, 1.0], 'dnn_hidden_units': [1024, 256, 64], 'learning_rate': 0.01}\n",
      "encoder_decoder_stacked_lstm_autoencoder: current_batch_size = \n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "encoder_decoder_stacked_lstm_autoencoder: number_of_features = \n",
      "3\n",
      "encoder_decoder_stacked_lstm_autoencoder: X = \n",
      "Tensor(\"stack:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: X_sequence = \n",
      "[<tf.Tensor 'unstack:0' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:1' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:2' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:3' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:4' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:5' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:6' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:7' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:8' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:9' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:10' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:11' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack:12' shape=(?, 3) dtype=float64>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: Y = \n",
      "Tensor(\"ReverseSequence:0\", shape=(?, 13, 3), dtype=float64)\n",
      "WARNING:tensorflow:From <ipython-input-19-6284c27e708e>:41: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: lstm_cells = \n",
      "[<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a3304eb00>, <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a3305c4a8>, <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a3304ee10>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: dropout_lstm_cells = \n",
      "[<tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x1a3304eeb8>, <tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x1a3659bf28>, <tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x1a3659bfd0>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: stacked_lstm_cells = \n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x1a365a70f0>\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: decoder_stacked_lstm_cells = \n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x1a365a70f0>\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: lstm_cells = \n",
      "[<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a365a7b70>, <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a365a7be0>, <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x1a365a7c50>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: dropout_lstm_cells = \n",
      "[<tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x1a365a7b38>, <tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x1a330dda20>, <tensorflow.python.ops.rnn_cell_impl.DropoutWrapper object at 0x1a365a7cc0>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: stacked_lstm_cells = \n",
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x1a365a7d68>\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: encoder_stacked_lstm_cells = <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x1a365a7d68>\n",
      "encoder_decoder_stacked_lstm_autoencoder: encoder_outputs = \n",
      "[<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_2:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_5:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_8:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_11:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_14:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_17:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_20:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_23:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_26:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_32:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_35:0' shape=(?, 16) dtype=float64>, <tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0' shape=(?, 16) dtype=float64>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: encoder_states = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/Add_25:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_38:0' shape=(?, 64) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/Add_25:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_38:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Add_25:0' shape=(?, 16) dtype=float64>, h=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0' shape=(?, 16) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: encoder_final_states = \n",
      "LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Add_25:0' shape=(?, 16) dtype=float64>, h=<tf.Tensor 'encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0' shape=(?, 16) dtype=float64>)\n",
      "encoder_decoder_stacked_lstm_autoencoder: encoder_final_c = \n",
      "Tensor(\"encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Add_25:0\", shape=(?, 16), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: encoder_final_h = \n",
      "Tensor(\"encoder/rnn/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0\", shape=(?, 16), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: decoder_intial_states = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'encoder/dense/BiasAdd:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'encoder/dense_1/BiasAdd:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'encoder/zeros:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'encoder/zeros_1:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'encoder/zeros_2:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'encoder/zeros_3:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: unstacked_labels = \n",
      "[<tf.Tensor 'unstack_1:0' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:1' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:2' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:3' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:4' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:5' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:6' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:7' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:8' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:9' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:10' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:11' shape=(?, 3) dtype=float64>, <tf.Tensor 'unstack_1:12' shape=(?, 3) dtype=float64>]\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'encoder/dense/BiasAdd:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'encoder/dense_1/BiasAdd:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'encoder/zeros:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'encoder/zeros_1:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'encoder/zeros_2:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'encoder/zeros_3:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_2:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_1:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_2:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_1:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_2:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_1:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_2:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_2:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:1\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_5:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_3:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_5:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_3:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_5:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_3:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_5:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_5:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_1/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_1/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_1/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_1/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:2\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_8:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_5:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_8:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_5:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_8:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_5:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_8:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_8:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_2/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_2/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_2/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_2/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:3\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_11:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_7:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_11:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_7:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_11:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_7:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_11:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_11:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_3/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_3/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_3/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_3/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:4\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_14:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_9:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_14:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_9:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_14:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_9:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_14:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_14:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_4/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_4/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_4/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_4/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:5\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_17:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_11:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_17:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_11:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_17:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_11:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_17:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_17:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_5/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_5/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_5/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_5/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:6\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_20:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_13:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_20:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_13:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_20:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_13:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_20:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_20:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_6/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_6/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_6/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_6/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:7\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_23:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_15:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_23:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_15:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_23:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_15:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_23:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_23:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_7/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_7/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_7/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_7/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:8\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_26:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_17:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_26:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_17:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_26:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_17:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_26:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_26:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_8/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_8/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_8/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_8/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:9\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_19:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_29:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_19:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_29:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_19:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_9/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_9/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_9/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_9/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:10\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_32:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_21:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_32:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_21:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_32:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_21:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_32:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_32:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_10/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_10/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_10/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_10/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:11\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_35:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_23:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_35:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_23:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_35:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_23:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_35:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_35:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_11/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_11/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = Tensor(\"decoder/dnn_11/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \n",
      "Tensor(\"decoder/dnn_11/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \n",
      "Tensor(\"unstack_1:12\", shape=(?, 3), dtype=float64)\n",
      "\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \n",
      "Tensor(\"decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_25:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_38:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_25:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_38:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_25:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: decoder_outputs = \n",
      "[<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_2:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_5:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_8:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_11:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_14:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_17:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_20:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_23:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_26:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_32:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_35:0' shape=(?, 64) dtype=float64>, <tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0' shape=(?, 64) dtype=float64>]\n",
      "encoder_decoder_stacked_lstm_autoencoder: decoder_states = \n",
      "(LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Add_25:0' shape=(?, 12) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_38:0' shape=(?, 12) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Add_25:0' shape=(?, 32) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_38:0' shape=(?, 32) dtype=float64>), LSTMStateTuple(c=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Add_25:0' shape=(?, 64) dtype=float64>, h=<tf.Tensor 'decoder/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_38:0' shape=(?, 64) dtype=float64>))\n",
      "encoder_decoder_stacked_lstm_autoencoder: stacked_decoder_outputs = \n",
      "Tensor(\"stack_1:0\", shape=(?, 13, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: reshaped_stacked_decoder_outputs = \n",
      "Tensor(\"Reshape:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: network = \n",
      "Tensor(\"Reshape:0\", shape=(?, 64), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: network = Tensor(\"dnn/dense/Relu:0\", shape=(?, 1024), dtype=float64), units = 1024\n",
      "encoder_decoder_stacked_lstm_autoencoder: network = Tensor(\"dnn/dense_1/Relu:0\", shape=(?, 256), dtype=float64), units = 256\n",
      "encoder_decoder_stacked_lstm_autoencoder: network = Tensor(\"dnn/dense_2/Relu:0\", shape=(?, 64), dtype=float64), units = 64\n",
      "encoder_decoder_stacked_lstm_autoencoder: logits = \n",
      "Tensor(\"dnn/dense_3/BiasAdd:0\", shape=(?, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: predictions = \n",
      "Tensor(\"Reshape_1:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: absolute_error = \n",
      "Tensor(\"Abs:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_time = \n",
      "Tensor(\"Mean:0\", shape=(?, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_time_tiled = \n",
      "Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 39), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_time_tiled_reshaped = \n",
      "Tensor(\"Reshape_2:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: centered_error_across_time = \n",
      "Tensor(\"sub_1:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: covariance_matrices_across_time = \n",
      "Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 3, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: inverse_covariance_matrices_across_time = \n",
      "Tensor(\"map_2/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 3, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mahalanobis_right_matrix_product = \n",
      "Tensor(\"map_3/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 3, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_vectorized_across_time = \n",
      "Tensor(\"map_4/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 13, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_across_time = \n",
      "Tensor(\"map_5/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_features = \n",
      "Tensor(\"Mean_1:0\", shape=(?, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_features_tiled = \n",
      "Tensor(\"map_6/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 39), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_features_tiled_reshaped = \n",
      "Tensor(\"map_7/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 3, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mean_across_features_tiled_transposed = \n",
      "Tensor(\"map_8/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: centered_error_across_features = \n",
      "Tensor(\"sub_2:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: covariance_matrices_across_features = \n",
      "Tensor(\"map_9/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 13, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: inverse_covariance_matrices_across_features = \n",
      "Tensor(\"map_10/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 13, 13), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mahalanobis_right_matrix_product = \n",
      "Tensor(\"map_11/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 13, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_vectorized_across_features = \n",
      "Tensor(\"map_12/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 3, 3), dtype=float64)\n",
      "encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_across_features = \n",
      "Tensor(\"map_13/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 3), dtype=float64)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "shutil.rmtree(arguments[\"output_dir\"], ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/lstm_autoencoder_anomaly_detection_module\n",
    "python -m trainer.task \\\n",
    "    --train_file_pattern=\"data/train.csv\" \\\n",
    "    --eval_file_pattern=\"data/eval.csv\"  \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --batch_size=32 \\\n",
    "    --sequence_length=13 \\\n",
    "    --horizon=0 \\\n",
    "    --reverse_labels_sequence=True \\\n",
    "    --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "    --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "    --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "    --dnn_hidden_units=\"1024 256 64\" \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.1 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r data gs://$BUCKET/lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/trained_model\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/train.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/eval.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --batch_size=32 \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "  --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "  --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "  --dnn_hidden_units=\"1024 256 64\" \\\n",
    "  --train_steps=1000 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: mae\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 1\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: sequence_length\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 120\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: encoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: decoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: lstm_dropout_output_keep_probs\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"0.9 1.0 1.0\", \"0.95 0.95 1.0\", \"0.95 0.95 0.95\"]\n",
    "    - parameterName: dnn_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"256 128 64\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/hyperparam\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET/lstm_autoencoder/staging \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/train.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/eval.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r trained_model gs://qwiklabs-gcp-8923d4964bfbd247-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"lstm_autoencoder_anomaly_detection\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://$BUCKET/trained_model/export/exporter/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create $MODEL_NAME --regions $REGION\n",
    "gcloud ml-engine versions create $MODEL_VERSION --model $MODEL_NAME --origin $MODEL_LOCATION --runtime-version 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_instances = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local prediction from local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sequences.json', 'w') as outfile:\n",
    "  test_data_anomalous_string_list = [[np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list] for _ in range(0, number_of_prediction_instances)]\n",
    "  json_string = \"\"\n",
    "  for item in test_data_anomalous_string_list:\n",
    "    json_string += \"{\" + ','.join([\"{0}: \\\"{1}\\\"\".format('\\\"' + CSV_COLUMNS[i] + '\\\"', item[i]) for i in range(0, len(CSV_COLUMNS))]) + \"}\\n\"\n",
    "  json_string = json_string.replace(' ', '').replace(':', ': ').replace(',', ', ')\n",
    "  outfile.write(\"%s\" % json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf /tools/**/**/**/**/**/*.pyc\n",
    "model_dir=$(ls ${PWD}/trained_model/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=${PWD}/trained_model/export/exporter/${model_dir} \\\n",
    "    --json-instances=./sequences.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud ML-Engine prediction from deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataframe to instances list to get sent to ML-Engine\n",
    "instances = [{column: np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list for column in CSV_COLUMNS} for _ in range(0, number_of_prediction_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send instance dictionary to receive response from ML-Engine for online prediction\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "request_data = {\"instances\": instances}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'lstm_autoencoder_anomaly_detection', 'v1')\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
