{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# BUCKET = 'qwiklabs-gcp-8923d4964bfbd247-bucket' # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "# PROJECT = 'qwiklabs-gcp-8923d4964bfbd247' # REPLACE WITH YOUR PROJECT ID\n",
    "# REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# # Import os environment variables\n",
    "# os.environ['PROJECT'] = PROJECT\n",
    "# os.environ['BUCKET'] =  BUCKET\n",
    "# os.environ['REGION'] = REGION\n",
    "# os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_sequence_before_anomaly = 80.0\n",
    "percent_sequence_after_anomaly = 0.0\n",
    "\n",
    "def create_time_series_normal_parameters():\n",
    "    normal_frequency_noise_scale = 0.1\n",
    "    normal_frequence_noise_shift = 0.0\n",
    "\n",
    "    normal_amplitude_noise_scale = 1.0\n",
    "    normal_amplitude_noise_shift = 1.0\n",
    "\n",
    "    normal_noise_noise_scale = 0.2\n",
    "\n",
    "    normal_freq = (np.random.random() * normal_frequency_noise_scale) + normal_frequence_noise_shift\n",
    "    normal_ampl = np.random.random() * normal_amplitude_noise_scale + normal_amplitude_noise_shift\n",
    "\n",
    "    return {\"normal_freq\": normal_freq, \"normal_ampl\": normal_ampl, \"normal_noise_noise_scale\": normal_noise_noise_scale}\n",
    "  \n",
    "\n",
    "def create_time_series_normal(number_of_sequences, sequence_length, normal_freq, normal_ampl, normal_noise_noise_scale):\n",
    "    # Normal parameters\n",
    "    normal_noise = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length)]\n",
    "\n",
    "    sequence = np.sin(np.arange(0, number_of_sequences * sequence_length) * normal_freq) * normal_ampl + normal_noise\n",
    "\n",
    "    sequence = sequence.reshape(number_of_sequences, sequence_length)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def create_time_series_with_anomaly(number_of_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, normal_freq, normal_ampl, normal_noise_noise_scale):\n",
    "    sequence_length_before_anomaly = int(sequence_length * percent_sequence_before_anomaly / 100.0)\n",
    "    sequence_length_after_anomaly = int(sequence_length * percent_sequence_after_anomaly / 100.0)\n",
    "    sequence_length_anomaly = sequence_length - sequence_length_before_anomaly - sequence_length_after_anomaly\n",
    "\n",
    "    # normal parameters\n",
    "    normal_noise_before = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_before_anomaly)]\n",
    "    normal_noise_after = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_after_anomaly)]\n",
    "\n",
    "    # Anomalous parameters\n",
    "    anomalous_frequency_noise_scale = 2.0\n",
    "    anomalous_frequence_noise_shift = 1.0\n",
    "\n",
    "    anomalous_amplitude_noise_scale = 1.0\n",
    "    anomalous_amplitude_noise_shift = 1.0\n",
    "\n",
    "    anomalous_noise_noise_scale = 1.0\n",
    "\n",
    "    anomalous_freq = (np.random.random() * anomalous_frequency_noise_scale) + anomalous_frequence_noise_shift\n",
    "    anomalous_ampl = np.random.random() * anomalous_amplitude_noise_scale + anomalous_amplitude_noise_shift\n",
    "    anomalous_noise = [np.random.random() * anomalous_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_anomaly)]\n",
    "\n",
    "    sequence_before_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_before_anomaly) * normal_freq) * normal_ampl + normal_noise_before\n",
    "    sequence_before_anomaly = sequence_before_anomaly.reshape(number_of_sequences, sequence_length_before_anomaly)\n",
    "\n",
    "    sequence_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_anomaly) * anomalous_freq) * anomalous_ampl + anomalous_noise\n",
    "    sequence_anomaly = sequence_anomaly.reshape(number_of_sequences, sequence_length_anomaly)\n",
    "\n",
    "    sequence_after_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_after_anomaly) * normal_freq) * normal_ampl + normal_noise_after\n",
    "    sequence_after_anomaly = sequence_after_anomaly.reshape(number_of_sequences, sequence_length_after_anomaly)\n",
    "\n",
    "    return np.concatenate(seq = [sequence_before_anomaly, sequence_anomaly, sequence_after_anomaly], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/seaborn/timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_normal_parameters = create_time_series_normal_parameters()\n",
    "\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_normal(5, 50, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsvXmcK2d55/t7Skurpd73/fTZV5/FPraPd4PNYg9gSEgGwgDhkhgSErJw5waSTGYu92YuSW6SO7lhSAwhLMngZAiLAWOwAePteDm2z774bH1O73tL3VJrrXf+qEVvSaXWVpJK0vv9fPrT6lJ11StV1fu8z06MMQgEAoGg/pAqPQCBQCAQVAYhAAQCgaBOEQJAIBAI6hQhAAQCgaBOEQJAIBAI6hQhAAQCgaBOEQJAIBAI6hQhAAQCgaBOEQJAIBAI6hRnpQewEV1dXWx0dLTSwxAIBIKq4dVXX11gjHXnsq+tBcDo6CiOHTtW6WEIBAJB1UBE13LdV5iABAKBoE4RAkAgEAjqFCEABAKBoE4RAkAgEAjqFCEABAKBoE4RAkAgEAjqFCEABAKBoE4RAkAgqAKmzi5iZWqt0sMQ1BhCAAgEVcBzXzyNE49dqfQwBDWGEAACQRUgJ2TICVbpYQhqDCEABIIqgMkAmBAAAmsRAkAgqAIYY4oQEAgsRAgAgaAKYAxgstAABNYiBIBAUA0wBiZMQAKLEQJAIKgCGBMuAIH1CAEgEFQBTBYagMB6hAAQCKoB4QMQlAAhAASCKoAxBoj5vy5JxBJ48q9eK0kmeNECgIiGiehnRHSWiM4Q0e+Y7ENE9DdEdImIThLRjcWeVyCoJ5hwAtcta4thXDs2i7lLK5Yf24qewHEAn2KMvUZEzQBeJaInGWNnuX0eALBd/bkVwBfU3wKBIAeYDJEHUKfopr8SXP+iNQDG2DRj7DX19SqAcwAGU3Z7CMDXmMKLANqIqL/YcwsE9YLQAOoXTQCU4vpb6gMgolEAhwC8lPLWIIBx7u8JpAsJ7RgPE9ExIjo2Pz9v5fAEgupFOIHrFm3eL4X8t0wAEFETgH8D8LuMsUChx2GMPcIYO8wYO9zd3W3V8ASCqkbRACo9CkElsL0GQEQuKJP/PzPGvmWyyySAYe7vIXWbQCDIAcYgMsHqFF0AlEADtCIKiAD8A4BzjLG/yrDbYwA+pEYDHQHgZ4xNF3tugaBukEUxuHpFl/slkP9WRAHdAeCDAE4R0XF12x8CGAEAxtjfAXgcwIMALgEIAfiIBecVCOoGpRSE0ADqkVKagIoWAIyx5wBQln0YgE8Uey6BoB7RHnwhAOqTpAnI+mOLTGCBwObo874wAdUltvYBCKxhbWEdyxOrlR6GwI6U0AQgsD+l1ACFALAJx/71DTz9309WehgCG5KcACo8EEFF0Ew/ts4DEBRHLJxAPJqo9DAENmJlag2Xj04nE4FEIlhdol93oQHULqLaoyCV8z8Zx3NfOq3fF8IEVJ8IJ3AdwGQmVngCA3KCGRrBiDyA+sT2mcCC4lGqPQoBIEiiLQr4CSCyFsO3PvMcViatrw0vsCeshBqgEAB2QdR6EaQgJ5ixBhAD1hbXsXRtFUvjImKsXhAmoDpAFj1fBSlok3/SBMRKGhMusCeycALXASLVX5ACk5mSA6CHATIREVSPlDAMWAgAmyBzD7pAAKgrfl4DYKU1BwjsiawtAEQmcA3DWFLVEwig+AAAYxy4MAHVHyIKqA5gMkSqp8CA9uDL3G99m7hX6oZShgELAWATNHVfINDQV36JZFNw4QOoP3gNYOrMIr7/2RchJ6yRBkIA2ATGRCKYwIi+2k8kJwBhAqo/kiZAYGEsgJnzy4itJ8vGyAkZP/7LVzF3cTnvYwsBYBOEBiBIRU4xAaWGhArqA17rM/MHRENxXH91DrNvrOR9bCEAbILo+SpIJdUEJPIA6hPGLQBgsgAo5p4QAqCCrAciePWbF/UHW0QBKSyNr+KFfzxT95Nc0gmsxQEm1wiyCAOtG/hVP68NasgmWkGuWCIAiOjLRDRHRKczvH8vEfmJ6Lj68ydWnLfamTixgNe/dQmB2ZCoBsoxcWIeZ5+8jkgoVumhVBS9DrzwAdQ1jM8DMMkJSI0WywcrmsIDwFcA/C2Ar22wz7OMsXdYdL6aIFW1Fw+1Qqrzs17RIj2EE7i+4Z3AZt3BKm4CYow9A2DJimPVE7zqxmTUpQvg+S+fwYnHLhu26RNevL7tHNrKL5kQlrIaFNiWkD9i2bGYbL4A8E8H4Z8JFpUdXk4fwG1EdIKIfkhEe8t4XtvCXzilzovxoQ7MhvBvf/As1i28mezGzPklzF00Ri9oE14iXt+TXOrKjr9HhACwL5OnF/A/fvOnmDqziOlzizj/s/GijmcoBaJHBAEvfOUsjn7lbHrGeB5YZQLKxmsANjHG1ojoQQDfAbDdbEciehjAwwAwMjJSlsGtLayjqauxLOfi4R9wJjNdxSMiAMDK5BqWx9cQmA2hsbWh7OMrB7KJ8ztpArKXBrAytQaHU0Jzj7cs50u17YoooOpg9sIywICX/vk8govriIbi2HKkH+7GwqbbtHlCfR2LxEFElTcBZYMxFmCMramvHwfgIqKuDPs+whg7zBg73N3dXZLxxCMJ/OgvjuHqyzOYODmPRz/5NBau+Etyro0wPNjJci869WALN/N9aJ/XTp97bWEdj/3JUXzrM89j5kL+CTeFoN8fifQVoIgCsi8LYwGAgMWxAMKrMcgJhslTCwUfz5AHwPkAkr5D5f1CnMBlEQBE1EfqspaIblHPu1iOc5vx0j+fx/jr8zj35HVcf20eADB1pvzDMbPtgffu14G6z2SWNtHrgs8mPgAmMzz9hZNgMkNjqxtPfO4VTKgPdHApjJPfv1KSQl1pmpDQAKqCxbEANt/ch4F9nTjywd1we50YPz5f8PH4PIDUiCCWYHqYcMVMQET0DQD3AugiogkA/xmASxk0+zsA7wXwG0QUB7AO4H2sQsXvZ99YxrmnrsPT7MLM+SUE5kL69kgwhvBqFK19vrKMxaDamXn3NZlgk4mwFJi1wkyNfqk0l56fwsy5Jdz18D4MH+rBE//PK/jxXxzDO/7kCCZOzOO1f7uE4YPdaB9qtvS86aUg6mNRUI3EwnF8+zPPY8tt/QguhtH9tlbsf8cWAMDsxWWMH58HkxlIoryPbV4NlrcgpOyXB1ZFAb2fMdbPGHMxxoYYY//AGPs7dfIHY+xvGWN7GWMHGGNHGGMvWHHeQpg+pwQr3fHRfZATDGvz65CchNmLK/j5353E9/7z0bIlZCVtvNxFFCYg3eRRKcEXjybw4j+dQ8gfQSKWwKv/8yI6R1uw4+4heFsb8OAf3wIAuPryjKLuQ1n1WY1sstpnRTj8BKXjwtMTCMyGcPy7SkRb1+ZW/b2Rgz1YX4noWmO+8I5fgwmIGcPH7R4FZAsWxwJo7m7EyI09cHkcAIBdbx5BOBDF9VfnEF6NYema9Q+zGdmce/UiAFIFrlzhzz15agGnHx/D1RencfHZKawtrOPm9+3QV2+eJje6Rlsxd3EFi1dVAXDN+h69ZtffTCgIKoscl3H68atKIIl6WTo2JbXBzbf2oW3Qh2f+/lRBEX1GU3Fym15BoIhFQV0KgM7RFjicEob2d8Pb1oDd9w8DACSn8oBPnS1PSoOhsJNZjQ/NBGSzaBgrkWWWLHesbauwBjBzXnHyLlwNYPL0AnydHgzeYIxZ6NnehvlLKwguhQEAiyVYNOirfV4AxAu39wpKw7XX5rC2EMbtH9mDgb2daO33wdPk1t93Njjw5t8+hGgwhmP/+kbexzcNA1b9AXKiOL9QucJAbUE0FENgNoTtdw8CAO743/Yiuh5Dc7cXTV0ejN7ch/Hj85g+u4gd9wzC1eCAw+Uo2XiM0lx9zZuAWH1oAGkmoAprAJqZcOFqALH1GHq2t+mhuRrd29og/3AMANDS68XStYAhhNcK9OvPhfwUk/YvsJbTP7yK4YM9GD8+D7fXiaH9Xejd3o5YJJ62b8dIM7be3o8rL07jyAd3w+XJfeo1bwnKDD+AjaOA7MLSdUVN7xxtAQB4Wtxo6fWBJMIv/vlduOUDu9C/pwNTZxbx6G8/jaNfPVfS8WR1AsvpK8Baw9QEpCeClV8DiIZiWLzqh7PBgeWJVawthNGzrS1tv97tyW3b7xlEeDWG0LK1CXupYaAAIGvJcbV7S1QF64EIXvz6ebz8jQuYODmPwX1dkBwSGppcaOo0zynafvcQYusJjB2bzetcum1fTjcB8V3ihAkoC5rDThMAPC6PE5JEGNjbiURURiIm4/LRKcQjibR9rcKQCayHgXI76KUAatcEZNYIh1UwD2D2wjIYA3bcPahPsvxkr+Hr9MDb1oCWXi/6d3cCsN4RbBbfrUdICQ2goqzOKtGD147NIrQUweAB07QmA3272tHc04iLP5/M61yGMFBmXDTyCyghADaAMSUZw9Pihrctc1bt6M29uOc39+P+3z2E2HoC116b09+7+Nwkfvo3xxG1qEqlociXHumVngdQ2yag9M+nmTzKLfhkmeHMj67B4ZKw+62bACh+IbMFAxFh/zu3YN8Do+jc1AzJSQVHeWSCmYTDFuPwE1iHXxUAGkP7swsAIsKOu4cwdWYRq/PrOZ/LtBaQ6gNgKT6A5Yn8ghHqRgCc+sFVjL8+j71v27ShnVZySNh+5yCGb+yBt6MB5566joWrfpx76jqe+cJJXHlxGk/8+TFLNAPTFG8RBlqxMNBXvnEBEycXcOSDu9E24ENDk0sJGMjgB9r3wCj2vHUTXB4nRg/34fJzSsTQGz+fgCwzHP3aWTzzyKmCx5NaDhoQAsAurM6GAAIGb+hC52hLRrNPKtvuUvyPl55VtIDTT4zhO3/0/IaJhIYwT5bclm4CAuYv5VfRoC6cwOHVKF75lzcwenMvDj60Naf/kSTC3rduwiuPvoHv/JGSttCzow077h7Cc186jeuvz2HLkf6ixpWpxof+fj1EASWSUUALV/3oGGmuSBjo/BU/Tj1+FbvuG8bu+5UaVEf+w240trqz/KfCzjcN4cqL0/j2Z55HJBjDxMkFXDk6DbfXibt+fV9BzuHUlpCAEACV5sWvn0N4NQoAaOr04C2/f2Nez2dzdyMG9nbijWcmcPA9W3H+J9exMhlEcDGcsR4Zb+JJDR1PDQPN1zRYFwJg4sQ8WILhwLu25JWJt/+dW7Dppl4sja/C1+FB95ZWxMIJPPel0wguhosel9G2p25jDHJcBklUJyYgxZYZWg7jO3/8Au775EFOAyjP52YywwtfOYPGFjduef9OfbsWLZYLA3s70dzdiLWlMHp2tOHK0WkASr/W0EoEvnZPQeMCjBoA09P+8z6cwALGXplBaCWCtsEmNPd44WxwAMgvUnD73YP4+RdO4tTjY1iZDAJQIs4yFqTkzcMbJILJJv60bNS0ANDUo+uvz6Ox1W3IzssFIkLbYBPaBpv0bW4fweGWEFwuXgCYJ4IB//r7z2D/OzfXfBQQb+KKhRMAAyLBeNnzAOYurWD+kh93/vo+uL2ugo5BEuH+37sR8WgCbYNN+NnfHkfbYBNOPz6G5Ym1ogQAHwaqCUWhAZSfcCCKtQXluV+6toqdbxou6DhbjvTh+Hcu4+V/Pg8AIAIWx/wYvbnXdH99Vc+M5R+01T9vPch3sVjTPoBnHzmFf/uPz2LixDyGD/UUVIcjFSKCr91jSchfasMPZSPD2uI6goth7v3aXO4Zb9yk4zdpAirP59aSuIZuyO7I24jO0Rb07mhHg8+Ft//Bzdj/TqUWzMrEWkHHMw0D5VZ7gvKyMGa0r7f0FlZC3uFy4I6PKi1Rene0o22oCQtXM0eQGRJG9XwhlhQCJgvJXKlpAbBw1Y/AbAjRUBwjh6wrLe1tb0DIAg2AmUS7yDLTJX2tm4B4W7/+Os7KHga6PL4Gt9cJX2f+q/SNaGxxo6HJhZXJwgSAaSmIOsgEnjgxj6svz1R6GGloYb7tw0qZh5bewvtCDOzpxJ2/vg+3/MpOdI226iHqpvCOXy4MVOulYVhICQGgwGSGwGwIQ/u7sPdtmzB0wDoB4Ovw6CUAisFMA9Bfc36BWhUABid4Irnq17WBMpmAlsZX0T7UbGkWL6Boi+1DTVguQANg6kIAMF7/eigHffqJazj+ncvZdywzC1cCaO5pxOZbFVNNcxECAAB2vWkYvTva0TnagvWVCELLYdPOgMnWsakRQanF4IQPQCe0HEYiKmPT4V49qsMqvKoJqNjU/+Sql9MAEskVXnIFWA8mIE4bKKMGwBjD8vgqttxWXERXJtoGm3D1xZm87xWzooBAfUQBMVm25edbGPOja7QVu+4bAUmEzpH0/JBC6NqsHGf6/DJCS2Gc/fE1/PJf36ObrI21gKC/1q0FfM9o4QNQ0BI1ilHTMuHraEAiJiOyVlxCmD7Bc9EuBiefpu7VrAag/JYNAkCGmemjVASXwoiG4ugYsbaWv0b7UBMiwRiefeQUAinJQxthCP3kX9eBCcgsObDSREMxrM6to3O0Bd7WBhx69zZLfIqAUliwqcuDsz+6hhPfu4LV+XVDkImuEcgwXe0nYsnscDnPVnE1KwC0h62lz3oB4FUjOgIzQSxeLzz932yFzwwaANT37fUwWAUf5aT7Q+JJYVCOWkDLan2ojuHSCIDRw73YdFMPLj4ziXM/uZ7z/xk0ANNEMOvGaDdkLijALqxMKeGafESgVUgOCbvfsgmzbywjHFByDPzq+YBUDYATAJpciKfPGTmfu8ix25bATAiSg+DLMUMvH3wdigB45pFTeOw/HUU8WlhWsKmTj+/+VOMmIJl3aHFmn6RmVPrPvaBGAFndzUvD19mIt3zqJrT0+bA6l1kDiEcSxoWAidkHME8OqzUYlxxoF/zTyoTcOlCaboE77x2CwyXB26GUqfHP8AJA/c1SXxsXSiIMlCMwG0RzjxeSRWoaj6YBrEwGkYjJWFvIva4Hj1m4I2/jrfUoID4RTjZzApf4cwcX13H6B2Po2dGGhqbC4v9zpbmnEatzme+Tb//h8zj5/av632YF4IB6MQGp8e2M4YWvnMXcpZVKDwn+6SBIopKYlAHA0+zG/b97CG/91E1wNjh0gQPAuOrnQ4M1DSCW7jfMlRoWAKGSmH8AJQyUJ5/CTjzmPgD1YnIOn1oXAEDyJi6nBvDcl88gEZdxz8f2l/Q8ANDc7cVahvuEMYbATNCQXZ7JCVwPUUBaOGMiJuPsj69h4kThDdWtwj8dRHN3IxzO0k2Zw4d60LW5Fa39Pvink9qifq0ZvyhMPhuJeIUFABF9mYjmiOh0hveJiP6GiC4R0UkiutGK85rBGMP8FT8CM6GSSWuHU4KnxQ2HW/n6NlLtN8LMxGN2MWvVBMTfrHFNAMTlskQBxaMJTJxYwJ63jqC1vzRqPU9zTyMiwRgiwfTAgVg4oWpBvAko+b4xD6D2BYAsK+VQUp2clcQ/HSyZ+SeV1j4fAjPpPgBZ5ueM5PVP2EAD+AqAt2/w/gMAtqs/DwP4gkXnNRBcXMeP/vxVfPePX0AiLmNgT2cpTgMA2HJbP276xe1wuCRLNQBjuddajwIy1wD4UhCXnpvExWfzq5+eC8vjq2AyQ/fW9Fr/paC5R1mMrM6nLxZianlxs3h/wDwTuJYFgF7kLG4PEyiTGfwzwbIsFACgtd+L1bkQtxjUBsJbBbhFox4FlP93ZUkeAGPsGSIa3WCXhwB8jSn6y4tE1EZE/YyxaSvOr/HcP5zBzPkl3PKBXdhxz6ChL6fV3P7hPQCAC09PYG0D2+5GbJQIZvDy16wASL5OxBRHOl/fXE4wnH3qOqLBOLbflXthtlzQsjo7N1kTy52N5h4lGGF1bh1do8aaVNF1pYWgMR8kQxhoon58ANpnrURnOJ61RSWnqK1MAqCl3wfGlJLTbYNNxigg3fGbYc6wqQ9gEMA49/eEus1SFq8FsPnWPuz/d5tLOvnzNPc0mq7qcsHUCVxHJiDZxASUMJiAZCQiCQRmg5ZHvSyMBeD2OvWJudQ0d6sagIm5MBpSBUAmDYB/ra2Ka/OWAAB98tfvgwqbgPzTSiZ3OU1AynkVM1DSCcy9NtEAaiIKiIgeJqJjRHRsfj535080pPRkbSvTRdJo7vYWbgIysXUbsvrqJAoI4NRYPhEszhCPypDjLKMDtVAWxwLoHG2xvPxDJhp8Lri9TtNIIF0DSDDEowm8+s2LhoZDxvujPjQAlihvPkgmtC5xJBHaB0sTKpyKNoetTCmCx6wYnCFwxOADsGci2CQAvnbqkLotDcbYI4yxw4yxw93dudfv8c8oK6ty2ek0mrsbEVmL5dwmMhGX8egnf4arL8+YRrsYUv1r3gSUyQeQFAbaRMiHxRWLnJCxdH21bOYfjeYer6m2mNQAZMxdXMHr37qEmQtL+vtpxQJRBwKAJZ+LcneG43nlGxcw/vo8bvvQbnhaymNVcHtd8HV49BpSSR+AudUgUQWJYI8B+JAaDXQEgN9q+79flZZlFwC9mmqf2wo1GophbSGMwEwwQyIYFwZaR07gZBQQMzSE0ZLsNDXcClamlPwNs16/paS1z4fFa6tpE5ruBI4z/T1eAzAIyjqJAgKgX/tKaQDnfzqOUz+4ij1v3YQ9ao/octE22KRXkTWE/jITHwAXOp6vudiqMNBvADgKYCcRTRDRR4no40T0cXWXxwFcAXAJwBcB/KYV5+VZmQqCqDS1fzaiuVuxIQdyDAWNR7iVrpkGwD3gte4DME5sydWe5vxOxJMawMqUdRrA+OtzAIC+XR2WHTMXtt7Rj/WVCC4fncbz/3gGU2cWARg1AG2yS0TNHcL1kgcAAAlVAJSrMxxPcCmM5//xDIb2d+HIB3eV/fztQ01YmQoaEkIZS/p+zDQAuQANwKoooPdneZ8B+IQV58qEf1rJ/M3UwLtUtA00gSTC0rUANt/Sl3X/eCRp7zWz8Sc1ANRBFBDnA1Af9niMs33HZd03YKUJ6MqLM+je2qoL73IxcqgHLb1ePPvIKcgJhtBKBAN7Ow0+AO1a8+VFWAYNsVZhugagCkMTJzBjDEvXVkumxV19aQYswXDkQ7shOcrvKm0bakI8ksDawrrRB2AyZySdwKjfYnArU+VL1OBxNjjQPtSE+cv+7DsjeVMbMl7rtBQEf6/qUUDcylebGAFjbZRiCMwGsTgWwOYjpSn/vBEkEfa+fRRygsHT4sbM2SUwmXECQNY1IX7SM2sKX8u1gHTnr6YBmGjA02eX8O0/fB7LBTbbycaVl6bRsakZbQPWF3/LhXa16Nzy5JppMTjeamBMBMvvPDUhAJispNKX2/6v0bWlFfNX/DmtyjSThiHMjU8ES3AXuw5NQPzKV3Ose9sbEFqKIBaOo1iuvqR0mspFWysFe94ygnd99jbc+oFdiARjWLweQEwzAcUzaQBmC4QyDrrM6BpALLMGsO5XWrJGTTKri2VtcR1zb6xgy63lXyRoaIJnZWJNtwQo5aDVlxm6xFXEB1Bp/NOKU699qDLSuntrKyJrsZwcwdqDzTJqAJw0V69xPTiBtZU/rwHE1pXvqkWNi173R4s+55UXZ9C9rfzmHw2SCD3b2jCwV8lSnz6zpAs6OZF0Aht8ACZZwTXtA1BvfH2xZOIEjukLKYbwWtQyDREA3vj5BABg85HKLBIAoKHJBW9bg0ED4Ns/Cg2AY+6iUi2wZ1t50vpT6d6iZHbOX8luBopzN65pXY94coVXzsYolcAYBaT6AExKa/vU4nu5htpmwj+jmH8qubLT8HV40NLnxdTZRaMJyEQDqLc8gFQTUMLECRxXtUEmM7z+rUv48V+8asm5w4EoTv3gKjYd7tUTsiqFp9WNSDCWXAAw8znD2EWwjjSA2TeWMXdpBbMXV+D2Oitmr+sYbobDJWFBNQNdfHYy42TFm4DMS0Gkh4HWgwkooav7yvdDjmSCVmObJgCKMwHp5p9bK7ey4+nb1YH5Syv652IJpkd0ZHYC14EGkOIENtUAwkntIBqMmxbZK4QT37uCeDiBw/9+hyXHKwaHQ1I+e3L+T2sCA6R0BKv2TOB8eObvT+Gpv34NM+eX0LO9zbIWbfkiOSV0b23F+PE5TJ1exM+/cBJnfnzNdN941EQDyJAIVk9RQEkfgPLb6U7emlr/hWI0gGgohovPTqJnWxuauipj/kmlfbAJ4dWYXh6C1wAymYDqox+A8ltbDJjlAegLKdXubZWZ9OrLMxi5qVd3wlYScpKSF8OF/tq1GmjZWQ9E4J8OIrSs/O7d3l7R8Wy7axArk0G8+PVzAICJEwum+2k3biYfAOOdfDXeE1g28QFon9XpTobz6iagYGEaQDQUw/c/+xICMyHsf+eWQodrOVp7wSjvBNYEYcxcA9AXBTUqAHjNV9cATJzASQ3AWDiuGCJrMazNr6NnW2v2ncuAwykpiwK+QvCG/QDyvy+qVgDMXlgGALgalVSGnu2Vsf9rbDnSD2eDA8sTa3A2ODB3ccVULY1zoW3JRDBze55Z2nctYagGmrLKc3ACQGvAEylQA5g4uYCl66t4828fwOjNvQUdoxSk1q3iu6Fl0gA0alUD4APptMWSmQYQ030AsiF/ohgW1fag5c4Qz4TkUDSAZDQI3weY1wrTNYScz2HJSCvAzIVlOFwS7vzoXrT0eSsuANyNTmxRowZuft9OMJlh6nS6FsBnAps6dLSLyRd+qlENgA+bTQ31401AjS3F+QC0olrDB3sK+v9S4etqhMOV/JxKFJCJE9hkAqzVMFCz5ECzBRAfTMGbzophYcxmAsApKdVxTXqEZOwfnWfWdNUKgNkLy+je2oqttw/gl//qHrg8liQ1F8XhX96Be39zP3bfPwy314lxEzOQIRPY1AfAh4HWTxRQugBIagCuRidcjc6CfQD+qSCaujxwNpQ3SzwbkkR68qKzwWGsgR81f8A32lYL8OaujfIAdBOQ2kCef14KZXEsAF+HR19wVBrJIanzhPJ3Jh8AT76F86pSAMTCcSyMBdC7s7y1XLLhbfdg252DkBwSura0Ynl8NW2fpAYgm0rzpAbAaX4W3Nx2xCgAjOGfvAlJbuMeAAAgAElEQVTI6XagwecsWAPwT1cuSTAbbf2KH8DT7FLaYZppABke9loUArKZBhA3ZsAymekLKb5sdLG+ssUxv21W/wAgOcnQGtMQGZhhos+3fWZVCoCl66tgCWYbZ40ZTZ0eBJfCadv5RDDZzARk4vFP3adWkDfUAJK3prNBgtvrKijrkzGm9nOtfFSHGZoG0NDkhsyFgfICMdNEX4sCwJAbklIQjzGGf/m9n+PcU9c5DSDpNynGMR6PJOCfCtpKAGhOYEObWPUjmuVGAPn7C6tSACxcVWx1XZvtKwB8HR6EViJpkto0DDRTHgC36q/FSKCcTECk2ELdXiciBWgAoZUIYuFE2dr55YsWCeRpVmrNa99DPIsTeKPt1YzBBBQx+kESURnBxTCWJ9aMYaBZVsW5MHd5BYwlkzrtQNIJrG7gykFnmujrQgNYHPPD0+LWo0PsiK/DAzAgpNYs0UiYJYJlLAed/L9ajATaSABoJiCn2wEigtvr0uvm54NfLSNtVxPQppt6cMsHdqFvlxLGHA8nNUSNTMI/MBvC+Z+Nm75XrciGwICkAEjEZd0HFF6L6lFAcpxxbTILF4jTZ5dABPTurGw4OY9kEgaqv87oA6gDJ/DCWABdZWznVwi+DiV5KZRiBoplKQWRdAKj5k1AG0WyaCYgzXFbqAagRQBVolJsLjjdDuz/d5v1aCCzUhiZuPjMJJ774mmEA8XXSLILGU1AMVm//pHVWFoiGFDcMzJzfgkdm1rQ4HMVfAyrkRySIREMANdGNoMGUOtO4EQsgeWJNVvZ6szwdioCINUPoMc2Z1DVDBoAq3UBkPkz8RoAALh9hfkAFq4G4GxwwKdmE9sVreY8b/bIht4sx8JeCZWGX9kmorwGwHQNMLIW1X0AjF8hF/iMxKMJzF1cQf8eewWVSE5SLAUm80CmlX7NC4Dl8TWwBEPXZnsLAG3CSRMAemRDBgEgcw0/uGtciyagjYSaNvE7G5Rb1N3oRHQ9npfj89qxWbzx9AS23NZfsTIhuSI5lfHlJQC0dplTpamJXwkMGkCKD0DTAILLEYP2rE2GhT4j85dXkIjJ6N9tMwHgUPIAzO75TJ81XyFYdQJg4apScbNz1D7OGjMamlxwuCQEl1J8ANFcNQDzMsC1xEaTuW4C4jQAMOTcE4AxhmceOYWuLa24/Vf3FD/YEqNpALECBEAtaQD8PR/nnhHFB6Bce97klcmUmg/T55YAAvpsFlbucJKeB5Bq7c4UBZQvVScAps8vw9PiRnOPPQp6ZYKI4OvwILiYpwmIr/ZYxyYgzfav/W7wKol+ueYChJYjiKzFsOOeQUNSmV2R1OqniTx8AFqymN/CfsmVxiwTGFA0ALMgAD55rmABcHYJnSPNaGiyj/0fUBcFavav5DRO1VZZBKxqCv92IrpARJeI6NMm7/8qEc0T0XH159cKOQ9jDNNnFjGwt9PWDmANn0kugLbCy+TsM5aDTt9eS+TrAwByrwek9RCudE33XNEEQF4agOYDqFUTEO8D4JzAxv2TyXOFaMmJmGb/7yxgtKVFMwsCSDNhWrUgLLp+AhE5AHwewFsATAB4hYgeY4ydTdn1Xxhjv1XMufxTQb2RdjXg6/Bg5vyy/jeTWbLzVUYTkHlp15rUADbIbtYmfgcXBQTkXhE0oHaIaqkWAeAswAmshkmuzq0jEUvA4bK/ppMNY+lrY3ScmfYnJ4ormjh/2Y9ETEbfLnuZfwAYmtFLDqMAsMokbIUGcAuAS4yxK4yxKIBHATxkwXHTmDq7CABVJQCCy2F9Ik+k2DTNMGb91bYA2Chu25HqA/AqGkCu9YD8MyE4XBKaOu0d/aOhPeCpAiD1wefRS2jLDIHZUOkGV0YyaYWJmGx67fk2moU8I7r9f7d94v81eLNPqgkojQINIlYIgEEAfDbKhLotlV8kopNE9E0iGi7kRBMnFtDU1Wh7+79GU1cjWIJhbUHpFcw/3NkSObSGMJqlq5gsR7uS+rDzVj1nqgkoTx9AYCaIll6v7aN/NLTVXup3Yvrgqx8pHknoAmKlRvwAGz0XZteeFekEnj63hI7hZnia3Hn/b6nhTUBSlvvYkU1AZDpHQf+VP98DMMoY2w/gSQBfzbQjET1MRMeI6Nj8/Ly+/Y1nJnD9tTlsvWOgKuz/ANCt1iqafUMxA2k2zY2qUhr7ezJILvOJoRZITQRLLQAHcGGgugDYWAM4+YOr+Ml/ex3+mWDVmH+AzCt9bTsvyHSHcSyh1ziqFUdwRg1AzQT2tBgnar4UdL79cAGlrliXjco/8Dh4E5Bz4zkvq4aQ6f8K+i8jkwD4Ff2Quk2HMbbIGNPiIb8E4KZMB2OMPcIYO8wYO9zd3Q1Aqefy3JdOY2BvJ2567zYLhlweOoab4fI4MPuG0rRec/Bpk5kZ+s2s1gLSJHstmoBSH3a+AFyqCUj7zWeHmjF1egFXX5rBymQQLX1eK4dbUjI9wNrEzwsIB+cvaGhywdfhqRlHcNoczmnA0VAcLT3Ja0oSGZrB5FsGIRKMIRyI2rZOFPEagGPjqdqRRUBkwgoB8AqA7US0mYjcAN4H4DF+ByLq5/58F4Bz+Zxg7uIK5DjD4V/envWLsBOSQ0L3tjZdA9DqAGldzMxIrmaUH21iqFUTEL+yNdcAVGewqgmllo1OJbya1BCqJQIIMNcAyEH692PUALSyETIcTgmtA74a0gCM97lLvf5aHkBjW4O+OHB7nYZyyfnWAtIjxWxaJoQ369AGviAAuqUgX4qeTRljcQC/BeBHUCb2f2WMnSGizxLRu9TdPklEZ4joBIBPAvjVfM6xeC0AIqBjxN7Zv2b07mjH0vgqoqGYbgLaUAPgm34zpk98tVj5UZYZJAfptn+nmQBQf0sOCeQgQ6MUM8KBqH68qtIATB5wSSJTDUDPGo4mIDkJbQNNWJleq4meEalmQW0BIKtOYLfXqcfrN/hchmSxfBdJdg8VNlzzbBpAgQtjS9poMcYeB/B4yrY/4V5/BsBnCj3+wlU/WgeabNfRKRd6d7YDTNFiNNy5aABMCW/TTUAWZf7ZCU0DIInAEkxf2QGqkCQYinM5XVLWcrfh1Sh23DuE1n4f+mxU2TEbZg84SUnhaKYBgCmaUduAD7H1BEIrEdvXPMpG6kLHqWsAihPY7XXC0+RGZDUGh0vK2jltI/zTQZBEaO6150LBEAWUTQOwuRO4KBavBWxf/C0TPdvaQBJh+vxyUgNo5DIOM1xXJkN/wIHaTQQjiXT1ltcAGtsa8I7/dCu23J60Hjpc0obFruKRBOKRBFp6vdj/ji1VZS4kExsuSdADHsw0AAC6CQioDUdwahSQs0FZLCViCcTCcbi9LjQ0ueBscCgaIWcSzNdP5p8OormnseAImlJj1AAyTBTqZkelTEClZj0QQWgpgq4qFQDuRie6t7Vi6syC7gR2cSagTDef1v5Nk+y1GgUkSaSHuPECQJIIfbs6jNuyaADrao0YrblKNWEmrCRJAqmbKYM5QHJKelOZWnAEp97nLo9y/cOrMYApz5OnxQ1XoxOSQ8qpdWYm7NwqFEjVAMznCW3+qFkNYHFM6f5VrRoAAAzu7cTCZT8WLiuF7Bq5ULaM4V1q8wddA6hZE1DSvKGbgCg99R0AnC7HhrVywquqAGipRgGQnvZPEvTkCEkyTwpyOCV42xrganTURC5AemSYKgACShCh2+fEwYe24o6P7IUkGX1C+WjJTGbwz9hbADgMiWAZwoR1AVC5KKCSoguATdUrAAb2dYIx4MyPrmH4ULehkxkv2dPrfcicE7iGTUApGkCmpBdHFg1AFwBVrgFodm/eB5DJHCA5JRAR2geb9Uq51UyqD0ByKs5/Tbtze13o3NSC4YPdIAcZNIB8yiOsTK0hEZVtLQDMFgWpaOGfdk8EK5iFsQCauxttV6kvH3q2temr2wPv2moMfdzA0cMStZ0HIKsCQErxAWQKecsqAKrZBMSt4Fy8ADAJA+XvGW2BMHhDJ+YvriC8Vt3dwdIzoQkOp4SwXxUAXACF5EjVAHJ7RhhjOPq1c3B5HBg52G3BqEtDLiYgbZ+aFQCLY9XrANZwuBwYOdSDwRs60bezPWONj1QBICeSmcC1awKiNBNQJoeXw+XITQBUuQnI6XHo2/Ssd0qWyjCUCFD/b/hgNxgDJk8ulGfAJSJNADgUAaBrAFxUmJSiAeQqAC49N4Wp04u4+f074eu0b1mZTI5/nmJ9AJaEgZYKJjMEZkLYfrdZaaHq4s2/fVBv8JVRnXdIAPgbWtYvcCFp7nZHFwApeQC8vZsnFxOQ5KAN8yzsCr/C0zUAoqQTmAiQCEgww76aBtC1tQ0NTS6MH5/H1tsHyjdwi9EmccmhZPlKDgmSk/Sy6t5Wzn/mMN4PufgAZJnh9W9dQudoC3bfN2Lx6K3FsDjMYAKqaR+AJt27qtj+r0FctAt/MfmLTClXQ44zLhO4NjUASUqucrXJbGMTUGYn8HogCk+zu2pqRfEYNQBFgBH33Sghoeq+zvT7R5IIQwe6MXFivqqTBjUNgNcGHS4JTA2I8HJ5DoX4AK6+NIPAbAgH3701o13dLhjLQdehCUir+1LtJqBUjFEcnI03ZeIyRAHVoA+AyTDNA8hsAsquAVSj+Qcw3hO8D0C7J4iSwiCTCXHkxh6EV2OYvbBUjiGXBF0AuJL3gjb5NfcYq7tKEqX0zc7+jJx5YgytAz6MHu61cNSlweE0NwHx04S2T02GgcYjCTS2NRikfi1AJhoAmYQ+sgSrgyggLgzUlc0HkIMAqEIHMJCiARicwMo2/v4wKwwHACOHuuFwS7hydKYMIy4Nsi4AtHtB0l+nlvZIXRXn8oz4p4JKR0Gbr/6BVA2AEwDaa0q+rkkNIBFNVHX4ZyZSw/gA48Nu2FcigIDXvnkJ3//si+UaYlmQmTEKSHKSoQBaKg63Q++ClXYsmSEcqF4BoNj7lc/tUk1ARidw0lfiMNEgtf8bOdSDqy/PVG3muK4BOJOLAe0Z4SuBAummwmxm0kQsgUgwBm9bw4b72QUpQzVQ7TV/z1SsGFwpiUdldIw0V3oYlpOayAMY7b08xKm5M+eXsba4XpYxloPUKCCSJIPKn4rDKUE20QAmTi3gHz/0I6zOhqrWBAQkFwbOTGGgelkIbjJIefC3HOlDOBBVOl1VIak+AHKQPhGmawBk+r+ZCK0okUTVIwDMNYBkj4ikP7EmNQCAoX24qdKDsByjE1iz8SKzAOCYPlOdD7YZTGYg4pzjasx3Rg0ggwnoytFpSE6C2+dC1+bq1Ri1e8EsEYxg7gROffCHDnQDBMy9sYJqRLPjJ30Akv4ZUzUAs7DpjVhfUSOJqlAAUEarQbpZMB9sHy/XMVSLGkC6asc/7AYIuPPX9qFztAVPfO4VTJ1drImwWCAZBaQVtJJUc9BGPoB4igBgjGHi+DxGDvXgvt85VOohlxQtDDgpAADSvxzeB2DuBAYUM1BLjxdL11fLMmar2dAElM0HkMXsFVpRykk0VosA0J4NZu4PkDjzaaHF4GwuAMi2zRqKgQx1XbgoD5OVLxFh15uVhmv9ezowfXYRjLGqDHVMRW8IwxU8kxwbaABuCSzBICdk/YFYuraK0EoEwwfsm9GZK9rDrBVAkyQJTLX/Gey9G2gAANA+0oyl8SoXAJoT2Kk4gUkiNKUkbaX5ALJoAJoAqBYNAFCubyImm/cGMNwTNWgCcrgkQzXIWsEsioOPhuHhtw3s6cTaQhirc7XhB9DCQPn8CMm5kQag1obntIDxE0rf6KEDXSUebenRHmyDBmAIAzXuB5g7/zqGm+GfCSIe2bh7mh2RTfIAXA1OtPR60ya51OSobHkAoeUIiABPa/UIAH21b+IDMDQMqsVMYGeBao3dMUvkUZwA6fvyC/1etcHJ/OUVtNi0iUU+yClhoOSQ9M5fZmjCMhGXoRUEmDm/hPbh5poIFU5qAMlEMKgToqE3QIYoII2OkWaAAcsTq+je2lbqYVtKmgbgIBz+99sRXY+n7Zua/ZrNBLTuj8DT2pAxq9aOKNc6kWL2qxMnsKMGV/+AsdSB0QewsQbQNtgEyUFYvFad6n0qqVFAkkMJA81YDVRdFfIagH86iPbB2ggU0CZ2syggUNJUlq1VoBY5V41mICYDIE4zdkho6fWha7Q1bV9KKRmSiwmomsw/ALfaNwsJJeLyAAoTajYXALYeXsEYTUBJaW6WB2DM+pPQNtSEpeuBUg+xLGgCwGACUrUAM/TG8GqGeCIuY21+vap6/26ErgE0JMtiJ01ASQ3ArBooT0uPF84GR1U6guVE+qIgE4bnyCVlFwDLVSgAuIgffZuFJiBLZlgiejsRXSCiS0T0aZP3G4joX9T3XyKi0VyOu1Hv3GrGNKSLsmsAgNIXoZY0AIkrBUFqBFDmMFDNB6DYtlfnQmDMvk298yUtD8BhLAaXNAGZmBA5SCJ0bW7B5eenMHepusJBtXtC4qKAMpEqALL5ANZXIlUTAaSRjIYytxpIlRYAROQA8HkADwDYA+D9RLQnZbePAlhmjG0D8NcA/iynY1eRrS4fzML4MoWBpgqFjpFmrK9EEPJHSjrGcpBmApIIO+4ZxNbb+k3313xCmgkoMBMCkB4eWK1o94UrWzE4vhpohgf/zo/ug8vjxA//68uIhmIlHLW1pGmFG/R1NpTPcDs29AHIMsO6v/o0ADIzAXH1w1LLqOSLFRrALQAuMcauMMaiAB4F8FDKPg8B+Kr6+psA7qNaiGMsEFMTEGWKAjL+rZXGWLpW/Wag1IedHIS9bxvFjnuHTPeXUgSAf0ZpgWjnrk75QCaJYMlVgXkYaKaVX9tgE+74tX2IhROYv1w9ncL0RYFJ9EsqxAtCt7RhFdRwIArGqisEFMiiAZB5lFA+WCEABgGMc39PqNtM92GMxQH4AXSaHYyIHiaiY0R0bH5+3oLh2Y+MtYDMTECpGsAm1cFXA2YgWUaKvXfj29FhogE0+FxVW/8nFT0MVMsD4PsBZEgE22jl171FcZzOX6keASBrJqAcJjY+WMDpdmSsBbS2sI5nHzkFAGjutm8DGDM2CgM1aAC1EgXEGHuEMXaYMXa4u7v6k3vMMDUBcVEexp2ND4CnyQ1vRwOWJ9ZKOcSyoFUDzXUVk2oC8s8Ea8b8A3A+ADfvA0jmAehJwRnKBKfS4HOhpc9bVQKAyUxPCAQ2tm0bEuLcDtNaQIwx/OzzJzBzfgk3v2+HUiqjijDzhZCJAKikE3gSwDD395C6zXQfInICaAWwaMG5qxIyMwFJ6at9AKZ+gdY+HwKzoZKNr1ykF4PbWACkOoEDM0G01IgDGEguDBxOpQuWQSvkooBy8QFodG9pxUI1mYASxgqxG90TRg1AMvUBXDk6jdkLy7j1P+xO68ddDejfg6nfMPkdZEokzXp8C8b4CoDtRLSZiNwA3gfgsZR9HgPwYfX1ewH8lDFWex1OciQvE5DJRW3u8SIwGyzdAMuEHgWUY0Er3gQUjyawthhGay1pAHpzD6UonsQXg+PKhWfqB2BG99ZWBJfCCC2HSzJmq0lNDtw4Csg4KZqZgF7/9mV0bmrO6FeyO2YagMEEpL+WTMPIsx6/2AGqNv3fAvAjAOcA/Ctj7AwRfZaI3qXu9g8AOonoEoDfB5AWKlpPmAoA3t7Lq3smQqG1z4t1f9Q0O7KakFM1gGwCQEsEi8pYmVoDGNA+VBtJYIDxwZackkkpCO7B1wRDlu+sa4uSCVwtZiBlUSBxwjDzFMU7iiVHuhOYMYbVuRAG9nVVVfYvj24KM0n+M0bQFaYBWBJozxh7HMDjKdv+hHsdBvBLVpyrFkhra4f00q6JRLIEQCpaGYjV2VBVt8vUo4C4xJaN4EtBLI8rPpD2GqoWqzVAJyK4PA44G5yIqfV8iAsI0grDSRm0Rp5ONWhgZXINm26yfxtE3S8k5Z4HoPUMSDUBxcIJJGIyGqu4R4Te8tGkGiiR0VQmSYR8qz/ZzglcD2gXzrD6zWDjNZPqLb2K3bvazUDppSCyRQElfQDLE6uQnFRzTmDtO7j/d2/EgYe2GDQAvT+wpGkJ2Vd8Lo8Tbq8TwcXqMAHlEwaaDB5QzGWpiWDraq6Mp7V6BUBWDYCSQrBiGoAgf8hBgFYOGVrMt/Kew0nQUnfMVnjNqgZQ7Y7g1IYwWZ3AXC2g5fE1tTZS7axhJGey+UmXGsKpfyd80TxVaOYa+ufrbERwqToEgJyqFW5wfYnTEiRHeimIcEDpANZYRdU/U+H9Qvo2viOYSURQXse3YIyCAtDq3hvUejMNwOSauhudaGx114QA4Es/ZHUCqxNePCpjeWK1psw/ANDY4k5raZkMAkp1CFPOoX++Dg/WqkUDSDBDU6ANNQBnch9ypJuANA2gmk1AySARs2qgxix6oQFUEcqNTSkrPPUlJ+0z2Xibe701IABSIhmyCADN7BEORLG2EMbu+2vHAQwAB9+9FXsfGDVsS676eYcwDPVystHU6cHC1epxApPE5QFsVAqCMx1KDkrTANZrQQPYyATEVQPN1FAq6/EtGKOgACQHGbs8ZbCFZ7qorb0+zF1cweP/9eWqUe9TkXWHX25OYEAJBV0YUyazthopA63h8jjhTZmskpUgKMUElHv2p6/Dg3AginjU/g1iZNUsaFYDJxV9daxqDBl9AFWtAWxkAjKWUSEp+yIq7fgWjVOQJ5oJSJ/0MlR7zHSFdr9lBCM39WDq9CIuvzBV4tGWhnydwIDiCNbKYHQM15YJyBS+HDRfGTQfE1Cn0iynGnIBNLNgLolgvOlQ8QEYTUBhfxQNPlfBZRLsgGktIG1biglI4ueTHKneb6bK0b32mgWI8wfwD3YmE1DPtjbc98lDaB9uxvjx6qyZlJYJnMPqxeFSeqR6ml1oqrK6LoXAx/6DeA2Acq4A6etQBEA1aIraPdG7vR2bbupBU1fma8w7isnMBOSPVvXqH4BpNJRZ9q8WEZSvGUgIgAohOVIkNkGv+5OLCUhj5FA3Zi4sV1XJXw2WR+EvDW3S697aljUGvhZIrvr5PIDcw0CBpAYwfXYJL379XNbWiZVE8wu19vvwlk/dtGFPcP6+SfUBMMawHoigsYpDQIFMGoBJHoC6oMy3KqgQABUiNQ/AoAEYMoE3Ps7wwW6wBMPkqYVSDbVkaKu9nu1tGD7UnZOqruUCdG9NbxFYi/AtISlVA8jDBwAAx79zGad/OGbrVpFyQs55EqMUAaAVgzvx2GX82//xLEIrETS2VK8DGIDp4oi4PICGJjckJ8HV4CgoFFREAVUIRW2V9VW/wQfAX+wsF7RnexvcXicmTy1i863mjVTsCGMMjCmfb3BfFwb3deX0f7wGUA8YSkHopaEVzTFXAeDyONHgcyESVLTElcmgaY9dO8Dk3DRBgIuG0XwAcUWzmbmwjJVJJUly6Ibc7iu70rO9DUMHugzmPgcXBrrlSB+6t7bC7XUJAVBNpJqADAWwcvABJI8joX24WamNU0VopQDzLWCVFAD2nMCsxmD2SdEA8ikB7O3wIBKMgUgpC2FXGJccmQ0+DJQkAmPK/2ud4oDqzgIGgKH93Rja343wWlTfZmwII+ktUQ1BJTkiBECFIN0EpP5tMAHxPoDsx2rt82LiRPWYgOYureDaq3MA8i9g5XQ70NLrrZkmMNlImoBgiAhq8LnycnAO7e9C1+YWzF1csfViQVajgHLBYAJSV8WJuIzVuaQAqOYcAB5+Icj3A+CRCggDFQKgQiirllQVPz0MNBdHZ0ufD6GfTyIWjuv9ZO2KnJDxzN+f0leh+a5YbnrvNsRj9nViWo4h+zf5+v7fO6T7Q3Lh1g/sAgA8+Zev6uYRO5KXBsAlSWn30epcCHKCoXtLK+av+Ks6C5iHnwYyhcgKDaCKSCsGxwmDfJzAQLI6aGDG/tVBLz4zaTBB5BvJ07O93eoh2RrTctBE8LZ7Cjpe22ATxo/PQ47LBXeRKiX5CQA+E1j5LNq9degXtmHi1AL695h2nq06DBWEOSdw6j4iDLRKSK8GmswJ4J17uVxQzQaoNUm3M6ceH4Pbm1x3VFuHpnJjqBbLLRYKpW2gCXKC2baMiJxgOa9iDSGQ6utlVQB0bm7B7R/egwafqzQDLTP8QilZDjplH66sSq4IAVAh3I0uuDzOtAqPQEpGbA4Pg1YS2a4PtYYcl+GfDmLkxh59mxAAG2MoAKc7hAv/ztoGlcWCXf0A+WgAqWGggBLh5GxwwNtWG7Z/HUr+NlQQ5ncRmcDVw60f3IV7Pn5DSqKPmQ8g+7FcHica2xoQsLkGsDq/DiYz9O/u0LcJAbAxvBPYkBVcIG2DTSACFq4GrBie5WgNYXJB9wFIRhNQS5+35pIEeUtBpvLpQgOoIpo6G9HS60sJ7VPeyycTWKO1zwv/jL01AP+0IqDaBpu4SIZKjsj+GFb9uhO48OO5PE50b2/DxAl7lg9RooBy+4C6D8DJawBraFUbJtUSfDgwHxjAk0uHuFSKevyIqIOIniSii+pvUw8dESWI6Lj6k9owvq5JreUBpDqBc7ugLX0+22sAmgBo7ffB166o6NXaq7Vs8F3ALNAAAGDkYA8WrgYQWokUPTyryUsD4IrBaQsKOcHQVkN9ojX4eULitIHUfcpdCuLTAH7CGNsO4CfI3Ox9nTF2UP15V4Z96hJjS0hlmzERLLfjVEOjeP9MEA1NLnia3fC2KVEswgS0McnJ0BofAKCUDwGACRsWESzEB0CcCQgAenfWXqRYahKg8tq4z74HR3HgXVvzOm6xAuAhAF9VX38VwLuLPF7dYbDnSSYaQI4Pgx4KauM+wYGZkO6w9nYoGkCt2WqtxjwKqLjvrGNTM7ztDbh+fK7o8VmNnMhDAOhlsY0r354azRLXS4Jz9cN4hg90Y/Tm3ryOWawA6GWMTauvZwBkOruHiI4R0YtEJIQED7eq01XaPF/A5TkAABUOSURBVMNAAcUEBMCQBm83/NNBPWRVi2OPVGEV03JiWPlxmcDFHnPkxh5MnFhAPGKvJjFahdhc0UuqcALA7a2N0M801HtA/34s0J6zCgAieoqITpv8PMTvxxhjAFiGw2xijB0G8CsA/j8iyqinENHDqrA4Nj9vPxXVagwagHZd80wEA5IagF1zAeKRBIJLYbT2KwJA8wGElu1nh7YTuTj/CmHLkX7EIwmM20wLyMcEBADtw81oHUgGU3Ruqt0mQXqkoDZPWHAfZM0EZozdn3lANEtE/YyxaSLqB2B6NzHGJtXfV4joaQCHAFzOsO8jAB4BgMOHD2cSKDWD0QdgogHkKAFcHie87Q221QA0waRpKpogyNdpVW8QJwEog/OvEPp2d8DT4saVozO2qiLLZJZXKOO7/+/bAQDnfzoOAOjeVrtVYrWKsJnCQAuhWBPQYwA+rL7+MIDvpu5ARO1E1KC+7gJwB4CzRZ63ZiBOnUuGgebvAwAULcCuAmDxmhJ33jGirNA2He7FXQ/vw6H3bKvksOyPSakQSx58ibD51j5cPz6HWNg+gQNynhqAxpYjfdj5piHc/L6dJRiVPdC7wumLguKPWawA+ByAtxDRRQD3q3+DiA4T0ZfUfXYDOEZEJwD8DMDnGGNCAKgQp84Z2/+pO+RxhVr6fLY1AS2OBeBscOgrfyLCznuH4WzIvaBZPcJH/pAFeQA8mw73IhGVMfvGsjUHtIB8fQAabq8Ld/36DTVT+sEMIq33r/J3WUxAG8EYWwRwn8n2YwB+TX39AoAbijlPLWPI9OS8+yQRWILlpe639nkRDkQRDcVs5whbHAugY6RZxP3niWkxOIu+w55tbSCJMHthGUP7uy05ZrGwPKKA6g41GdCqaDBAZAJXHGMimLbRWAMmV/RIIJvVBGIyw+K1ALpsXqnUjhhLQagvLQqddTc60bmpGTMX7KEB8F3iBOlofkJLTYFFH0FQFMZqoEYNQNme+7E6hhX7+rjNEnwCsyHE1hO2L1VtR/haUbw/wCp6d7Zj7tKK3k6xkmhd4oSWaI6WBFbI3JAJIQAqjKHaY0rPV+X9PExA/T6M3NiDUz+4iqiN4us1B3DnZiEA8sVoAjJus4K+ne1IRGUsjFW+OBxLKEIo34Jm9YK+RuTmiWIRAqDCmIWB6t5+5H+Rb3zvNkRDcZx54pq1Ay2CuYsrkJyE9qHajdEuFfpcnxokYBG9O5SyCXZwBDNVCREmIHP0XtDCB1A7mBWDM17k/I7XNdqK3h3tGLdJtUdZZrjy0jSG9ncbGt0IcsSkFIQV4X8a3nYPGlvdWB5fte6gBRALx/HsF08pY6qRPr5Wo9v/LcoIB4QAqDiGlpC8vbcIdb97aysWrwUgJypv1505t4TQUgTb7hyo9FCqEoPZh4yaolW0DTZVvE/wye9fxeWj07jpvduxVdwr5qhzhNAAagjJQdh25wD6d3eYV/wr4Bp3bW5BIipj+twSvv2Hz2NhzG/lkPPi0nOTcDU6DF3ABLnDLxAkrmCglbQNNGFlag2MVSbxXk7IeOPpCQzt78KhX9gmnMAZ0IrfkYU+ANEUvsIQEe79zQMAgPkriiPOWPI1/4vctUWphvjSP5/H0rVVXDs2h67R8ldITMRljL0yi9Gb++B0i4SvQuCdwFtvH0BjCVodtg36EA3Fse6PVqSV4vjxeQSXwrjtw7vLfu5qQjMBWekLEgLARphFBBWy4mvt88HV6MDSNcWuO3+5MhrA9LklREPxvEvUCpLwuSEdI816KQ0raR1QGqisTK5VRABcfHYS3rYGjBwSWuJGaH4gK2tCCROQjTDN+izgGpNE6NqsrPidDQ7MX16piHp/7dgsnA0ODN7QVfZz1wpWPuyZaB+obKP4peur6N3ZbiiCKEhHSxZNLhSLP6b4xm0EX+ul2HTv7q2tIAJueHAUkbUY1ubXrRpmTjDGcO3VWQzu7xLmn2KwMOIjE94OD1yNjoo4gmWZYW1+Hc093rKfu+pQAwAamt1oG/RZElYtTEA2wlALqMikn/3v3IJNN/XC4Zbw+rcvY+6yv6wP2eyFZYSWItj0y0KtLwY+D6B05yC09TdhZbJ8GgBjDCzBEFyOQE4wvZ+FIDOaE9jpduC9f3G3JccUGoCNsCoKCAA8TW707mhHx1AzHC4J85dXrBpmVhhjOPY/L8LT4sbmW/rKdt5axMq0/41oH27C0vVA2UyFV1+awT99/CdYVCPUhADIjtIO0tpjCgFgI4xOYGs8/ZJTQteWVsyWseDX5MkFzJxbwqH3bIPLI5TMYuD9QqWke1sbwqsxrM6Vp5Dg4lgA0VAcl56fAgA09zSW5bzVjKEdpEUIAWAnMmQFF8vAng4sXPGXrT7Qxeem4GlxY9d9w2U5Xy2TTA4srQDoUTtpzV3yY2VyDZG10t4ra4thAMD11+YgOQi+TiEAssJ7gC1CCAAbwXv3rVT9+/d2gjFg5nx5tIDFMT96trWJ0g8WoE/8Jf4q24ea4GxwYOzlGXzrM8/hm//xGUycLF05keCSIgDkOENTd6NI/soBSbLeFCieUBvBh/xZme3Xs60NDpeEqTOLRR8rG7FwHCtTQVH62SJKUQHUDMmhmArHXpkFSzC4vS787G9PgMml8QkEF5NRaS0iAig3uL7QViEEgI0wOIEttP063Q707mzH1NnSCYBEXMbEyXksjgUAppSjEFhAmZzAQNIMtPnWfhx8z1ZE1mJ6KW8rYYwhuBTWk86ahQM4J/h5wSqEALARZk5gqyo/9u/pwNK11ZL5AV76+jk88bljOPq1cwAgun9ZRDITuPQSYPCGTkgOwv53bsbA3k4AwNRp6xcN4dUo5DjD5luVCLFWtZOdYGNaerxo6bNWWBYlAIjol4joDBHJRHR4g/3eTkQXiOgSEX26mHPWMobS0BL0xA8r0LqFlSLW+8qL0zj75HU4XBIWxwLwtLjh7fBYfp56pFxhoAAwuK8LH/zi/eja3Apfuwdtg76SmA2DqgO4f08H3v4Hh7Hj3iHLz1GLvPmTB3HHR/ZaesxiNYDTAH4BwDOZdiAiB4DPA3gAwB4A7yeiPUWetzbRTUDJpA+raB9S6r0sWywA1gMRPP/lM+je2op7fmM/AKBzU0vJbdb1QimawGwEH7Y7sLcLMxeWkbC4XaTmAPZ1NmLoQDfcjSJUuFIUJQAYY+cYYxey7HYLgEuMsSuMsSiARwE8VMx5a5VUE5CVc2hTtxcOl4TlCWsFwEv/dB6x9Tju/tgN2HxLH0Zu6sHW2/otPUc9Y+gRUWYG9nYgHklg4Yq1xQQ1DcAntMSKUw7ROwhgnPt7AsCtZThv1TGwtxP7HhhF+0CT5Y0/JInQNuDDyuSaEtlhwfFlmeHy0WnsfvOwXpfkrZ+6yYrhClTKlQhmhlZWfHEsoLeOtILgUhiSg9DY4rbsmILCyCoAiOgpAGb5/H/EGPuu1QMioocBPAwAIyMjVh/e1nia3TjyQaUmusERbBFtQ82YOb+EJ/7sFfg6PLj7Y/uLOt76SgQswdA+LHr9lgyyNhggH3wdHjQ0uSyLBJLjMl795kVMnlqAt8Mjev/agKwCgDF2f5HnmATAp4QOqdsyne8RAI8AwOHDhyvTosgGGHoCWET7YBMuPz+F4GLYktorui1XqPIlgzcLlv/chM5NLUporwVcf30eJx67AgDo22WdRiEonHKYgF4BsJ2INkOZ+N8H4FfKcN6qhu8JYBVtg03669W5EBKxBByuwks1a8k8vk4hAEqFt60BJFFFGrUAQOdoC87++BrkuFxwvf5EXIbkIFx8bhKNrW7c9uE9ovyzTShKABDRewD8/wC6AfyAiI4zxt5GRAMAvsQYe5AxFiei3wLwIwAOAF9mjJ0peuQ1TilMQFo3qfbhJiyPryEwG8q7pvjq/DoaW91wuh1CAygDzT1efPCR++D2uipy/s5NzUjEZKxMBQvuRvbE515BeDUK/1QQe966CVuOiCABu1BsFNC3GWNDjLEGxlgvY+xt6vYpxtiD3H6PM8Z2MMa2Msb+tNhB1wNUgrofLb1ePPR/3YY7P7oPAOCfzq8BSCIu41uffhYnvnsZgFLQy+GW0NBUmcmpXqjU5A9AL+lRqBkouBTG9NklLI+vQU4wbLtrwMrhCYpEZALbFJKsr/wHAN1b2/ScgHwFgH86iNh6ApNqclBwKQxfh0fE/Ncwrf0+ONwS5vMIBZUTMmS1htD4caWg3H2/cxD3fHw/OjeJDHE7ITIwbIrViWA8bq8LjW0NWJkK4uyT1+BqcKJ/bweaMpTkHT8xj/WVCBwuZb2wcNmPeDSB4GI44/8IagPJIWFgT2delUEf/9OX0THcjNs/shfjr8+hqcuD0Vv6xELBhggBYFOUktClO35rvw9Xjk7j4jNKQBYRsOW2ftz18A1pPXxPfu8KFq4GsPt+JSxXTjDMX/YjuBjWa8YIapehA90YPz4P/3QQrf0b1+2JheOYvbCM9UAU8WgCk6cXsf2uQTH52xRhArIppYgC4mnr9yERk9GzrQ2/8Gd3Yt+Dm3H5hWm88I9n0toCrs6tI7Yex+UXpuDtUKJRps8tIrQSERFAdcDwwW4AwEv/4zwe/Z2nsTS+mnHfpeurYEwxF06eWkA8ksDQga5yDVWQJ0IA2JRSlH7l6RxtAUmE2z+yBx3Dzbj1A7tw6D1b8cbPJ3Hye1f0/RJxWQ/3DC6G0bujHe1DTbhydAZMZkIA1AEtvV609vtw/dU5rM2v49yT1zPuu3BVdRYz4PQPx0AE9O/uKNNIBfkiTEA2RXIQJEfpBMDONw1h+FC3wYZ/6Be3wz8TwiuPvgEmAwffvRVr8+vgFYL2oSa0DzXhtW9eAiBCQOuFXfcN48qL0/A0u3H5hSlsvWMAkbUoNt3Ua9hvccwPZ4MD8UgC02eX0L2ltaJRTIKNEQLAptzwjs0ILUdKdnzJIaU5cCWJcO8nDiCyFsPZp67h4Lu3IqA2Ce8cVTJC24eaMXpzLxwuB849dV10/qoTbnhwM254cDOmzixi/PV5fP//fBHkIHzoi/fD5XGCMYZETMbCWAB9u9qxOBbAuj+K/r1i9W9nhAnIpnSOtGD4QHfZzytJhKH9XQgtRRDyR7CqCoAbHhyFs8GBnq2tICIceOcWvO+/3Qtfu9AA6on+3R3o3taK9uFmMDUYAABe/sYF/NPHfoLl8TV0bW5F12alkFz/HhEkYGeEBiBIo3NzMvknMBuCwyVh6+0D2Hr7gCjgVeeQRHjos7cjshbD1z/2FGYvLMPb3oDTPxxDY6sboaUI+na2w+l2YPrcEvp2ipo/dkYIAEEaWjvHxasBrM6to7nHKyZ+gYGGJhfah5oxc2EZc5dX4HRLePef3gGWYPB1eNC/pwPb7howNJgR2A9xdQRpuL0utPR6sTDmR2A2hOYekewlSKd3Rxsu/HQcjAG3fGAXvK3JgnUOl0MkCVYBwgcgMKVzcwvmLq1gdS5kSeloQe3Rt7MdjAG9O9ux74HRSg9HUABCAAhM6RptQWgpAiaztFA/gQAAhg/2YOsdA7j3N/ZDEibCqkSYgASmbLtrEOv+KHa/ZQStfRun/wvqk4YmF970iQOVHoagCIQAEJjia/fo7SkFAkFtIkxAAoFAUKcIASAQCAR1ihAAAoFAUKcIASAQCAR1SlECgIh+iYjOEJFMRIc32G+MiE4R0XEiOlbMOQUCgUBgDcVGAZ0G8AsA/j6Hfd/EGFso8nwCgUAgsIiiBABj7BwA0e5NIBAIqpBy+QAYgB8T0atE9HCZzikQCASCDciqARDRUwD6TN76I8bYd3M8z52MsUki6gHwJBGdZ4w9k+F8DwPQhESEiE7neI5apwuAMKEpiO8iifgukojvQmFTrjtSagPwQiCipwH874yxrA5eIvovANYYY/9vDvseY4xldC7XE+K7SCK+iyTiu0givov8KbkJiIh8RNSsvQbwVijOY4FAIBBUkGLDQN9DRBMAbgPwAyL6kbp9gIgeV3frBfAcEZ0A8DKAHzDGnijmvAKBQCAonmKjgL4N4Nsm26cAPKi+vgKg0JKBjxQ+uppDfBdJxHeRRHwXScR3kSeW+AAEAoFAUH2IUhACgUBQp9hSABDR24noAhFdIqJPV3o85casdAYRdRDRk0R0Uf3dXulxlgoi+jIRzfEhwJk+Pyn8jXqvnCSiGys3cuvJ8F38FyKaVO+P40T0IPfeZ9Tv4gIRva0yoy4NRDRMRD8jorNqCZrfUbfX5b1hBbYTAETkAPB5AA8A2APg/US0p7KjqghvYowd5MLaPg3gJ4yx7QB+ov5dq3wFwNtTtmX6/A8A2K7+PAzgC2UaY7n4CtK/CwD4a/X+OMgYexwA1OfkfQD2qv/z39XnqVaIA/gUY2wPgCMAPqF+5nq9N4rGdgIAwC0ALjHGrjDGogAeBfBQhcdkBx4C8FX19VcBvLuCYykpapLgUsrmTJ//IQBfYwovAmgjov7yjLT0ZPguMvEQgEcZYxHG2FUAl6A8TzUBY2yaMfaa+noVwDkAg6jTe8MK7CgABgGMc39PqNvqCbPSGb2MsWn19QyU8Np6ItPnr9f75bdUs8aXOXNg3XwXRDQK4BCAlyDujYKxowAQKKUzboSiwn6CiO7m32RK6Fbdhm/V++eHYsrYCuAggGkAf1nZ4ZSX/9XOHaM0EERhHP+/Qm2stBK0iOANRCy8gHZ2VqbwAvbewQuIlYiVFlt7Am00IqKSMkXSaSv6LGZCFiGiIdlZMt8PhiW7W7wZhn3k7eyY2TxwCRy6+3v5mubG/9QxAXSAldLv5XguG+7eicce4TuLDaDb//saj710ESYxrP/ZzRd377r7p7t/AScMyjxTPxZmNkN4+J+7+1U8rbkxojomgFtgzcwaZjZLeKlVJI6pMr9snVEAzXhbE/jrRnzTYlj/C2A/rvjYBN5K5YCp9KOOvctga5UC2DOzOTNrEF5+3lQd36RY2Hf+FHhy9+PSJc2NUbl77RrhK+IXoE3YdTR5TBX2fRW4j+2x339gkbDC4RW4BhZSxzrBMbgglDY+CHXbg2H9B4ywaqwNPADrqeOvYCzOYl9bhIfcUun+ozgWz8B26vjHPBZbhPJOC7iLbSfXuTGOpi+BRUQyVccSkIiIVEAJQEQkU0oAIiKZUgIQEcmUEoCISKaUAEREMqUEICKSKSUAEZFMfQNcSkPuYZ9BFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_with_anomaly(5, 50, percent_sequence_before_anomaly, percent_sequence_after_anomaly, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_normal_sequences = 7000\n",
    "\n",
    "number_of_validation_normal_1_sequences = 500\n",
    "number_of_validation_normal_2_sequences = 500\n",
    "number_of_validation_anomalous_sequences = 500\n",
    "\n",
    "number_of_test_normal_sequences = 750\n",
    "number_of_test_anomalous_sequences = 750\n",
    "\n",
    "sequence_length = 30\n",
    "number_of_tags = 3\n",
    "tag_columns = [\"tag_{0}\".format(tag) for tag in range(0, number_of_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'normal_ampl': 1.4417099953968768,\n",
       "  'normal_freq': 0.030003401425553345,\n",
       "  'normal_noise_noise_scale': 0.2},\n",
       " {'normal_ampl': 1.7365692626738212,\n",
       "  'normal_freq': 0.07289546412545128,\n",
       "  'normal_noise_noise_scale': 0.2},\n",
       " {'normal_ampl': 1.2083356124328901,\n",
       "  'normal_freq': 0.0391578564603241,\n",
       "  'normal_noise_noise_scale': 0.2}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_data_list = [create_time_series_normal_parameters() for tag in range(0, number_of_tags)]\n",
    "tag_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_normal_sequences_array.shape = \n",
      "(7000, 3)\n"
     ]
    }
   ],
   "source": [
    "training_normal_sequences_list = [create_time_series_normal(number_of_training_normal_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "training_normal_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = training_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_training_normal_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "np.random.shuffle(training_normal_sequences_array)\n",
    "print(\"training_normal_sequences_array.shape = \\n{}\".format(training_normal_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_normal_1_sequences_array.shape = \n",
      "(500, 3)\n",
      "validation_normal_2_sequences_array.shape = \n",
      "(500, 3)\n",
      "validation_anomalous_sequences_array.shape = \n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "validation_normal_1_sequences_list = [create_time_series_normal(number_of_validation_normal_1_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_normal_1_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_normal_1_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_normal_1_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_normal_1_sequences_array.shape = \\n{}\".format(validation_normal_1_sequences_array.shape))\n",
    "\n",
    "validation_normal_2_sequences_list = [create_time_series_normal(number_of_validation_normal_2_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_normal_2_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_normal_2_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_normal_2_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_normal_2_sequences_array.shape = \\n{}\".format(validation_normal_2_sequences_array.shape))\n",
    "\n",
    "validation_anomalous_sequences_list = [create_time_series_with_anomaly(number_of_validation_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_anomalous_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_anomalous_sequences_array.shape = \\n{}\".format(validation_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_sequences_array.shape = \n",
      "(750, 3)\n",
      "test_anomalous_sequences_array.shape = \n",
      "(750, 3)\n"
     ]
    }
   ],
   "source": [
    "test_normal_sequences_list = [create_time_series_normal(number_of_test_normal_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "test_normal_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = test_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_test_normal_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"test_normal_sequences_array.shape = \\n{}\".format(test_normal_sequences_array.shape))\n",
    "\n",
    "test_anomalous_sequences_list = [create_time_series_with_anomaly(number_of_test_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "test_anomalous_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = test_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_test_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"test_anomalous_sequences_array.shape = \\n{}\".format(test_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = \"data/training_normal_sequences.csv\", X = training_normal_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/validation_normal_1_sequences.csv\", X = validation_normal_1_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/validation_normal_2_sequences.csv\", X = validation_normal_2_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/validation_anomalous_sequences.csv\", X = validation_anomalous_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/test_normal_sequences.csv\", X = test_normal_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/test_anomalous_sequences.csv\", X = test_anomalous_sequences_array, fmt = '%s', delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.36555436,-1.41363978,-1.3725824,-1.19917717,-1.19037995,-1.34530532,-1.28334924,-1.16440172,-1.13816264,-1.23500562,-1.27883018,-1.17064597,-1.24705575,-1.16377086,-1.18731101,-1.12763875,-1.08950841,-1.01890701,-1.10339493,-0.92257018,-0.93112761,-0.89866394,-0.9758921,-0.85413083,-0.77941187,-0.75448674,-0.85110952,-0.72441939,-0.68924724,-0.70399542;1.85584264,1.76763964,1.91164727,1.8859888,1.86109418,1.75942824,1.87054831,1.70726921,1.69433906,1.67905286,1.71494609,1.49483031,1.5005212,1.50835608,1.24726657,1.19896101,1.20560575,0.98545696,0.97683363,0.77159142,0.79991579,0.60226675,0.41424117,0.39039879,0.25682505,0.13599595,-0.12306467,-0.14737839,-0.27533379,-0.37843692;0.93288063,0.77580939,0.79054545,0.77186809,0.7307057,0.60462956,0.51500735,0.50598767,0.49170661,0.41343177,0.4422462,0.30066522,0.31046036,0.24030613,0.24261682,0.15028022,0.21987115,0.06817332,-0.00610797,0.08296967,-0.06800466,-0.16695265,-0.20395097,-0.23789672,-0.27638344,-0.24285765,-0.36555241,-0.29864256,-0.31616596,-0.40732529\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/training_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19149009,0.08749189,0.16966319,0.1425715,0.18482511,0.32189926,0.36364864,0.30604133,0.39576785,0.55342924,0.6008935,0.59388652,0.55601044,0.74141064,0.64942679,0.66113373,0.77428171,0.83583777,0.91909638,0.83031486,0.86805717,0.92867216,0.90424296,1.07527569,1.08842635,0.9976109,1.19573771,1.17531767,1.21340549,1.24844917;0.11256097,0.19227675,0.2931511,0.57214897,0.69752184,0.7575514,0.86643939,0.96282585,0.96888796,1.17370232,1.28909065,1.4213808,1.49693818,1.55955232,1.57701175,1.58025455,1.62874546,1.77022875,1.75686688,1.83860114,1.91322579,1.7997582,1.76879461,1.89656453,1.85780376,1.74112625,1.72853802,1.61449465,1.57717516,1.61190363;0.12378051,0.13789813,0.24062172,0.20004921,0.37855715,0.30037846,0.44564743,0.34305005,0.55966747,0.50851178,0.47824881,0.65658516,0.6425036,0.69712871,0.75309973,0.72849233,0.71744501,0.78443554,0.9065012,0.9046721,0.91646457,0.94715806,1.02045765,1.12481614,1.14948969,1.12104952,1.12691796,1.1261442,1.13475024,1.28992178\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_normal_1_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10097305,0.23040197,0.09037027,0.17636997,0.29455408,0.40865476,0.40059843,0.30088354,0.36089397,0.50597822,0.52218812,0.60036528,0.62433794,0.56680733,0.75296747,0.67467335,0.72862485,0.86680869,0.90274065,0.89177681,0.98007764,1.01115884,0.90821784,0.96294676,1.14229189,1.00682334,1.16402809,1.15331243,1.22405848,1.12129027;0.04608178,0.16505292,0.41733175,0.43237022,0.56162959,0.68451601,0.78455383,0.99909121,1.05487141,1.1527609,1.33855941,1.31780487,1.45355194,1.59628797,1.49349805,1.58980703,1.75806119,1.74107932,1.72367114,1.82013533,1.90745878,1.7888212,1.76481817,1.79514019,1.88473917,1.78406288,1.71468659,1.60129159,1.67548118,1.51508665;0.05325801,0.2452935,0.26158546,0.16884896,0.33385683,0.26121202,0.4104164,0.38697979,0.47838702,0.46535345,0.55645463,0.68280294,0.71796141,0.61799763,0.66640262,0.82129077,0.73952486,0.86504555,0.83746756,0.86998498,0.8947997,0.92263801,0.9466029,0.95744339,1.15618467,1.12344707,1.03134402,1.11210177,1.08591362,1.13104651\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_normal_2_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18266504,0.06292994,0.25617506,0.15821367,0.36892228,0.21691008,0.31791798,0.45200072,0.49135783,0.50769717,0.6026884,0.51974026,0.53789044,0.59048751,0.64405928,0.68947044,0.81914047,0.82635168,0.91931389,0.84830877,0.87095462,1.0204594,1.01232726,1.11112818,0.48808109,0.27756071,-0.3389819,0.98654402,0.11419942,1.56278525;0.00653996,0.28409502,0.38709219,0.41553415,0.58479973,0.65999534,0.74735556,0.98313291,1.12102802,1.1230949,1.25832357,1.44006234,1.43063136,1.45448021,1.64496818,1.56193693,1.79485169,1.79446112,1.72712626,1.84211247,1.85761307,1.89934109,1.82529834,1.90161553,0.48586619,2.34806786,0.29118378,-1.41421864,2.26888319,1.8235448;1.49831920e-03,9.70853227e-02,1.36073713e-01,1.87788556e-01,3.65771148e-01,4.09573072e-01,3.02981018e-01,4.36673096e-01,5.65378914e-01,4.65155874e-01,4.73183082e-01,6.04128286e-01,7.26875471e-01,6.97431591e-01,7.61014013e-01,7.80092375e-01,7.42551512e-01,8.17999708e-01,8.10544964e-01,8.92021458e-01,8.99760844e-01,1.07837014e+00,1.07476070e+00,9.54632071e-01,3.98261685e-01,8.54402670e-01,-5.50091946e-01,1.62068955e+00,-3.15858154e-01,1.52835722e+00\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_anomalous_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03656967,0.21730723,0.17355658,0.26213597,0.19518173,0.29395037,0.32058532,0.3905663,0.47307869,0.51317038,0.48897769,0.63873453,0.50928578,0.63942183,0.7723493,0.68074895,0.81305858,0.87244934,0.76219374,0.89755914,0.97157897,0.86340862,0.93576391,1.05601185,1.14798407,1.14537461,1.17988859,1.15687064,1.10172846,1.12210094;0.12149195,0.30843017,0.30880777,0.41933526,0.54375307,0.66864219,0.84588677,0.90464763,0.95858777,1.22889902,1.32070902,1.33652754,1.35591605,1.572199,1.60749496,1.60316684,1.61308405,1.73711536,1.72125633,1.76137173,1.72886126,1.89187642,1.85606651,1.78718969,1.75519211,1.7904792,1.7144414,1.66374494,1.69111995,1.53813193;0.15758857,0.24102667,0.23268655,0.32093008,0.19053705,0.35924416,0.47026588,0.39549879,0.55309325,0.59090015,0.62070197,0.63315848,0.56164048,0.75630491,0.63052224,0.73226301,0.82927782,0.77472108,0.97926752,0.82795602,0.91514331,1.04695162,1.1031617,1.00668232,1.05827612,1.03168804,1.22225223,1.20303345,1.09416207,1.2034583\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/test_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16379087,0.19250462,0.16026269,0.13415243,0.36386331,0.3333667,0.31101489,0.4821154,0.37087396,0.40769877,0.60613263,0.51533383,0.55589414,0.71809181,0.72188144,0.72990712,0.76997763,0.77686932,0.88694692,0.92604023,0.83370259,0.89401904,0.89335667,1.08233661,0.736593,2.26773166,-0.98623348,1.81139262,1.04083328,-0.67972807;0.13537194,0.13316796,0.33431896,0.39480013,0.51424638,0.79040563,0.89375504,0.87782183,1.12106491,1.20656491,1.33750451,1.44317318,1.50315652,1.44467803,1.65800671,1.63164214,1.66170565,1.64598512,1.8721291,1.82940348,1.81699258,1.93308982,1.92283341,1.76310533,0.7018551,1.20412931,-0.70779087,2.25442799,-1.31697256,2.49841269;0.06520762,0.20035668,0.18733888,0.33055015,0.31035264,0.25894005,0.3014322,0.4861972,0.39127955,0.52356987,0.61790581,0.67491451,0.67153426,0.62868148,0.68421534,0.74279935,0.76777177,0.75094181,0.79444194,0.82077651,0.91874674,0.90843229,0.97879154,1.01940148,0.63167922,0.97776383,1.30486228,0.08091435,-0.12967605,-0.10897343\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/test_anomalous_sequences.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging to be level of INFO\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "CSV_COLUMNS = tag_columns\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[\"\"], [\"\"], [\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    def _input_fn():\n",
    "#         print(\"\\nread_dataset: _input_fn: filename = \\n{}\".format(filename))\n",
    "#         print(\"read_dataset: _input_fn: mode = \\n{}\".format(mode))\n",
    "#         print(\"read_dataset: _input_fn: batch_size = \\n{}\".format(batch_size))\n",
    "#         print(\"read_dataset: _input_fn: params = \\n{}\\n\".format(params))\n",
    "\n",
    "        def decode_csv(value_column, sequence_length):\n",
    "            def convert_sequences_from_strings_to_floats(features):\n",
    "                def split_and_convert_string(string_tensor):\n",
    "                    # Split string tensor into a sparse tensor based on delimiter\n",
    "                    split_string = tf.string_split(source = tf.expand_dims(input = string_tensor, axis = 0), delimiter = \",\")\n",
    "#                     print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \\n{}\".format(split_string))\n",
    "\n",
    "                    # Converts the values of the sparse tensor to floats\n",
    "                    converted_tensor = tf.string_to_number(split_string.values, out_type = tf.float64)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \\n{}\".format(converted_tensor))\n",
    "\n",
    "                    # Create a new sparse tensor with the new converted values, because the original sparse tensor values are immutable\n",
    "                    new_sparse_tensor = tf.SparseTensor(indices = split_string.indices, values = converted_tensor, dense_shape = split_string.dense_shape)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \\n{}\".format(new_sparse_tensor))\n",
    "\n",
    "                    # Create a dense tensor of the float values that were converted from text csv\n",
    "                    dense_floats = tf.sparse_tensor_to_dense(sp_input = new_sparse_tensor, default_value = 0.0)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \\n{}\".format(dense_floats))\n",
    "\n",
    "                    dense_floats_vector = tf.squeeze(input = dense_floats, axis = 0)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \\n{}\\n\".format(dense_floats_vector))\n",
    "\n",
    "                    return dense_floats_vector\n",
    "                    \n",
    "#                 print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "                for column in CSV_COLUMNS:\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \\n{}\".format(column))\n",
    "                    features[column] = split_and_convert_string(features[column])\n",
    "                    features[column].set_shape([sequence_length])\n",
    "\n",
    "#                 print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "\n",
    "                return features\n",
    "                \n",
    "#             print(\"\\nread_dataset: _input_fn: decode_csv: value_column = \\n{}\".format(value_column))\n",
    "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS, field_delim = \";\")\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: columns = \\n{}\".format(columns))\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            features = convert_sequences_from_strings_to_floats(features)\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            return features\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename = filename)\n",
    "#         print(\"\\nread_dataset: _input_fn: file_list = \\n{}\".format(file_list))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(filenames = file_list)    # Read text file\n",
    "#         print(\"read_dataset: _input_fn: dataset.TextLineDataset(file_list) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Decode the CSV file into a features dictionary of tensors\n",
    "        dataset = dataset.map(map_func = lambda x: decode_csv(x, params[\"sequence_length\"]))\n",
    "#         print(\"read_dataset: _input_fn: dataset.map(decode_csv) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine amount of times to repeat file based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        # Repeat files num_epoch times\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "#         print(\"read_dataset: _input_fn: dataset.repeat(num_epochs) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Group the data into batches\n",
    "        dataset = dataset.batch(batch_size = batch_size)\n",
    "#         print(\"read_dataset: _input_fn: dataset.batch(batch_size) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine if we should shuffle based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "#             print(\"read_dataset: _input_fn: dataset.shuffle(buffer_size = 10 * batch_size) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Create a iterator and then pull the next batch of features from the example queue\n",
    "        batch_features = dataset.make_one_shot_iterator().get_next()\n",
    "#         print(\"read_dataset: _input_fn: batch_features = \\n{}\".format(batch_features))\n",
    "\n",
    "        return batch_features\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out_input_function(args):\n",
    "    with tf.Session() as sess:\n",
    "        fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"],\n",
    "          mode = tf.estimator.ModeKeys.EVAL,\n",
    "          batch_size = args[\"batch_size\"],\n",
    "          params = args)\n",
    "\n",
    "        features = sess.run(fn())\n",
    "        print(\"try_out_input_function: features = {}\".format(features))\n",
    "\n",
    "        print(\"try_out_input_function: features[tag_0].shape = {}\".format(features[\"tag_0\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_out_input_function(args = arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model function to be used in our custom estimator\n",
    "def encoder_decoder_stacked_lstm_autoencoder(features, labels, mode, params):\n",
    "#     print(\"\\nencoder_decoder_stacked_lstm_autoencoder: features = \\n{}\".format(features))\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: labels = \\n{}\".format(labels))\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: mode = \\n{}\".format(mode))\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: params = \\n{}\".format(params))\n",
    "\n",
    "    # 0. Get input sequence tensor into correct shape\n",
    "    # Get dynamic batch size in case there was a partially filled batch\n",
    "    current_batch_size = tf.shape(input = features[CSV_COLUMNS[0]], out_type = tf.int32)[0]\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: current_batch_size = \\n{}\".format(current_batch_size))\n",
    "\n",
    "    # Get the number of features \n",
    "    number_of_features = len(CSV_COLUMNS)\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: number_of_features = \\n{}\".format(number_of_features))\n",
    "\n",
    "    # Stack all of the features into a 3-D tensor\n",
    "    X = tf.stack(values = list(features.values()), axis = 2) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: X = \\n{}\".format(X))\n",
    "\n",
    "    # Unstack all of 3-D features tensor into a sequence(list) of 2-D tensors of shape = (current_batch_size, number_of_features)\n",
    "    X_sequence = tf.unstack(value = X, num = params[\"sequence_length\"], axis = 1)\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: X_sequence = \\n{}\".format(X_sequence))\n",
    "\n",
    "    # Since this is an autoencoder, the features are the labels. It works better though to have the labels in reverse order\n",
    "    if params[\"reverse_labels_sequence\"] == True:\n",
    "        Y = tf.reverse_sequence(input = X,  # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "                                seq_lengths = tf.tile(input = tf.constant(value = [params[\"sequence_length\"]], dtype = tf.int64), \n",
    "                                                      multiples = tf.expand_dims(input = current_batch_size, axis = 0)), \n",
    "                                seq_axis = 1, \n",
    "                                batch_axis = 0)\n",
    "    else:\n",
    "        Y = X  # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: Y = \\n{}\".format(Y))\n",
    "  \n",
    "  ################################################################################\n",
    "  \n",
    "    # 1. Create encoder of encoder-decoder LSTM stacks\n",
    "    def create_LSTM_stack(lstm_hidden_units, lstm_dropout_output_keep_probs):\n",
    "        # First create a list of LSTM cells using our list of lstm hidden unit sizes\n",
    "        lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units = units, forget_bias = 1.0, state_is_tuple = True) for units in lstm_hidden_units] # list of LSTM cells\n",
    "#         print(\"\\nencoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: lstm_cells = \\n{}\".format(lstm_cells))\n",
    "\n",
    "        # Next apply a dropout wrapper to our stack of LSTM cells, in this case just on the outputs\n",
    "        dropout_lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(cell = lstm_cells[cell_index], \n",
    "                                                            input_keep_prob = 1.0, \n",
    "                                                            output_keep_prob = lstm_dropout_output_keep_probs[cell_index], \n",
    "                                                            state_keep_prob = 1.0) for cell_index in range(len(lstm_cells))]\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: dropout_lstm_cells = \\n{}\".format(dropout_lstm_cells))\n",
    "\n",
    "        # Create a stack of layers of LSTM cells\n",
    "        stacked_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells = dropout_lstm_cells, state_is_tuple = True) # combines list into MultiRNNCell object\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: create_LSTM_stack: stacked_lstm_cells = \\n{}\\n\".format(stacked_lstm_cells))\n",
    "\n",
    "        return stacked_lstm_cells\n",
    "  \n",
    "    # Create our decoder now\n",
    "    decoder_stacked_lstm_cells = create_LSTM_stack(params[\"decoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_stacked_lstm_cells = \\n{}\".format(decoder_stacked_lstm_cells))\n",
    "  \n",
    "    # Create the encoder variable scope\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        # Create separate encoder cells with their own weights separate from decoder\n",
    "        encoder_stacked_lstm_cells = create_LSTM_stack(params[\"encoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_stacked_lstm_cells = {}\".format(encoder_stacked_lstm_cells))\n",
    "\n",
    "        # Encode the input sequence using our encoder stack of LSTMs\n",
    "        encoder_outputs, encoder_states = tf.nn.static_rnn(cell = encoder_stacked_lstm_cells, \n",
    "                                                           inputs = X_sequence, \n",
    "                                                           initial_state = encoder_stacked_lstm_cells.zero_state(batch_size = current_batch_size, dtype = tf.float64), \n",
    "                                                           dtype = tf.float64)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_outputs = \\n{}\".format(encoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_states = \\n{}\".format(encoder_states)) # tuple of final encoder c_state and h_state for each layer\n",
    "\n",
    "        # We just pass on the final c and h states of the encoder\"s last layer, so extract that and drop the others\n",
    "        encoder_final_states = encoder_states[-1]\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_final_states = \\n{}\".format(encoder_final_states))\n",
    "\n",
    "        # Extract the c and h states from the tuple\n",
    "        encoder_final_c, encoder_final_h = encoder_final_states\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_final_c = \\n{}\".format(encoder_final_c))\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: encoder_final_h = \\n{}\".format(encoder_final_h))\n",
    "\n",
    "        # In case the decoder\"s first layer\"s number of units is different than encoder\"s last layer\"s number of units, use a dense layer to map to the correct shape\n",
    "        encoder_final_c_dense = tf.layers.dense(inputs = encoder_final_c, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "        encoder_final_h_dense = tf.layers.dense(inputs = encoder_final_h, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "\n",
    "        # The decoder\"s first layer\"s state comes from the encoder, the rest of the layers\" initial states are zero\n",
    "        decoder_intial_states = tuple([tf.contrib.rnn.LSTMStateTuple(c = encoder_final_c_dense, h = encoder_final_h_dense)] + [tf.contrib.rnn.LSTMStateTuple(c = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64), h = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64)) for units in params[\"decoder_lstm_hidden_units\"][1:]])\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_intial_states = \\n{}\".format(decoder_intial_states))\n",
    "    \n",
    "    ################################################################################\n",
    "\n",
    "    # 2. Create decoder of encoder-decoder LSTM stacks\n",
    "    # The rnn_decoder function takes labels during TRAIN/EVAL and a start token followed by its previous predictions during PREDICT\n",
    "    # Starts with an intial state of the final encoder states\n",
    "    def rnn_decoder(decoder_inputs, initial_state, cell, inference):\n",
    "        # Create the decoder variable scope\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # Load in our initial state from our encoder\n",
    "            state = initial_state # tuple of final encoder c_state and h_state of final encoder layer\n",
    "#             print(\"\\nencoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \\n{}\".format(state))\n",
    "            \n",
    "            # Create an empty list to store our hidden state output for every timestep\n",
    "            outputs = []\n",
    "            \n",
    "            # Begin with no previous output\n",
    "            previous_output = None\n",
    "            \n",
    "            # Loop over all of our decoder_inputs which will be sequence_length long\n",
    "            for index, decoder_input in enumerate(decoder_inputs):\n",
    "                # If there has been a previous output then we will determine the next input\n",
    "                if previous_output is not None:\n",
    "                    # Create the input layer to our DNN\n",
    "                    network = previous_output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#                     print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = \\n{}\".format(network))\n",
    "                    \n",
    "                    # Create our dnn variable scope\n",
    "                    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "                        # Add hidden layers with the given number of units/neurons per layer\n",
    "                        for units in params[\"dnn_hidden_units\"]:\n",
    "                            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size, dnn_hidden_units[i])\n",
    "#                             print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: network = {}, units = {}\".format(network, units))\n",
    "                            \n",
    "                        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "                        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size, number_of_features)\n",
    "#                         print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: logits = \\n{}\\n\".format(logits))\n",
    "                    \n",
    "                    # If we are in inference then we will overwrite our next decoder_input with the logits we just calculated.\n",
    "                    # Otherwise, we leave the decoder_input input as it was from the enumerated list\n",
    "                    # We have to calculate the logits even when not using them so that the correct dnn subgraph will be generated here and after the encoder-decoder for both training and inference\n",
    "                    if inference == True:\n",
    "                        decoder_input = logits # shape = (current_batch_size, number_of_features)\n",
    "\n",
    "#                     print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: decoder_input = \\n{}\\n\".format(decoder_input))\n",
    "                \n",
    "                # If this isn\"t our first time through the loop, just reuse(share) the same variables for each iteration within the current variable scope\n",
    "                if index > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                # Run the decoder input through the decoder stack picking up from the previous state\n",
    "                output, state = cell(decoder_input, state)\n",
    "#                 print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: output = \\n{}\".format(output)) # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#                 print(\"encoder_decoder_stacked_lstm_autoencoder: rnn_decoder: state = \\n{}\".format(state)) # tuple of final decoder c_state and h_state\n",
    "                \n",
    "                # Append the current decoder hidden state output to the outputs list\n",
    "                outputs.append(output) # growing list eventually sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "                \n",
    "                # Set the previous output to the output just calculated\n",
    "                previous_output = output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "        return outputs, state\n",
    "  \n",
    "    # Train our decoder now\n",
    "  \n",
    "    # Encoder-decoders work differently during training/evaluation and inference so we will have two separate subgraphs for each\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN    or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # Break 3-D labels tensor into a list of 2-D tensors\n",
    "        unstacked_labels = tf.unstack(value = Y, num = params[\"sequence_length\"], axis = 1) # list of sequence_length long of shape = (current_batch_size, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: unstacked_labels = \\n{}\".format(unstacked_labels))\n",
    "\n",
    "        # Call our decoder using the labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to false\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = unstacked_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = False)\n",
    "    else:\n",
    "        # Since this is inference create fake labels. The list length needs to be the output sequence length even though only the first element is actually used (as our go signal)\n",
    "        fake_labels = [tf.zeros(shape = [current_batch_size, number_of_features], dtype = tf.float64) for _ in range(params[\"sequence_length\"])]\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: fake_labels = \\n{}\".format(fake_labels))\n",
    "        \n",
    "        # Call our decoder using fake labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to true\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = fake_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = True)\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_outputs = \\n{}\".format(decoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: decoder_states = \\n{}\".format(decoder_states)) # tuple of final decoder c_state and h_state\n",
    "    \n",
    "    # Stack together the list of rank 2 decoder output tensors into one rank 3 tensor\n",
    "    stacked_decoder_outputs = tf.stack(values = decoder_outputs, axis = 1) # shape = (current_batch_size, sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: stacked_decoder_outputs = \\n{}\".format(stacked_decoder_outputs))\n",
    "    \n",
    "    # Reshape rank 3 decoder outputs into rank 2 by folding sequence length into batch size\n",
    "    reshaped_stacked_decoder_outputs = tf.reshape(tensor = stacked_decoder_outputs, shape = [current_batch_size * params[\"sequence_length\"], params[\"decoder_lstm_hidden_units\"][-1]]) # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: reshaped_stacked_decoder_outputs = \\n{}\".format(reshaped_stacked_decoder_outputs))\n",
    "\n",
    "    ################################################################################\n",
    "    \n",
    "    # 3. Create the DNN structure now after the encoder-decoder LSTM stack\n",
    "    # Create the input layer to our DNN\n",
    "    network = reshaped_stacked_decoder_outputs # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: network = \\n{}\".format(network))\n",
    "    \n",
    "    # Reuse the same variable scope as we used within our decoder (for inference)\n",
    "    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "        # Add hidden layers with the given number of units/neurons per layer\n",
    "        for units in params[\"dnn_hidden_units\"]:\n",
    "            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size * sequence_length, dnn_hidden_units[i])\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: network = {}, units = {}\".format(network, units))\n",
    "\n",
    "        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: logits = \\n{}\".format(logits))\n",
    "    \n",
    "    # Now that we are through the final DNN for each sequence element for each example in the batch, reshape the predictions to match our labels\n",
    "    predictions = tf.reshape(tensor = logits, shape = [current_batch_size, params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"encoder_decoder_stacked_lstm_autoencoder: predictions = \\n{}\".format(predictions))\n",
    "    \n",
    "    with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        absolute_error_mean_batch_time_variable = tf.get_variable(name = \"absolute_error_mean_batch_time_variable\", # shape = (number_of_features,)\n",
    "                                                                  shape = [number_of_features],\n",
    "                                                                  dtype = tf.float64,\n",
    "                                                                  trainable = False)\n",
    "\n",
    "        absolute_error_inverse_covariance_matrix_batch_time_variable = tf.get_variable(name = \"absolute_error_inverse_covariance_matrix_batch_time_variable\", # shape = (number_of_features, number_of_features)\n",
    "                                                                                       shape = [number_of_features, number_of_features],\n",
    "                                                                                       dtype = tf.float64,\n",
    "                                                                                       trainable = False)\n",
    "\n",
    "        absolute_error_mean_batch_features_variable = tf.get_variable(name = \"absolute_error_mean_batch_features_variable\", # shape = (sequence_length,)\n",
    "                                                                  shape = [params[\"sequence_length\"]],\n",
    "                                                                  dtype = tf.float64,\n",
    "                                                                  trainable = False)\n",
    "\n",
    "        absolute_error_inverse_covariance_matrix_batch_features_variable = tf.get_variable(name = \"absolute_error_inverse_covariance_matrix_batch_features_variable\", # shape = (sequence_length, sequence_length)\n",
    "                                                                                   shape = [params[\"sequence_length\"], params[\"sequence_length\"]],\n",
    "                                                                                   dtype = tf.float64,\n",
    "                                                                                   trainable = False)\n",
    "    \n",
    "    # Now branch off based on which mode we are in\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "    \n",
    "    # 3. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        \n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step(),\n",
    "            learning_rate = params[\"learning_rate\"],\n",
    "            optimizer = \"Adam\")\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        eval_metric_ops = {\n",
    "            \"rmse\": tf.metrics.root_mean_squared_error(labels = Y, predictions = predictions),\n",
    "            \"mae\": tf.metrics.mean_absolute_error(labels = Y, predictions = predictions)\n",
    "        }\n",
    "\n",
    "        absolute_error = tf.abs(x = Y - predictions) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error = \\n{}\".format(absolute_error))\n",
    "\n",
    "        # Time based\n",
    "        absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error, shape = [current_batch_size * params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "\n",
    "        absolute_error_mean_batch_time = tf.reduce_mean(input_tensor = absolute_error_reshaped_batch_time, axis = 0) # shape = (current_batch_size * sequence_length,)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_mean_batch_time = \\n{}\".format(absolute_error_mean_batch_time))\n",
    "\n",
    "        absolute_error_reshaped_batch_time_centered = absolute_error_reshaped_batch_time - absolute_error_mean_batch_time # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_time_centered = \\n{}\".format(absolute_error_reshaped_batch_time_centered))\n",
    "\n",
    "        absolute_error_reshaped_batch_time_covariance_matrix = tf.matmul(a = absolute_error_reshaped_batch_time_centered, # shape = (number_of_features, number_of_features)\n",
    "                                                                         b = absolute_error_reshaped_batch_time_centered, \n",
    "                                                                         transpose_a = True) / tf.cast(x = current_batch_size * params[\"sequence_length\"] - 1, dtype = tf.float64)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_time_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_time_covariance_matrix))\n",
    "\n",
    "        absolute_error_reshaped_batch_time_inverse_covariance_matrix = tf.matrix_inverse(input = absolute_error_reshaped_batch_time_covariance_matrix) # shape = (number_of_features, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_time_inverse_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_time_inverse_covariance_matrix))\n",
    "\n",
    "        # Features based\n",
    "        absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                           elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int32), \n",
    "                                                           dtype = tf.float64)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "        absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, shape = [current_batch_size * number_of_features, params[\"sequence_length\"]]) # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "\n",
    "        absolute_error_mean_batch_features = tf.reduce_mean(input_tensor = absolute_error_reshaped_batch_features, axis = 0) # shape = (sequence_length,)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_mean_batch_features = \\n{}\".format(absolute_error_mean_batch_features))\n",
    "\n",
    "        absolute_error_reshaped_batch_features_centered = absolute_error_reshaped_batch_features - absolute_error_mean_batch_features # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_features_centered = \\n{}\".format(absolute_error_reshaped_batch_features_centered))\n",
    "\n",
    "        absolute_error_reshaped_batch_features_covariance_matrix = tf.matmul(a = absolute_error_reshaped_batch_features_centered, # shape = (sequence_length, sequence_length)\n",
    "                                                                             b = absolute_error_reshaped_batch_features_centered, \n",
    "                                                                             transpose_a = True) / tf.cast(x = current_batch_size * number_of_features - 1, dtype = tf.float64)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_features_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_features_covariance_matrix))\n",
    "\n",
    "        absolute_error_reshaped_batch_features_inverse_covariance_matrix = tf.matrix_inverse(input = absolute_error_reshaped_batch_features_covariance_matrix) # shape = (sequence_length, sequence_length)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_features_inverse_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_features_inverse_covariance_matrix))\n",
    "\n",
    "        with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "            absolute_error_mean_batch_time_variable.assign(value = absolute_error_mean_batch_time) # shape = (number_of_features,)\n",
    "            absolute_error_inverse_covariance_matrix_batch_time_variable.assign(value = absolute_error_reshaped_batch_time_inverse_covariance_matrix) # shape = (number_of_features, number_of_features)\n",
    "            absolute_error_mean_batch_features_variable.assign(value = absolute_error_mean_batch_features) # shape = (sequence_length,)\n",
    "            absolute_error_inverse_covariance_matrix_batch_features_variable.assign(value = absolute_error_reshaped_batch_features_inverse_covariance_matrix) # shape = (sequence_length, sequence_length)\n",
    "    else: # mode == tf.estimator.ModeKeys.PREDICT\n",
    "        def mahalanobis_distance(error_vectors_reshaped, mean_vector, inverse_covariance_matrix, final_shape):\n",
    "            error_vectors_reshaped_centered = error_vectors_reshaped - mean_vector # time_shape = (current_batch_size * sequence_length, number_of_features), features_shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance: error_vectors_reshaped_centered = \\n{}\".format(error_vectors_reshaped_centered))\n",
    "\n",
    "            mahalanobis_right_matrix_product = tf.matmul(a = inverse_covariance_matrix, # time_shape = (number_of_features, current_batch_size * sequence_length), features_shape = (sequence_length, current_batch_size * number_of_features)\n",
    "                                                         b = error_vectors_reshaped_centered,\n",
    "                                                         transpose_b = True)\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance: mahalanobis_right_matrix_product = \\n{}\".format(mahalanobis_right_matrix_product))\n",
    "\n",
    "\n",
    "            mahalanobis_distance_vectorized = tf.matmul(a = error_vectors_reshaped_centered, # time_shape = (current_batch_size * sequence_length, current_batch_size * sequence_length), features_shape = (current_batch_size * number_of_features, current_batch_size * number_of_features)\n",
    "                                                        b = mahalanobis_right_matrix_product)\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance: mahalanobis_distance_vectorized = \\n{}\".format(mahalanobis_distance_vectorized))\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance: mahalanobis_distance_vectorized.shape = \\n{}\".format(mahalanobis_distance_vectorized.shape))\n",
    "\n",
    "            mahalanobis_distance_flat = tf.diag_part(input = mahalanobis_distance_vectorized) # time_shape = (current_batch_size * sequence_length,), features_shape = (current_batch_size * number_of_features,)\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance: mahalanobis_distance_flat = \\n{}\".format(mahalanobis_distance_flat))\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance: mahalanobis_distance_flat.shape = \\n{}\".format(mahalanobis_distance_flat.shape))\n",
    "\n",
    "            mahalanobis_distance_final_shaped = tf.reshape(tensor = mahalanobis_distance_flat, shape = [-1, final_shape]) # time_shape = (current_batch_size, sequence_length), features_shape = (current_batch_size, number_of_features)\n",
    "\n",
    "            return mahalanobis_distance_final_shaped\n",
    "          \n",
    "        absolute_error = tf.abs(x = Y - predictions) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#         print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error = \\n{}\".format(absolute_error))\n",
    "        \n",
    "        with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error,  # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "                                                            shape = [current_batch_size * params[\"sequence_length\"], number_of_features])\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "\n",
    "            mahalanobis_distance_batch_time = mahalanobis_distance(error_vectors_reshaped = absolute_error_reshaped_batch_time,  # shape = (current_batch_size, sequence_length)\n",
    "                                                                   mean_vector = absolute_error_mean_batch_time_variable, \n",
    "                                                                   inverse_covariance_matrix = absolute_error_inverse_covariance_matrix_batch_time_variable, \n",
    "                                                                   final_shape = params[\"sequence_length\"])\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_batch_time = \\n{}\".format(mahalanobis_distance_batch_time))\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_batch_time.shape = \\n{}\".format(mahalanobis_distance_batch_time.shape))\n",
    "\n",
    "            # Features based\n",
    "            absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                             elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int32), \n",
    "                                                             dtype = tf.float64)\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "\n",
    "            absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "                                                                shape = [current_batch_size * number_of_features, params[\"sequence_length\"]])\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "\n",
    "            mahalanobis_distance_batch_features = mahalanobis_distance(error_vectors_reshaped = absolute_error_reshaped_batch_features, # shape = (current_batch_size, number_of_features)\n",
    "                                                                       mean_vector = absolute_error_mean_batch_features_variable, \n",
    "                                                                       inverse_covariance_matrix = absolute_error_inverse_covariance_matrix_batch_features_variable,\n",
    "                                                                       final_shape = number_of_features)\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_batch_features = \\n{}\".format(mahalanobis_distance_batch_features))\n",
    "#             print(\"encoder_decoder_stacked_lstm_autoencoder: mahalanobis_distance_batch_features.shape = \\n{}\".format(mahalanobis_distance_batch_features.shape))\n",
    "            \n",
    "        batch_time_anomaly_flags = tf.where(condition = tf.reduce_any(input_tensor = tf.greater(x = tf.abs(x = mahalanobis_distance_batch_time), # shape = (current_batch_size,)\n",
    "                                                                                                y = params[\"time_anomaly_threshold\"]), \n",
    "                                                                      axis = 1), \n",
    "                                            x = tf.ones(shape = [current_batch_size], dtype = tf.int32), \n",
    "                                            y = tf.zeros(shape = [current_batch_size], dtype = tf.int32))\n",
    "        \n",
    "        batch_features_anomaly_flags = tf.where(condition = tf.reduce_any(input_tensor = tf.greater(x = tf.abs(x = mahalanobis_distance_batch_features), # shape = (current_batch_size,)\n",
    "                                                                                                    y = params[\"features_anomaly_threshold\"]), \n",
    "                                                                          axis = 1), \n",
    "                                                x = tf.ones(shape = [current_batch_size], dtype = tf.int32), \n",
    "                                                y = tf.zeros(shape = [current_batch_size], dtype = tf.int32))\n",
    "        \n",
    "        # Create predictions\n",
    "        predictions_dict = {\"predictions\": predictions, \n",
    "                            \"mahalanobis_distance_batch_time\": mahalanobis_distance_batch_time, \n",
    "                            \"mahalanobis_distance_batch_features\": mahalanobis_distance_batch_features, \n",
    "                            \"batch_time_anomaly_flags\": batch_time_anomaly_flags, \n",
    "                            \"batch_features_anomaly_flags\": batch_features_anomaly_flags}\n",
    "\n",
    "        # Create export outputs\n",
    "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = predictions_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our serving input function to accept the data at serving and send it in the right format to our custom estimator\n",
    "def serving_input_fn(sequence_length):\n",
    "    # This function fixes the shape and type of our input strings\n",
    "    def fix_shape_and_type_for_serving(placeholder):\n",
    "        current_batch_size = tf.shape(input = placeholder, out_type = tf.int64)[0]\n",
    "        \n",
    "        # String split each string in the batch and output the values from the resulting SparseTensors\n",
    "        split_string = tf.stack(values = tf.map_fn( # shape = (batch_size, sequence_length)\n",
    "            fn = lambda x: tf.string_split(source = [placeholder[x]], delimiter = ',').values, \n",
    "            elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "            dtype = tf.string), axis = 0)\n",
    "#         print(\"serving_input_fn: fix_shape_and_type_for_serving: split_string = {}\".format(split_string))\n",
    "        \n",
    "        # Convert each string in the split tensor to float\n",
    "        feature_tensor = tf.string_to_number(string_tensor = split_string, out_type = tf.float64) # shape = (batch_size, sequence_length)\n",
    "#         print(\"serving_input_fn: fix_shape_and_type_for_serving: feature_tensor = {}\".format(feature_tensor))\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    # This function fixes dynamic shape ambiguity of last dimension so that we will be able to use it in our DNN (since tf.layers.dense require the last dimension to be known)\n",
    "    def get_shape_and_set_modified_shape_2D(tensor, additional_dimension_sizes):\n",
    "        # Get static shape for tensor and convert it to list\n",
    "        shape = tensor.get_shape().as_list()\n",
    "        # Set outer shape to additional_dimension_sizes[0] since we know that this is the correct size\n",
    "        shape[1] = additional_dimension_sizes[0]\n",
    "        # Set the shape of tensor to our modified shape\n",
    "        tensor.set_shape(shape = shape) # shape = (batch_size, additional_dimension_sizes[0])\n",
    "#         print(\"serving_input_fn: get_shape_and_set_modified_shape_2D: tensor = {}, additional_dimension_sizes = {}\".format(tensor, additional_dimension_sizes))\n",
    "        return tensor\n",
    "            \n",
    "    # Create placeholders to accept the data sent to the model at serving time\n",
    "    feature_placeholders = { # all features come in as a batch of strings, shape = (batch_size,), this was so because of passing the arrays to online ml-engine prediction\n",
    "        feature: tf.placeholder(dtype = tf.string, shape = [None]) for feature in CSV_COLUMNS\n",
    "    }\n",
    "#     print(\"\\nserving_input_fn: feature_placeholders = {}\".format(feature_placeholders))\n",
    "    \n",
    "    # Create feature tensors\n",
    "    features = {key: fix_shape_and_type_for_serving(placeholder = tensor) for key, tensor in feature_placeholders.items()}\n",
    "#     print(\"serving_input_fn: features = {}\".format(features))\n",
    "    \n",
    "    # Fix dynamic shape ambiguity of feature tensors for our DNN\n",
    "    features = {key: get_shape_and_set_modified_shape_2D(tensor = tensor, additional_dimension_sizes = [sequence_length]) for key, tensor in features.items()}\n",
    "#     print(\"serving_input_fn: features = {}\".format(features))\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(args):\n",
    "    # Create our custome estimator using our model function\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn = encoder_decoder_stacked_lstm_autoencoder,\n",
    "        model_dir = args['output_dir'],\n",
    "        params = {\n",
    "            \"sequence_length\": args[\"sequence_length\"],\n",
    "            \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "            \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "            \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "            \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "            \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "            \"learning_rate\": args[\"learning_rate\"],\n",
    "            \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "            \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "    \n",
    "    early_stopping_hook = tf.contrib.estimator.stop_if_no_decrease_hook(\n",
    "        estimator = estimator,\n",
    "        metric_name = \"rmse\",\n",
    "        max_steps_without_decrease = 100,\n",
    "        min_steps = 1000,\n",
    "        run_every_secs = 60,\n",
    "        run_every_steps = None)\n",
    "    \n",
    "    # Create train spec to read in our training data\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args[\"train_file_pattern\"],\n",
    "            mode = tf.estimator.ModeKeys.TRAIN, \n",
    "            batch_size = args[\"train_batch_size\"],\n",
    "            params = args),\n",
    "        max_steps = args[\"train_steps\"], \n",
    "        hooks = [early_stopping_hook])\n",
    "    \n",
    "    # Create exporter to save out the complete model to disk\n",
    "    exporter = tf.estimator.BestExporter(\n",
    "        name = \"best_exporter\", \n",
    "        serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]),\n",
    "        exports_to_keep = 5)\n",
    "    \n",
    "    # Create eval spec to read in our validation data and export our model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(\n",
    "            filename = args[\"eval_file_pattern\"], \n",
    "            mode = tf.estimator.ModeKeys.EVAL, \n",
    "            batch_size = args[\"eval_batch_size\"],\n",
    "            params = args),\n",
    "        steps = None,\n",
    "        start_delay_secs = args[\"start_delay_secs\"], # start evaluating after N seconds\n",
    "        throttle_secs = args[\"throttle_secs\"],    # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "    \n",
    "    # Create train and evaluate loop to train and evaluate our estimator\n",
    "    tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"train_file_pattern\"] = \"data/training_normal_sequences.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/validation_normal_1_sequences.csv\"\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Sequence shape hyperparameters\n",
    "arguments[\"sequence_length\"] = sequence_length\n",
    "arguments[\"horizon\"] = 0\n",
    "arguments[\"reverse_labels_sequence\"] = True\n",
    "\n",
    "# Architecture hyperparameters\n",
    "\n",
    "# LSTM hyperparameters\n",
    "arguments[\"encoder_lstm_hidden_units\"] = [64, 32, 16]\n",
    "arguments[\"decoder_lstm_hidden_units\"] = [16, 32, 64]\n",
    "arguments[\"lstm_dropout_output_keep_probs\"] = [1.0, 1.0, 1.0]\n",
    "\n",
    "# DNN hyperparameters\n",
    "arguments[\"dnn_hidden_units\"] = [1024, 256, 64]\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_batch_size\"] = 7\n",
    "arguments[\"eval_batch_size\"] = 13\n",
    "arguments[\"train_steps\"] = 100\n",
    "arguments[\"learning_rate\"] = 0.01\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120\n",
    "\n",
    "# Anomaly thresholds\n",
    "arguments[\"time_anomaly_threshold\"] = 2.0\n",
    "arguments[\"features_anomaly_threshold\"] = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_is_chief': True, '_task_id': 0, '_save_checkpoints_steps': None, '_master': '', '_num_ps_replicas': 0, '_save_summary_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fea44099748>, '_device_fn': None, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_experimental_distribute': None, '_eval_distribute': None, '_model_dir': 'trained_model', '_train_distribute': None, '_tf_random_seed': None, '_save_checkpoints_secs': 600, '_evaluation_master': '', '_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_protocol': None, '_num_worker_replicas': 1}\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-26-b4cdc03033a2>:41: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-26-b4cdc03033a2>:52: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-26-b4cdc03033a2>:71: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From <ipython-input-26-b4cdc03033a2>:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1224471, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 100 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-16T05:54:49Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-16-05:54:52\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 0.041062433, mae = 0.15756865, rmse = 0.20284481\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Loading best metric from event files.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Loss for final step: 0.04707972.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "shutil.rmtree(path = arguments[\"output_dir\"], ignore_errors = True) # start fresh each time\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "estimator.evaluate(\n",
    "    input_fn = read_dataset(\n",
    "        filename = arguments[\"eval_file_pattern\"], \n",
    "        mode = tf.estimator.ModeKeys.EVAL, \n",
    "        batch_size = 2**16,\n",
    "        params = args),\n",
    "    steps = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/lstm_autoencoder_anomaly_detection_module\n",
    "python -m trainer.task \\\n",
    "    --train_file_pattern=\"data/training_normal_sequences.csv\" \\\n",
    "    --eval_file_pattern=\"data/validation_normal_1_sequences.csv\"  \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --batch_size=32 \\\n",
    "    --sequence_length=13 \\\n",
    "    --horizon=0 \\\n",
    "    --reverse_labels_sequence=True \\\n",
    "    --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "    --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "    --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "    --dnn_hidden_units=\"1024 256 64\" \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.1 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r data gs://$BUCKET/lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/trained_model\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/training_normal_sequences.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/validation_normal_1_sequences.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --batch_size=32 \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "  --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "  --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "  --dnn_hidden_units=\"1024 256 64\" \\\n",
    "  --train_steps=1000 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: mae\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 1\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: sequence_length\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 120\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: encoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: decoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: lstm_dropout_output_keep_probs\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"0.9 1.0 1.0\", \"0.95 0.95 1.0\", \"0.95 0.95 0.95\"]\n",
    "    - parameterName: dnn_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"256 128 64\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/hyperparam\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET/lstm_autoencoder/staging \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/train.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/eval.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r trained_model gs://qwiklabs-gcp-8923d4964bfbd247-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"lstm_autoencoder_anomaly_detection\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://$BUCKET/trained_model/export/exporter/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create $MODEL_NAME --regions $REGION\n",
    "gcloud ml-engine versions create $MODEL_VERSION --model $MODEL_NAME --origin $MODEL_LOCATION --runtime-version 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_instances = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local prediction from local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sequences.json', 'w') as outfile:\n",
    "  test_data_anomalous_string_list = [[np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list] for _ in range(0, number_of_prediction_instances)]\n",
    "  json_string = \"\"\n",
    "  for item in test_data_anomalous_string_list:\n",
    "    json_string += \"{\" + ','.join([\"{0}: \\\"{1}\\\"\".format('\\\"' + CSV_COLUMNS[i] + '\\\"', item[i]) for i in range(0, len(CSV_COLUMNS))]) + \"}\\n\"\n",
    "  json_string = json_string.replace(' ', '').replace(':', ': ').replace(',', ', ')\n",
    "  outfile.write(\"%s\" % json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_FEATURES_ANOMALY_FLAGS  BATCH_TIME_ANOMALY_FLAGS  MAHALANOBIS_DISTANCE_BATCH_FEATURES                            MAHALANOBIS_DISTANCE_BATCH_TIME                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     PREDICTIONS\n",
      "0                             0                         [0.3609838334886698, 4.285870630025189, 7.331315729850488]     [11.200225051015526, 6.302422447857527, 7.133295460598505, 3.6845064928930844, 1.1992155852273427, -0.5447650456691467, 1.9361679857458611, 2.5135337828994233, 3.3670350134756912, 2.801834665525715, 3.2638614318900023, 3.541391080605428, 3.664099186170625, 3.416207555117292, 2.564566278260998, 3.098551595086743, 2.8757662326783655, 2.944446882900374, 2.5923076746097533, 2.1807012131847516, 2.0858074862031426, 1.9917409626733913, 1.62776007937862, 1.8516518120374328, 1.6187862158429356, 1.3506691819893726, 1.4266776508038121, 0.916245277689105, 1.1588153124928775, 0.6932429883551378]       [[0.40790831607535405, 0.1314709457844174, 0.5576687073811238], [0.43602587445659946, 0.17757222683221155, 0.6069015880124863], [0.3512127548797239, 0.21285888671972972, 0.5175151726411527], [0.1909099143044662, 0.21988556982960333, 0.34479207136292733], [0.040418973163793795, 0.17753858364928443, 0.1429725034309734], [-0.0509568655966606, 0.09194563852431625, 0.004355091659595883], [-0.08330783203938308, 0.016570912079525543, -0.09101288320166176], [-0.06881216601098997, -0.029769675029759583, -0.14710909735112845], [-0.02545422735480067, -0.054139099720379724, -0.17432250347661074], [0.026162005532254992, -0.06318016409946059, -0.18271081384344537], [0.07035114897109203, -0.06326862594010416, -0.17972862965423084], [0.10298317583567794, -0.06115432572093968, -0.17086195101100035], [0.12275486068196033, -0.059759529818670276, -0.15992576179039994], [0.1325446447021621, -0.059709927281351854, -0.14905565544376917], [0.13507058867958704, -0.060570428974457, -0.1395431427826245], [0.1333047929537547, -0.06186325450600283, -0.13174619248597638], [0.12957501972301144, -0.06311568963235759, -0.12568779032052324], [0.12543274141445465, -0.06405043521587322, -0.12118708780378004], [0.12173961642047527, -0.06459182164907215, -0.11800065300607823], [0.11885158671119699, -0.06477537813727713, -0.11585305316272268], [0.11683186575243774, -0.06469143568155641, -0.11448686363663177], [0.11556024402109737, -0.0644478028583449, -0.11369037975106483], [0.11484933828459688, -0.06413491124649368, -0.11329287081646916], [0.11451330110251758, -0.06381515455948505, -0.11315895852080007], [0.11439953940226548, -0.06352461598267614, -0.11318362323599965], [0.11439803253932944, -0.06327971809577798, -0.1132899117191822], [0.1144384239273764, -0.06308409238539195, -0.11342533346170625], [0.11448172340816186, -0.06293406708460286, -0.11355719650153115], [0.1145107309835416, -0.06282258234253757, -0.11366782412073476], [0.1145212757344398, -0.06274161820933444, -0.11375020325989943]]\n",
      "0                             0                         [0.8219883497032462, 0.055461907539454736, 5.796212136374984]  [1.8645170892564253, 3.9836337735147618, 1.8542347166844362, -0.033575393978855395, 6.145771325074644, 1.768587041978376, 2.3834463585553176, 2.574674427996713, 3.2025060035922506, 3.462554262137925, 2.9688014712041824, 3.6392797244896276, 2.4264204124998616, 2.959295282571362, 2.80761968566804, 2.9381641266901752, 2.4564605322672155, 2.5326728489893084, 2.329083831591694, 2.4966532231496714, 2.6100743493246377, 2.2269274862369937, 1.8940177291539273, 1.2706479646215427, 2.0727683721480963, 1.283321209134646, 1.2595577603039132, 1.3285245101596748, 0.7936106573927502, 0.8289920919325134]  [[0.37855912823148397, 0.3276541141938409, 0.5632095514622795], [0.3501254623144073, 0.40179052461442666, 0.6251374222675912], [0.22515646361341451, 0.3602035910439996, 0.5419469824099867], [0.08054095374897387, 0.25646720021005315, 0.3662084132746196], [-0.03151181099608344, 0.11457717050204691, 0.1582802260107087], [-0.0735656770467564, -0.0015116772693178938, 0.0012455391787867809], [-0.06506950284183292, -0.062137909794032886, -0.10367219303485772], [-0.023467819203992712, -0.07973294050354368, -0.16834460180524838], [0.0265330855539838, -0.07879421952072602, -0.19678329564302918], [0.07057309376638986, -0.0707319806127257, -0.2010697614898708], [0.10301517396811792, -0.06387586121475072, -0.19211571596958757], [0.1228883435207395, -0.0601743945077822, -0.17758698776082535], [0.13245917797754408, -0.05963134608009171, -0.16206085728971217], [0.1348398765249712, -0.06067834918805032, -0.14827145051346688], [0.13301327220448572, -0.062305068821883136, -0.1370436583088163], [0.12932313629958392, -0.06378481999272204, -0.12850752240003463], [0.12526404902580357, -0.06476952444161771, -0.12235315458617271], [0.12167188230255364, -0.06521059217947714, -0.11813091332520045], [0.11886763975935492, -0.06524063241560435, -0.11542327699190935], [0.116894657302246, -0.06499721505617723, -0.11380188466180594], [0.11564816298125981, -0.06461904012203379, -0.11294234625261126], [0.11494736842320051, -0.06420762259338653, -0.11259339403471624], [0.11461117628280672, -0.06382351370814521, -0.11256543991687998], [0.11449046256068489, -0.06349531194356842, -0.11271733745163773], [0.11447777034805405, -0.06323117599133868, -0.11294912650683739], [0.1145046955963535, -0.063027991673204, -0.11319510701997565], [0.11453371482929951, -0.06287737118269199, -0.11341656780152568], [0.11454890669653782, -0.0627692359492831, -0.11359459674839492], [0.11454711575694608, -0.06269374285074342, -0.11372400575300895], [0.1145315361956404, -0.0626422519620754, -0.11380801994443521]]\n",
      "0                             0                         [-0.7618405752393946, 2.0472636261020174, 6.098479530952007]   [6.521727017579424, -1.13311344924653, 8.03878130454521, 1.633957236993516, 5.1722551804192936, 1.9725571764099188, 2.732036936348763, 3.3075975824484942, 2.837005259517098, 3.400724364028486, 3.221745996301097, 3.7083227645891315, 3.3432414771262673, 3.0155772309930704, 2.534317481099305, 3.105172199891673, 2.3694590811009135, 2.1743436566992056, 2.665290039374535, 2.113722510995461, 2.2822697733486925, 2.219368510787872, 1.3349604511571596, 1.6440898311880034, 1.3121233049677417, 1.453964453397509, 1.4051181140381663, 1.0402576931639516, 0.7785925296092168, 0.6789342070316884]           [[0.3829533731757644, 0.25815732060171304, 0.4567965700990432], [0.3802547786186998, 0.3152579366752464, 0.4723974947561075], [0.2653772710275144, 0.304541481470157, 0.37154542276460634], [0.11877047566911571, 0.24288612834565965, 0.2097726629395046], [0.0010326551867217704, 0.12807118106130783, 0.06064016587552695], [-0.05961507668009036, 0.028228527461563454, -0.04955901023801987], [-0.06509953014811934, -0.033945322831514635, -0.12010576428241318], [-0.035318298934147466, -0.0629528305111137, -0.15990288655529533], [0.010147646720637171, -0.07215897036483895, -0.17595558813617576], [0.0548440084753406, -0.07003508341556805, -0.1769293848515773], [0.09059275882963123, -0.0652907064400175, -0.1697265431384127], [0.11441595476573711, -0.061675897853288725, -0.15939178209303875], [0.12796162273256217, -0.06031461495696813, -0.14847365609648752], [0.13335130452901128, -0.06042678643953017, -0.13891004990388858], [0.1334564291837782, -0.06141387805819551, -0.131131195744939], [0.13072692612239134, -0.06260733380709199, -0.12516943084489726], [0.12693321667274482, -0.06359424004946845, -0.12079842793019624], [0.1231712414976621, -0.06422622604335601, -0.1177398888365393], [0.1200156011286763, -0.0644905251003336, -0.11567893493101356], [0.11765924918510122, -0.0644720107717644, -0.11436155562719413], [0.11607446942972642, -0.06427763501232706, -0.1135854042140042], [0.11511629080171952, -0.06400083568007225, -0.11319100571960736], [0.114609163966041, -0.06370674121820497, -0.11305326771630123], [0.11439242411530277, -0.06343413803570055, -0.11307442145664287], [0.11434005664299478, -0.06320195627342154, -0.11318133268811145], [0.11436454310405629, -0.06301583949869344, -0.11332249761250687], [0.11441212789593543, -0.06287356414459117, -0.11346446859163592], [0.11445469892883418, -0.06276894124797816, -0.11358791720674663], [0.11448107181045357, -0.06269447527643673, -0.11368413510788972], [0.11448998841759604, -0.06264291477333511, -0.11375161240931908]]\n",
      "0                             0                         [0.561971881159741, 3.144492070559409, 5.559988352106144]      [4.190194300663683, 1.4710439689342678, 4.347724378274937, 1.861738909890693, 0.8429147407495471, -0.007912718377042854, 2.986952287500884, 2.9942066846844533, 2.9259537577803285, 3.2956412674144637, 3.3808188797491345, 3.36897968755574, 3.3339141580150278, 3.1402571775057986, 3.197480884429736, 3.0238814589302065, 2.632343817529189, 2.603985277555193, 2.146164780937704, 2.0720342998955474, 2.027203436708546, 1.9680754954880402, 2.052353745457873, 1.6596117277828235, 1.6047091122912733, 1.4427771439659787, 1.225641852560581, 1.113845788056635, 0.9966067415032669, 0.7585169723201464]       [[0.33261414458119937, 0.21896942990028034, 0.561473342223354], [0.3018065150458164, 0.2830216394029052, 0.6208930625082533], [0.19757000647544615, 0.29002159021488016, 0.5396420088915846], [0.07351365799229508, 0.2437578119024959, 0.36796631276133585], [-0.026363715612333402, 0.15066521828713944, 0.16465325677232442], [-0.06861987119190817, 0.052246946464006516, 0.011506562521672592], [-0.06519227853509761, -0.01630774763028914, -0.09405059040099742], [-0.027567933848942225, -0.049168223177326, -0.16046216235419267], [0.021162466139000344, -0.062045737802643136, -0.1925035074126921], [0.06531660749677705, -0.0632281001604259, -0.2006602156833275], [0.0988261328006164, -0.06115362655381604, -0.19470491423226718], [0.12032034523644376, -0.05959110458546074, -0.18199936869840025], [0.1314161982843685, -0.05964502377341918, -0.16705126011434707], [0.13490061045591645, -0.06073568392028128, -0.1530723509586949], [0.13371224410945937, -0.06222120414851279, -0.1411968523298007], [0.13028912310782564, -0.06363714195074457, -0.13182903333663318], [0.12625694406243093, -0.0646657890761779, -0.12485333659427134], [0.1225572818349173, -0.06521441631032526, -0.1199218653541175], [0.11960441583431776, -0.06534743452577238, -0.11663445339721976], [0.11746885324425622, -0.06519322172812395, -0.11459958916752089], [0.11607293317688804, -0.06486480583301896, -0.11344739116225916], [0.11525103785615583, -0.06446701865831894, -0.1128986117211177], [0.11482333391200701, -0.06407062616388112, -0.11273889529711315], [0.11463675435152938, -0.06371569618364517, -0.11280806002548263], [0.11457848451040951, -0.06341929731748192, -0.11299116957643773], [0.11457488361388972, -0.06318383078181775, -0.11321108729373697], [0.11458394356512634, -0.06300379691294408, -0.11342075437982138], [0.11458618249509868, -0.06287026809450791, -0.1135953934677208], [0.11457593241939772, -0.06277359953191552, -0.11372580274668079], [0.11455464035908296, -0.0627048326538576, -0.11381261776280252]]\n",
      "0                             0                         [4.296835155684376, 2.3230477007660206, 7.638307525527196]     [1.5018513090971766, -0.0003341042588377169, 1.1468325377055717, 2.4337433649057965, 5.17726614332079, 0.9060270793904831, 2.290363901022369, 2.6714794881889974, 3.3774165689532363, 2.8899046718379386, 2.917528735162025, 2.958298734244091, 3.0031848648942785, 2.4135073321687393, 3.032376433669132, 2.5987650981295265, 3.2865171676445346, 2.842949417208943, 2.2592300914027943, 2.424759960987704, 2.5488456557167414, 2.2457027166557064, 2.0463726171511234, 1.8088567708041736, 1.282714968138201, 1.382127919145569, 1.4040029172804078, 1.1766876433827096, 1.1227833500186646, 0.780372557594342]   [[0.37075072321636565, 0.19914324185730398, 0.47755553428234665], [0.37615240662434624, 0.24423111535593994, 0.5035144503364641], [0.27527571766710324, 0.256325685689864, 0.40825126300785886], [0.1331674101284051, 0.23114033854089616, 0.24234445510893135], [0.012804513479247215, 0.1462707774461434, 0.08098589347953217], [-0.052853229027031934, 0.054988797166639264, -0.034861310734712686], [-0.06681043280008764, -0.010694806205991617, -0.10991675399505133], [-0.04103934674099553, -0.046502448128839774, -0.15493312097644799], [0.0028413601558562734, -0.06261128830101828, -0.17472716966723387], [0.04848063398630531, -0.06617117292624275, -0.17855129964326485], [0.08569944675554732, -0.06390225924700535, -0.17316130403549462], [0.11138874166690112, -0.06131853479152888, -0.16369568369407145], [0.12657862172242845, -0.0602776694319449, -0.1528045077413116], [0.13307323425180617, -0.06036523939615951, -0.14282924781408352], [0.1338424739034082, -0.06128389609035462, -0.1344035691490438], [0.1314054097523602, -0.06246943709236574, -0.1277288007384748], [0.12766674109762394, -0.06351302192731917, -0.12269662859594663], [0.12383933564295833, -0.06422305729484698, -0.11906894569274482], [0.12055820462382881, -0.06457443939030963, -0.11657846827038246], [0.11806630308717202, -0.06461989362084253, -0.11494570690545919], [0.11636345458401713, -0.06445979185706324, -0.11394857404410505], [0.115313605340214, -0.0641911078800646, -0.11340480918415669], [0.11474073954496551, -0.06388717750694622, -0.11317115059121688], [0.11447973954793765, -0.06359476981237216, -0.1131344110421661], [0.11439905599152243, -0.06333883228848047, -0.11320910429478903], [0.11440609619373508, -0.06312893618770907, -0.1133344725465194], [0.1144430792785865, -0.06296501429128415, -0.11347048541371213], [0.11447910839017221, -0.0628418286501794, -0.11359334336473648], [0.11450120706750032, -0.06275199335429314, -0.1136913786360382], [0.11450701483172095, -0.06268796597068416, -0.11376126227680045]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 2019-02-26 15:57:25.814662: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf /tools/**/**/**/**/**/*.pyc\n",
    "model_dir=$(ls ${PWD}/trained_model/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=${PWD}/trained_model/export/exporter/${model_dir} \\\n",
    "    --json-instances=./sequences.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud ML-Engine prediction from deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataframe to instances list to get sent to ML-Engine\n",
    "instances = [{column: np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list for column in CSV_COLUMNS} for _ in range(0, number_of_prediction_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send instance dictionary to receive response from ML-Engine for online prediction\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "request_data = {\"instances\": instances}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'lstm_autoencoder_anomaly_detection', 'v1')\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
