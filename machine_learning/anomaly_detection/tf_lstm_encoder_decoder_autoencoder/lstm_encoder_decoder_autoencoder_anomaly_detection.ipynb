{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# BUCKET = 'qwiklabs-gcp-8923d4964bfbd247-bucket' # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "# PROJECT = 'qwiklabs-gcp-8923d4964bfbd247' # REPLACE WITH YOUR PROJECT ID\n",
    "# REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# # Import os environment variables\n",
    "# os.environ['PROJECT'] = PROJECT\n",
    "# os.environ['BUCKET'] =  BUCKET\n",
    "# os.environ['REGION'] = REGION\n",
    "# os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_sequence_before_anomaly = 80.0\n",
    "percent_sequence_after_anomaly = 0.0\n",
    "\n",
    "def create_time_series_normal_parameters():\n",
    "    normal_frequency_noise_scale = 0.1\n",
    "    normal_frequence_noise_shift = 0.0\n",
    "\n",
    "    normal_amplitude_noise_scale = 1.0\n",
    "    normal_amplitude_noise_shift = 1.0\n",
    "\n",
    "    normal_noise_noise_scale = 0.2\n",
    "\n",
    "    normal_freq = (np.random.random() * normal_frequency_noise_scale) + normal_frequence_noise_shift\n",
    "    normal_ampl = np.random.random() * normal_amplitude_noise_scale + normal_amplitude_noise_shift\n",
    "\n",
    "    return {\"normal_freq\": normal_freq, \"normal_ampl\": normal_ampl, \"normal_noise_noise_scale\": normal_noise_noise_scale}\n",
    "  \n",
    "\n",
    "def create_time_series_normal(number_of_sequences, sequence_length, normal_freq, normal_ampl, normal_noise_noise_scale):\n",
    "    # Normal parameters\n",
    "    normal_noise = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length)]\n",
    "\n",
    "    sequence = np.sin(np.arange(0, number_of_sequences * sequence_length) * normal_freq) * normal_ampl + normal_noise\n",
    "\n",
    "    sequence = sequence.reshape(number_of_sequences, sequence_length)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def create_time_series_with_anomaly(number_of_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, normal_freq, normal_ampl, normal_noise_noise_scale):\n",
    "    sequence_length_before_anomaly = int(sequence_length * percent_sequence_before_anomaly / 100.0)\n",
    "    sequence_length_after_anomaly = int(sequence_length * percent_sequence_after_anomaly / 100.0)\n",
    "    sequence_length_anomaly = sequence_length - sequence_length_before_anomaly - sequence_length_after_anomaly\n",
    "\n",
    "    # normal parameters\n",
    "    normal_noise_before = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_before_anomaly)]\n",
    "    normal_noise_after = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_after_anomaly)]\n",
    "\n",
    "    # Anomalous parameters\n",
    "    anomalous_frequency_noise_scale = 2.0\n",
    "    anomalous_frequence_noise_shift = 1.0\n",
    "\n",
    "    anomalous_amplitude_noise_scale = 1.0\n",
    "    anomalous_amplitude_noise_shift = 1.0\n",
    "\n",
    "    anomalous_noise_noise_scale = 1.0\n",
    "\n",
    "    anomalous_freq = (np.random.random() * anomalous_frequency_noise_scale) + anomalous_frequence_noise_shift\n",
    "    anomalous_ampl = np.random.random() * anomalous_amplitude_noise_scale + anomalous_amplitude_noise_shift\n",
    "    anomalous_noise = [np.random.random() * anomalous_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_anomaly)]\n",
    "\n",
    "    sequence_before_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_before_anomaly) * normal_freq) * normal_ampl + normal_noise_before\n",
    "    sequence_before_anomaly = sequence_before_anomaly.reshape(number_of_sequences, sequence_length_before_anomaly)\n",
    "\n",
    "    sequence_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_anomaly) * anomalous_freq) * anomalous_ampl + anomalous_noise\n",
    "    sequence_anomaly = sequence_anomaly.reshape(number_of_sequences, sequence_length_anomaly)\n",
    "\n",
    "    sequence_after_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_after_anomaly) * normal_freq) * normal_ampl + normal_noise_after\n",
    "    sequence_after_anomaly = sequence_after_anomaly.reshape(number_of_sequences, sequence_length_after_anomaly)\n",
    "\n",
    "    return np.concatenate(seq = [sequence_before_anomaly, sequence_anomaly, sequence_after_anomaly], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/seaborn/timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_normal_parameters = create_time_series_normal_parameters()\n",
    "\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_normal(5, 50, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_with_anomaly(5, 50, percent_sequence_before_anomaly, percent_sequence_after_anomaly, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_normal_sequences = 7000\n",
    "\n",
    "number_of_validation_normal_1_sequences = 500\n",
    "number_of_validation_normal_2_sequences = 500\n",
    "number_of_validation_anomalous_sequences = 500\n",
    "\n",
    "number_of_test_normal_sequences = 750\n",
    "number_of_test_anomalous_sequences = 750\n",
    "\n",
    "sequence_length = 30\n",
    "number_of_tags = 3\n",
    "tag_columns = [\"tag_{0}\".format(tag) for tag in range(0, number_of_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'normal_ampl': 1.6841108247310859,\n",
       "  'normal_freq': 0.08989812299598696,\n",
       "  'normal_noise_noise_scale': 0.2},\n",
       " {'normal_ampl': 1.392386034441465,\n",
       "  'normal_freq': 0.0835672880826654,\n",
       "  'normal_noise_noise_scale': 0.2},\n",
       " {'normal_ampl': 1.7139322369965313,\n",
       "  'normal_freq': 0.04302254868655767,\n",
       "  'normal_noise_noise_scale': 0.2}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_data_list = [create_time_series_normal_parameters() for tag in range(0, number_of_tags)]\n",
    "tag_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_normal_sequences_array.shape = \n",
      "(7000, 3)\n"
     ]
    }
   ],
   "source": [
    "training_normal_sequences_list = [create_time_series_normal(number_of_training_normal_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "training_normal_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = training_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_training_normal_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "np.random.shuffle(training_normal_sequences_array)\n",
    "print(\"training_normal_sequences_array.shape = \\n{}\".format(training_normal_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_normal_1_sequences_array.shape = \n",
      "(500, 3)\n",
      "validation_normal_2_sequences_array.shape = \n",
      "(500, 3)\n",
      "validation_anomalous_sequences_array.shape = \n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "validation_normal_1_sequences_list = [create_time_series_normal(number_of_validation_normal_1_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_normal_1_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_normal_1_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_normal_1_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_normal_1_sequences_array.shape = \\n{}\".format(validation_normal_1_sequences_array.shape))\n",
    "\n",
    "validation_normal_2_sequences_list = [create_time_series_normal(number_of_validation_normal_2_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_normal_2_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_normal_2_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_normal_2_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_normal_2_sequences_array.shape = \\n{}\".format(validation_normal_2_sequences_array.shape))\n",
    "\n",
    "validation_anomalous_sequences_list = [create_time_series_with_anomaly(number_of_validation_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_anomalous_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_anomalous_sequences_array.shape = \\n{}\".format(validation_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_sequences_array.shape = \n",
      "(750, 3)\n",
      "test_anomalous_sequences_array.shape = \n",
      "(750, 3)\n"
     ]
    }
   ],
   "source": [
    "test_normal_sequences_list = [create_time_series_normal(number_of_test_normal_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "test_normal_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = test_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_test_normal_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"test_normal_sequences_array.shape = \\n{}\".format(test_normal_sequences_array.shape))\n",
    "\n",
    "test_anomalous_sequences_list = [create_time_series_with_anomaly(number_of_test_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "test_anomalous_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = test_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_test_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"test_anomalous_sequences_array.shape = \\n{}\".format(test_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = \"data/training_normal_sequences.csv\", X = training_normal_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/validation_normal_1_sequences.csv\", X = validation_normal_1_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/validation_normal_2_sequences.csv\", X = validation_normal_2_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/validation_anomalous_sequences.csv\", X = validation_anomalous_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/test_normal_sequences.csv\", X = test_normal_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/test_anomalous_sequences.csv\", X = test_anomalous_sequences_array, fmt = '%s', delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17588209,1.05248997,0.93330248,0.79585779,0.7711927,0.64252314,0.36529185,0.31966415,0.12049,-0.02315838,-0.18408219,-0.34160167,-0.43443768,-0.59391723,-0.71859844,-0.90603057,-1.04229937,-1.17005102,-1.17986309,-1.26120316,-1.27902823,-1.34428808,-1.46130118,-1.44180555,-1.48290365,-1.62330873,-1.53447242,-1.4965669,-1.45858258,-1.44984319;1.10150036,1.06313744,1.25884615,1.30087775,1.38168733,1.30737667,1.35017569,1.46623021,1.49516453,1.51426792,1.57407614,1.39059597,1.42930138,1.47367729,1.47282596,1.37248325,1.33060112,1.28984858,1.11574097,1.20980608,1.03024745,0.896286,0.83327662,0.77688691,0.66021202,0.62147174,0.51847441,0.40077178,0.27729859,0.0695406;-0.48617041,-0.43517075,-0.48325102,-0.30316557,-0.20990735,-0.12772672,-0.04023885,-0.10826945,0.05447991,0.19084117,0.20829731,0.18911231,0.35013336,0.41824704,0.48543645,0.52950674,0.63418491,0.59581893,0.7812336,0.80467754,0.95337157,0.96570593,0.91906614,1.02271859,1.04774539,1.13208155,1.26233606,1.344035,1.44741418,1.47592732\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/training_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12861963,0.28185067,0.49474604,0.64731049,0.74643923,0.83694066,1.01080476,1.05947312,1.20598307,1.37761282,1.4006927,1.55876243,1.65470912,1.6990295,1.75363735,1.80409902,1.83416227,1.71197225,1.73024748,1.73211412,1.73893008,1.60906211,1.59873998,1.49971807,1.43998745,1.43692217,1.24297187,1.28143982,0.99966426,0.97651236;0.19418629,0.25338986,0.31755394,0.52286744,0.56912609,0.58550322,0.81298395,0.83265124,0.9833387,1.11033248,1.11426578,1.23645163,1.24601594,1.34997273,1.44372357,1.33380166,1.41097751,1.38865505,1.54497644,1.47023285,1.48506619,1.4927085,1.39965486,1.32217204,1.39451095,1.40944781,1.1538233,1.27011592,1.13384212,0.97137225;0.03507806,0.16484384,0.34519927,0.30709015,0.40272209,0.53911663,0.46143555,0.62038952,0.61188399,0.83113201,0.77383145,0.91692873,0.95173877,0.98756115,1.05019095,1.06969449,1.26495982,1.20325606,1.21395081,1.30370805,1.44812053,1.49220354,1.54791548,1.56825428,1.6694124,1.7060677,1.63952688,1.67633706,1.61266307,1.78599294\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_normal_1_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07926062,0.31683153,0.46078832,0.57793363,0.65493501,0.81362113,0.89283819,1.15635079,1.28038588,1.33958668,1.44906317,1.57477872,1.50792892,1.57359807,1.6070744,1.73273521,1.78209899,1.70636418,1.86937272,1.81361983,1.69329827,1.78899424,1.71332245,1.4952969,1.53353262,1.48973425,1.40775055,1.19524983,1.05958904,1.02051326;0.01552099,0.26918842,0.23562021,0.45884026,0.6246957,0.63654293,0.835029,0.9355236,0.95856336,1.00123507,1.08987593,1.28205558,1.20078273,1.35540604,1.42280316,1.3869439,1.44706248,1.48985042,1.42274043,1.57116209,1.49826653,1.50977083,1.43017961,1.37008223,1.26851781,1.38118279,1.18947251,1.12668832,1.02374132,0.99498604;0.11801673,0.23517483,0.29179475,0.3512082,0.43088089,0.47730365,0.62818716,0.55753183,0.60833035,0.81123633,0.78615799,0.91348883,0.98273587,0.98289682,1.0225584,1.10797033,1.1229877,1.31496175,1.27965914,1.43721891,1.36412811,1.53478696,1.44390708,1.59566897,1.61264254,1.54093176,1.71858975,1.71753595,1.71702103,1.63960496\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_normal_2_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0476823,0.28074525,0.34914423,0.51621364,0.65359154,0.86207158,0.96760278,1.05242615,1.27050722,1.23594765,1.33094354,1.57415613,1.62388792,1.58263491,1.69840307,1.64884327,1.69270195,1.85134124,1.69688059,1.70694494,1.75750465,1.72129195,1.6178674,1.66462212,0.80498641,1.86609252,-0.99651397,0.88798575,0.71010561,-1.03286938;0.03249302,0.21386191,0.29978007,0.48618017,0.54159305,0.69644072,0.85856622,0.96863921,0.87774294,1.04083465,1.07615052,1.17721273,1.23719152,1.42171343,1.2976085,1.49027767,1.4516873,1.53568397,1.48551058,1.39302767,1.48956015,1.47876518,1.37997463,1.37898674,0.94097143,1.96842347,0.35526889,-0.22238371,1.64650437,1.30840465;0.06810315,0.16830364,0.24192936,0.26705119,0.33068194,0.37814478,0.45824518,0.59192334,0.67064876,0.73911656,0.74923102,0.97220119,0.99405646,1.01814435,1.15597438,1.21168817,1.09917789,1.19566076,1.36107196,1.35738224,1.38537704,1.47109616,1.52447966,1.47860698,0.77104625,0.79774101,-0.60200345,2.07656895,-1.021773,2.24047112\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_anomalous_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.176879,0.28157551,0.34286069,0.52982998,0.76649023,0.76753343,0.88061119,1.04089243,1.18829358,1.3036942,1.35056761,1.59769674,1.62701931,1.72799965,1.74212534,1.80859273,1.69387853,1.74410074,1.82470612,1.75348494,1.810974,1.74328266,1.66007855,1.58222312,1.48830247,1.47091576,1.37619029,1.22862397,1.04758841,0.93353766;0.03906791,0.12840238,0.33903516,0.4519174,0.61535939,0.56626864,0.70745642,0.83893486,0.98330563,1.12679802,1.13292751,1.15806156,1.348221,1.29316906,1.40298199,1.4691991,1.42338782,1.37840343,1.5613833,1.56965028,1.45278208,1.5229311,1.53093928,1.41349641,1.45861741,1.31854487,1.32433731,1.18148279,1.05652598,1.05575192;0.02431107,0.18649367,0.26609438,0.3357815,0.32219665,0.46393944,0.50739788,0.63307434,0.72882216,0.66263628,0.82412096,0.78487919,1.03959438,1.00666211,0.97436207,1.09273816,1.26189411,1.33270172,1.23657456,1.41944898,1.33422063,1.50358264,1.57191028,1.5738186,1.64287035,1.57034759,1.59082108,1.65600343,1.71297964,1.71373622\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/test_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14955376,0.23047732,0.36196952,0.49111546,0.63762641,0.82023395,1.0096023,1.06980249,1.28867091,1.39251535,1.48119061,1.57009148,1.56357007,1.63660963,1.75210372,1.81625172,1.75111508,1.69595956,1.87491111,1.71425349,1.75317968,1.71303116,1.58086056,1.65491607,0.41597572,2.09456944,-0.81230328,-0.17067375,1.78832536,0.02986298;0.16077195,0.29585395,0.41916748,0.50965872,0.45707385,0.69903008,0.84168502,0.78248641,0.89070169,1.03158742,1.16801285,1.11538873,1.3288857,1.29098155,1.39821258,1.3741081,1.47880892,1.43888441,1.4496614,1.55691116,1.53985495,1.40312672,1.44129153,1.3389849,0.73717068,1.26642546,-0.44010851,0.13682272,1.67806866,0.98234553;0.04899632,0.17322884,0.17241653,0.36934092,0.34930705,0.5372362,0.48436729,0.54305035,0.63075421,0.79473468,0.91393188,0.92446061,0.91111338,1.0305638,1.02922687,1.21243504,1.23063918,1.19175378,1.28672561,1.39724611,1.44772632,1.46779586,1.55997817,1.55214631,0.62352634,2.14237723,-0.96213717,-0.69388262,1.84340708,-0.01916956\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/test_anomalous_sequences.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging to be level of INFO\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "CSV_COLUMNS = tag_columns\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[\"\"], [\"\"], [\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    def _input_fn():\n",
    "#         print(\"\\nread_dataset: _input_fn: filename = \\n{}\".format(filename))\n",
    "#         print(\"read_dataset: _input_fn: mode = \\n{}\".format(mode))\n",
    "#         print(\"read_dataset: _input_fn: batch_size = \\n{}\".format(batch_size))\n",
    "#         print(\"read_dataset: _input_fn: params = \\n{}\\n\".format(params))\n",
    "\n",
    "        def decode_csv(value_column, sequence_length):\n",
    "            def convert_sequences_from_strings_to_floats(features):\n",
    "                def split_and_convert_string(string_tensor):\n",
    "                    # Split string tensor into a sparse tensor based on delimiter\n",
    "                    split_string = tf.string_split(source = tf.expand_dims(input = string_tensor, axis = 0), delimiter = \",\")\n",
    "#                     print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \\n{}\".format(split_string))\n",
    "\n",
    "                    # Converts the values of the sparse tensor to floats\n",
    "                    converted_tensor = tf.string_to_number(split_string.values, out_type = tf.float64)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \\n{}\".format(converted_tensor))\n",
    "\n",
    "                    # Create a new sparse tensor with the new converted values, because the original sparse tensor values are immutable\n",
    "                    new_sparse_tensor = tf.SparseTensor(indices = split_string.indices, values = converted_tensor, dense_shape = split_string.dense_shape)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \\n{}\".format(new_sparse_tensor))\n",
    "\n",
    "                    # Create a dense tensor of the float values that were converted from text csv\n",
    "                    dense_floats = tf.sparse_tensor_to_dense(sp_input = new_sparse_tensor, default_value = 0.0)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \\n{}\".format(dense_floats))\n",
    "\n",
    "                    dense_floats_vector = tf.squeeze(input = dense_floats, axis = 0)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \\n{}\\n\".format(dense_floats_vector))\n",
    "\n",
    "                    return dense_floats_vector\n",
    "                    \n",
    "#                 print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "                for column in CSV_COLUMNS:\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \\n{}\".format(column))\n",
    "                    features[column] = split_and_convert_string(features[column])\n",
    "                    features[column].set_shape([sequence_length])\n",
    "\n",
    "#                 print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "\n",
    "                return features\n",
    "                \n",
    "#             print(\"\\nread_dataset: _input_fn: decode_csv: value_column = \\n{}\".format(value_column))\n",
    "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS, field_delim = \";\")\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: columns = \\n{}\".format(columns))\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            features = convert_sequences_from_strings_to_floats(features)\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            return features\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename = filename)\n",
    "#         print(\"\\nread_dataset: _input_fn: file_list = \\n{}\".format(file_list))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(filenames = file_list)    # Read text file\n",
    "#         print(\"read_dataset: _input_fn: dataset.TextLineDataset(file_list) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Decode the CSV file into a features dictionary of tensors\n",
    "        dataset = dataset.map(map_func = lambda x: decode_csv(x, params[\"sequence_length\"]))\n",
    "#         print(\"read_dataset: _input_fn: dataset.map(decode_csv) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine amount of times to repeat file based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        # Repeat files num_epoch times\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "#         print(\"read_dataset: _input_fn: dataset.repeat(num_epochs) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Group the data into batches\n",
    "        dataset = dataset.batch(batch_size = batch_size)\n",
    "#         print(\"read_dataset: _input_fn: dataset.batch(batch_size) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine if we should shuffle based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "#             print(\"read_dataset: _input_fn: dataset.shuffle(buffer_size = 10 * batch_size) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Create a iterator and then pull the next batch of features from the example queue\n",
    "        batch_features = dataset.make_one_shot_iterator().get_next()\n",
    "#         print(\"read_dataset: _input_fn: batch_features = \\n{}\".format(batch_features))\n",
    "\n",
    "        return batch_features\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out_input_function(args):\n",
    "    with tf.Session() as sess:\n",
    "        fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"],\n",
    "          mode = tf.estimator.ModeKeys.EVAL,\n",
    "          batch_size = args[\"batch_size\"],\n",
    "          params = args)\n",
    "\n",
    "        features = sess.run(fn())\n",
    "        print(\"try_out_input_function: features = {}\".format(features))\n",
    "\n",
    "        print(\"try_out_input_function: features[tag_0].shape = {}\".format(features[\"tag_0\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_out_input_function(args = arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model function to be used in our custom estimator\n",
    "def lstm_encoder_decoder_autoencoder_anomaly_detection(features, labels, mode, params):\n",
    "#     print(\"\\nlstm_encoder_decoder_autoencoder_anomaly_detection: features = \\n{}\".format(features))\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: labels = \\n{}\".format(labels))\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mode = \\n{}\".format(mode))\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: params = \\n{}\".format(params))\n",
    "\n",
    "    # 0. Get input sequence tensor into correct shape\n",
    "    # Get dynamic batch size in case there was a partially filled batch\n",
    "    current_batch_size = tf.shape(input = features[CSV_COLUMNS[0]], out_type = tf.int32)[0]\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: current_batch_size = \\n{}\".format(current_batch_size))\n",
    "\n",
    "    # Get the number of features \n",
    "    number_of_features = len(CSV_COLUMNS)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: number_of_features = \\n{}\".format(number_of_features))\n",
    "\n",
    "    # Stack all of the features into a 3-D tensor\n",
    "    X = tf.stack(values = list(features.values()), axis = 2) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: X = \\n{}\".format(X))\n",
    "\n",
    "    # Unstack all of 3-D features tensor into a sequence(list) of 2-D tensors of shape = (current_batch_size, number_of_features)\n",
    "    X_sequence = tf.unstack(value = X, num = params[\"sequence_length\"], axis = 1)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: X_sequence = \\n{}\".format(X_sequence))\n",
    "\n",
    "    # Since this is an autoencoder, the features are the labels. It works better though to have the labels in reverse order\n",
    "    if params[\"reverse_labels_sequence\"] == True:\n",
    "        Y = tf.reverse_sequence(input = X,  # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "                                seq_lengths = tf.tile(input = tf.constant(value = [params[\"sequence_length\"]], dtype = tf.int64), \n",
    "                                                      multiples = tf.expand_dims(input = current_batch_size, axis = 0)), \n",
    "                                seq_axis = 1, \n",
    "                                batch_axis = 0)\n",
    "    else:\n",
    "        Y = X  # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: Y = \\n{}\".format(Y))\n",
    "  \n",
    "  ################################################################################\n",
    "  \n",
    "    # 1. Create encoder of encoder-decoder LSTM stacks\n",
    "    def create_LSTM_stack(lstm_hidden_units, lstm_dropout_output_keep_probs):\n",
    "        # First create a list of LSTM cells using our list of lstm hidden unit sizes\n",
    "        lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units = units, forget_bias = 1.0, state_is_tuple = True) for units in lstm_hidden_units] # list of LSTM cells\n",
    "#         print(\"\\nlstm_encoder_decoder_autoencoder_anomaly_detection: create_LSTM_stack: lstm_cells = \\n{}\".format(lstm_cells))\n",
    "\n",
    "        # Next apply a dropout wrapper to our stack of LSTM cells, in this case just on the outputs\n",
    "        dropout_lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(cell = lstm_cells[cell_index], \n",
    "                                                            input_keep_prob = 1.0, \n",
    "                                                            output_keep_prob = lstm_dropout_output_keep_probs[cell_index], \n",
    "                                                            state_keep_prob = 1.0) for cell_index in range(len(lstm_cells))]\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: create_LSTM_stack: dropout_lstm_cells = \\n{}\".format(dropout_lstm_cells))\n",
    "\n",
    "        # Create a stack of layers of LSTM cells\n",
    "        stacked_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells = dropout_lstm_cells, state_is_tuple = True) # combines list into MultiRNNCell object\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: create_LSTM_stack: stacked_lstm_cells = \\n{}\\n\".format(stacked_lstm_cells))\n",
    "\n",
    "        return stacked_lstm_cells\n",
    "  \n",
    "    # Create our decoder now\n",
    "    decoder_stacked_lstm_cells = create_LSTM_stack(params[\"decoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_stacked_lstm_cells = \\n{}\".format(decoder_stacked_lstm_cells))\n",
    "  \n",
    "    # Create the encoder variable scope\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        # Create separate encoder cells with their own weights separate from decoder\n",
    "        encoder_stacked_lstm_cells = create_LSTM_stack(params[\"encoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_stacked_lstm_cells = {}\".format(encoder_stacked_lstm_cells))\n",
    "\n",
    "        # Encode the input sequence using our encoder stack of LSTMs\n",
    "        encoder_outputs, encoder_states = tf.nn.static_rnn(cell = encoder_stacked_lstm_cells, \n",
    "                                                           inputs = X_sequence, \n",
    "                                                           initial_state = encoder_stacked_lstm_cells.zero_state(batch_size = current_batch_size, dtype = tf.float64), \n",
    "                                                           dtype = tf.float64)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_outputs = \\n{}\".format(encoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_states = \\n{}\".format(encoder_states)) # tuple of final encoder c_state and h_state for each layer\n",
    "\n",
    "        # We just pass on the final c and h states of the encoder\"s last layer, so extract that and drop the others\n",
    "        encoder_final_states = encoder_states[-1]\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_final_states = \\n{}\".format(encoder_final_states))\n",
    "\n",
    "        # Extract the c and h states from the tuple\n",
    "        encoder_final_c, encoder_final_h = encoder_final_states\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_final_c = \\n{}\".format(encoder_final_c))\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_final_h = \\n{}\".format(encoder_final_h))\n",
    "\n",
    "        # In case the decoder\"s first layer\"s number of units is different than encoder\"s last layer\"s number of units, use a dense layer to map to the correct shape\n",
    "        encoder_final_c_dense = tf.layers.dense(inputs = encoder_final_c, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "        encoder_final_h_dense = tf.layers.dense(inputs = encoder_final_h, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "\n",
    "        # The decoder\"s first layer\"s state comes from the encoder, the rest of the layers\" initial states are zero\n",
    "        decoder_intial_states = tuple([tf.contrib.rnn.LSTMStateTuple(c = encoder_final_c_dense, h = encoder_final_h_dense)] + [tf.contrib.rnn.LSTMStateTuple(c = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64), h = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64)) for units in params[\"decoder_lstm_hidden_units\"][1:]])\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_intial_states = \\n{}\".format(decoder_intial_states))\n",
    "    \n",
    "    ################################################################################\n",
    "\n",
    "    # 2. Create decoder of encoder-decoder LSTM stacks\n",
    "    # The rnn_decoder function takes labels during TRAIN/EVAL and a start token followed by its previous predictions during PREDICT\n",
    "    # Starts with an intial state of the final encoder states\n",
    "    def rnn_decoder(decoder_inputs, initial_state, cell, inference):\n",
    "        # Create the decoder variable scope\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # Load in our initial state from our encoder\n",
    "            state = initial_state # tuple of final encoder c_state and h_state of final encoder layer\n",
    "#             print(\"\\nlstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: state = \\n{}\".format(state))\n",
    "            \n",
    "            # Create an empty list to store our hidden state output for every timestep\n",
    "            outputs = []\n",
    "            \n",
    "            # Begin with no previous output\n",
    "            previous_output = None\n",
    "            \n",
    "            # Loop over all of our decoder_inputs which will be sequence_length long\n",
    "            for index, decoder_input in enumerate(decoder_inputs):\n",
    "                # If there has been a previous output then we will determine the next input\n",
    "                if previous_output is not None:\n",
    "                    # Create the input layer to our DNN\n",
    "                    network = previous_output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: network = \\n{}\".format(network))\n",
    "                    \n",
    "                    # Create our dnn variable scope\n",
    "                    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "                        # Add hidden layers with the given number of units/neurons per layer\n",
    "                        for units in params[\"dnn_hidden_units\"]:\n",
    "                            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size, dnn_hidden_units[i])\n",
    "#                             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: network = {}, units = {}\".format(network, units))\n",
    "                            \n",
    "                        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "                        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size, number_of_features)\n",
    "#                         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: logits = \\n{}\\n\".format(logits))\n",
    "                    \n",
    "                    # If we are in inference then we will overwrite our next decoder_input with the logits we just calculated.\n",
    "                    # Otherwise, we leave the decoder_input input as it was from the enumerated list\n",
    "                    # We have to calculate the logits even when not using them so that the correct dnn subgraph will be generated here and after the encoder-decoder for both training and inference\n",
    "                    if inference == True:\n",
    "                        decoder_input = logits # shape = (current_batch_size, number_of_features)\n",
    "\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: decoder_input = \\n{}\\n\".format(decoder_input))\n",
    "                \n",
    "                # If this isn\"t our first time through the loop, just reuse(share) the same variables for each iteration within the current variable scope\n",
    "                if index > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                # Run the decoder input through the decoder stack picking up from the previous state\n",
    "                output, state = cell(decoder_input, state)\n",
    "#                 print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: output = \\n{}\".format(output)) # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#                 print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: state = \\n{}\".format(state)) # tuple of final decoder c_state and h_state\n",
    "                \n",
    "                # Append the current decoder hidden state output to the outputs list\n",
    "                outputs.append(output) # growing list eventually sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "                \n",
    "                # Set the previous output to the output just calculated\n",
    "                previous_output = output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "        return outputs, state\n",
    "  \n",
    "    # Train our decoder now\n",
    "  \n",
    "    # Encoder-decoders work differently during training/evaluation and inference so we will have two separate subgraphs for each\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN    or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # Break 3-D labels tensor into a list of 2-D tensors\n",
    "        unstacked_labels = tf.unstack(value = Y, num = params[\"sequence_length\"], axis = 1) # list of sequence_length long of shape = (current_batch_size, number_of_features)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: unstacked_labels = \\n{}\".format(unstacked_labels))\n",
    "\n",
    "        # Call our decoder using the labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to false\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = unstacked_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = False)\n",
    "    else:\n",
    "        # Since this is inference create fake labels. The list length needs to be the output sequence length even though only the first element is actually used (as our go signal)\n",
    "        fake_labels = [tf.zeros(shape = [current_batch_size, number_of_features], dtype = tf.float64) for _ in range(params[\"sequence_length\"])]\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: fake_labels = \\n{}\".format(fake_labels))\n",
    "        \n",
    "        # Call our decoder using fake labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to true\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = fake_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = True)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_outputs = \\n{}\".format(decoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_states = \\n{}\".format(decoder_states)) # tuple of final decoder c_state and h_state\n",
    "    \n",
    "    # Stack together the list of rank 2 decoder output tensors into one rank 3 tensor\n",
    "    stacked_decoder_outputs = tf.stack(values = decoder_outputs, axis = 1) # shape = (current_batch_size, sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: stacked_decoder_outputs = \\n{}\".format(stacked_decoder_outputs))\n",
    "    \n",
    "    # Reshape rank 3 decoder outputs into rank 2 by folding sequence length into batch size\n",
    "    reshaped_stacked_decoder_outputs = tf.reshape(tensor = stacked_decoder_outputs, shape = [current_batch_size * params[\"sequence_length\"], params[\"decoder_lstm_hidden_units\"][-1]]) # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: reshaped_stacked_decoder_outputs = \\n{}\".format(reshaped_stacked_decoder_outputs))\n",
    "\n",
    "    ################################################################################\n",
    "    \n",
    "    # 3. Create the DNN structure now after the encoder-decoder LSTM stack\n",
    "    # Create the input layer to our DNN\n",
    "    network = reshaped_stacked_decoder_outputs # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: network = \\n{}\".format(network))\n",
    "    \n",
    "    # Reuse the same variable scope as we used within our decoder (for inference)\n",
    "    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "        # Add hidden layers with the given number of units/neurons per layer\n",
    "        for units in params[\"dnn_hidden_units\"]:\n",
    "            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size * sequence_length, dnn_hidden_units[i])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: network = {}, units = {}\".format(network, units))\n",
    "\n",
    "        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: logits = \\n{}\".format(logits))\n",
    "    \n",
    "    # Now that we are through the final DNN for each sequence element for each example in the batch, reshape the predictions to match our labels\n",
    "    predictions = tf.reshape(tensor = logits, shape = [current_batch_size, params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: predictions = \\n{}\".format(predictions))\n",
    "    \n",
    "    with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        # Time based\n",
    "        absolute_error_count_batch_time_variable = tf.get_variable(name = \"absolute_error_count_batch_time_variable\", # shape = ()\n",
    "                                                                   shape = 1,\n",
    "                                                                   dtype = tf.float64,\n",
    "                                                                   initializer = tf.zeros(shape = 1, \n",
    "                                                                                          dtype = tf.float64),\n",
    "                                                                   trainable = False)\n",
    "        \n",
    "        absolute_error_mean_batch_time_variable = tf.get_variable(name = \"absolute_error_mean_batch_time_variable\", # shape = (number_of_features,)\n",
    "                                                                  shape = [number_of_features],\n",
    "                                                                  dtype = tf.float64,\n",
    "                                                                  initializer = tf.zeros(shape = [number_of_features], \n",
    "                                                                                         dtype = tf.float64),\n",
    "                                                                  trainable = False)\n",
    "        \n",
    "        absolute_error_covariance_matrix_batch_time_variable = tf.get_variable(name = \"absolute_error_covariance_matrix_batch_time_variable\", # shape = (number_of_features, number_of_features)\n",
    "                                                                               shape = [number_of_features, number_of_features],\n",
    "                                                                               dtype = tf.float64,\n",
    "                                                                               initializer = tf.zeros(shape = [number_of_features, number_of_features], \n",
    "                                                                                                      dtype = tf.float64),\n",
    "                                                                               trainable = False)\n",
    "\n",
    "        absolute_error_inverse_covariance_matrix_batch_time_variable = tf.get_variable(name = \"absolute_error_inverse_covariance_matrix_batch_time_variable\", # shape = (number_of_features, number_of_features)\n",
    "                                                                                       shape = [number_of_features, number_of_features],\n",
    "                                                                                       dtype = tf.float64,\n",
    "                                                                                       initializer = tf.zeros(shape = [number_of_features, number_of_features], \n",
    "                                                                                                              dtype = tf.float64),\n",
    "                                                                                       trainable = False)\n",
    "\n",
    "        # Features based\n",
    "        absolute_error_count_batch_features_variable = tf.get_variable(name = \"absolute_error_count_batch_features_variable\", # shape = ()\n",
    "                                                                       shape = 1,\n",
    "                                                                       dtype = tf.float64,\n",
    "                                                                       initializer = tf.zeros(shape = 1, \n",
    "                                                                                              dtype = tf.float64),\n",
    "                                                                       trainable = False)\n",
    "        \n",
    "        absolute_error_mean_batch_features_variable = tf.get_variable(name = \"absolute_error_mean_batch_features_variable\", # shape = (sequence_length,)\n",
    "                                                                      shape = [params[\"sequence_length\"]],\n",
    "                                                                      dtype = tf.float64,\n",
    "                                                                      initializer = tf.zeros(shape = [params[\"sequence_length\"]], \n",
    "                                                                                             dtype = tf.float64),\n",
    "                                                                      trainable = False)\n",
    "        \n",
    "        absolute_error_covariance_matrix_batch_features_variable = tf.get_variable(name = \"absolute_error_covariance_matrix_batch_features_variable\", # shape = (sequence_length, sequence_length)\n",
    "                                                                                   shape = [params[\"sequence_length\"], params[\"sequence_length\"]],\n",
    "                                                                                   dtype = tf.float64,\n",
    "                                                                                   initializer = tf.zeros(shape = [params[\"sequence_length\"], params[\"sequence_length\"]], \n",
    "                                                                                                          dtype = tf.float64),\n",
    "                                                                                   trainable = False)\n",
    "\n",
    "        absolute_error_inverse_covariance_matrix_batch_features_variable = tf.get_variable(name = \"absolute_error_inverse_covariance_matrix_batch_features_variable\", # shape = (sequence_length, sequence_length)\n",
    "                                                                                           shape = [params[\"sequence_length\"], params[\"sequence_length\"]],\n",
    "                                                                                           dtype = tf.float64,\n",
    "                                                                                           initializer = tf.zeros(shape = [params[\"sequence_length\"], params[\"sequence_length\"]], \n",
    "                                                                                                                  dtype = tf.float64),\n",
    "                                                                                           trainable = False)\n",
    "    \n",
    "    # Now branch off based on which mode we are in\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "    \n",
    "    # 3. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        \n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step(),\n",
    "            learning_rate = params[\"learning_rate\"],\n",
    "            optimizer = \"Adam\")\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        eval_metric_ops = {\n",
    "            \"rmse\": tf.metrics.root_mean_squared_error(labels = Y, predictions = predictions),\n",
    "            \"mae\": tf.metrics.mean_absolute_error(labels = Y, predictions = predictions)\n",
    "        }\n",
    "\n",
    "        if params[\"calculate_error_distribution_statistics\"] == True:\n",
    "            absolute_error = tf.abs(x = Y - predictions) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error = \\n{}\".format(absolute_error))\n",
    "\n",
    "            ################################################################################\n",
    "    \n",
    "            with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "                # This function updates the count of records used\n",
    "                def update_count(count_a, count_b):\n",
    "                    return count_a + count_b\n",
    "                \n",
    "                def singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable):\n",
    "                    # This function updates the mean vector incrementally\n",
    "                    def update_mean_incremental(count_a, mean_a, value_b):\n",
    "                        return (mean_a * count_a + tf.squeeze(input = value_b, axis = 0)) / (count_a + 1)\n",
    "\n",
    "                    # This function updates the covariance matrix incrementally\n",
    "                    def update_covariance_incremental(count_a, mean_a, cov_a, value_b, mean_ab, sample_covariance):\n",
    "                        if sample_covariance == True:\n",
    "                            cov_ab = (cov_a * (count_a - 1) + tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, tranpose_a = True)) / count_a\n",
    "                        else:\n",
    "                            cov_ab = (cov_a * count_a + tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, tranpose_a = True)) / (count_a + 1)\n",
    "                        return cov_ab\n",
    "                    \n",
    "                    # Time based\n",
    "                    mean_ab = update_mean_incremental(n_variable, mean_variable, batch_data)\n",
    "\n",
    "                    cov_variable = update_covariance_incremental(n_variable, mean_variable, cov_variable, batch_data, mean_ab, True)\n",
    "                    mean_variable = mean_ab\n",
    "                    n_variable = update_n(n_variable, batch_n)\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    \n",
    "                    # Features based\n",
    "                    \n",
    "                    \n",
    "                    return absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable\n",
    "                                                                                   \n",
    "                def non_singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable):\n",
    "                    # This function updates the mean vector using a batch of data\n",
    "                    def update_mean_batch(count_a, mean_a, count_b, mean_b):\n",
    "                        return (mean_a * count_a + mean_b * count_b) / (count_a + count_b)\n",
    "\n",
    "                    # This function updates the covariance matrix using a batch of data\n",
    "                    def update_covariance_batch(count_a, mean_a, cov_a, count_b, mean_b, cov_b, sample_covariance):\n",
    "                        mean_diff = tf.expand_dims(input = mean_a - mean_b, axis = 0)\n",
    "\n",
    "                        if sample_covariance == True:\n",
    "                            cov_ab = (cov_a * (count_a - 1) + cov_b * (count_b - 1) + tf.matmul(a = mean_diff, b = mean_diff, tranpose_a = True) * (count_a * count_b) / (count_a + count_b)) / (count_a + count_b - 1)\n",
    "                        else:\n",
    "                            cov_ab = (cov_a * count_a + cov_b * count_b + tf.matmul(a = mean_diff, b = mean_diff, tranpose_a = True) * (count_a * count_b) / (count_a + count_b)) / (count_a + count_b)\n",
    "                        return cov_ab\n",
    "                    \n",
    "                    # Time based\n",
    "                    \n",
    "                    # Find statistics of batch\n",
    "                    absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error, shape = [current_batch_size * params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "\n",
    "                    absolute_error_mean_batch_time = tf.reduce_mean(input_tensor = absolute_error_reshaped_batch_time, axis = 0) # shape = (current_batch_size * sequence_length,)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mean_batch_time = \\n{}\".format(absolute_error_mean_batch_time))\n",
    "\n",
    "                    absolute_error_reshaped_batch_time_centered = absolute_error_reshaped_batch_time - absolute_error_mean_batch_time # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time_centered = \\n{}\".format(absolute_error_reshaped_batch_time_centered))\n",
    "\n",
    "                    absolute_error_reshaped_batch_time_covariance_matrix = tf.matmul(a = absolute_error_reshaped_batch_time_centered, # shape = (number_of_features, number_of_features)\n",
    "                                                                                     b = absolute_error_reshaped_batch_time_centered, \n",
    "                                                                                     transpose_a = True) / tf.cast(x = current_batch_size * params[\"sequence_length\"] - 1, dtype = tf.float64)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_time_covariance_matrix))\n",
    "                    \n",
    "                    # Update running variables from batch statistics\n",
    "                    absolute_error_covariance_matrix_batch_time_variable.assign(value = update_covariance_batch(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                                                mean_a = absolute_error_mean_batch_time_variable, \n",
    "                                                                                                                cov_a = absolute_error_covariance_matrix_batch_time_variable, \n",
    "                                                                                                                count_b = current_batch_size, \n",
    "                                                                                                                mean_b = absolute_error_mean_batch_time, \n",
    "                                                                                                                cov_b = absolute_error_reshaped_batch_time_covariance_matrix, \n",
    "                                                                                                                sample_covariance = True)) # shape = (number_of_features, number_of_features)\n",
    "                    \n",
    "                    absolute_error_mean_batch_time_variable.assign(value = update_mean_batch(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                             mean_a = absolute_error_mean_batch_time_variable, \n",
    "                                                                                             count_b = current_batch_size, \n",
    "                                                                                             mean_b = absolute_error_mean_batch_time)) # shape = (number_of_features,)\n",
    "                    \n",
    "                    absolute_error_count_batch_time_variable.assign(value = update_count(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                         count_b = current_batch_size)) # shape = ()\n",
    "\n",
    "                    absolute_error_inverse_covariance_matrix_batch_time_variable.assign(value = tf.matrix_inverse(input = absolute_error_covariance_matrix_batch_time_variable)) # shape = (number_of_features, number_of_features)\n",
    "\n",
    "                    ################################################################################\n",
    "\n",
    "                    # Features based\n",
    "                    \n",
    "                    # Find statistics of batch\n",
    "                    absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                                       elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int32), \n",
    "                                                                       dtype = tf.float64)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "                    absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, shape = [current_batch_size * number_of_features, params[\"sequence_length\"]]) # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "\n",
    "                    absolute_error_mean_batch_features = tf.reduce_mean(input_tensor = absolute_error_reshaped_batch_features, axis = 0) # shape = (sequence_length,)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mean_batch_features = \\n{}\".format(absolute_error_mean_batch_features))\n",
    "\n",
    "                    absolute_error_reshaped_batch_features_centered = absolute_error_reshaped_batch_features - absolute_error_mean_batch_features # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features_centered = \\n{}\".format(absolute_error_reshaped_batch_features_centered))\n",
    "\n",
    "                    absolute_error_reshaped_batch_features_covariance_matrix = tf.matmul(a = absolute_error_reshaped_batch_features_centered, # shape = (sequence_length, sequence_length)\n",
    "                                                                                         b = absolute_error_reshaped_batch_features_centered, \n",
    "                                                                                         transpose_a = True) / tf.cast(x = current_batch_size * number_of_features - 1, dtype = tf.float64)\n",
    "                    print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_features_covariance_matrix))\n",
    "\n",
    "                    # Update running variables from batch statistics\n",
    "                    absolute_error_covariance_matrix_batch_features_variable.assign(value = update_covariance_batch(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                                                    mean_a = absolute_error_mean_batch_features_variable, \n",
    "                                                                                                                    cov_a = absolute_error_covariance_matrix_batch_features_variable, \n",
    "                                                                                                                    count_b = current_batch_size, \n",
    "                                                                                                                    mean_b = absolute_error_mean_batch_features, \n",
    "                                                                                                                    cov_b = absolute_error_reshaped_batch_features_covariance_matrix, \n",
    "                                                                                                                    sample_covariance = True)) # shape = (sequence_length, sequence_length)\n",
    "                    \n",
    "                    absolute_error_mean_batch_features_variable.assign(value = update_mean_batch(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                                 mean_a = absolute_error_mean_batch_features_variable, \n",
    "                                                                                                 count_b = current_batch_size, \n",
    "                                                                                                 mean_b = absolute_error_mean_batch_features)) # shape = (sequence_length,)\n",
    "                    \n",
    "                    absolute_error_count_batch_features_variable.assign(value = update_count(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                             count_b = current_batch_size)) # shape = ()\n",
    "\n",
    "                    absolute_error_inverse_covariance_matrix_batch_features_variable.assign(value = tf.matrix_inverse(input = absolute_error_covariance_matrix_batch_features_variable)) # shape = (sequence_length, sequence_length)\n",
    "\n",
    "                    return absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable\n",
    "                \n",
    "                # Check if batch is a singleton or not, very important for covariance math\n",
    "                singleton_batch_condition = tf.equal(x = current_batch_size, y = 1) # shape = ()\n",
    "                \n",
    "                absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable = \\\n",
    "                    tf.cond(pred = singlet_batch_condition, \n",
    "                            true_fn = singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable), \n",
    "                            false_fn = non_singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable))\n",
    "                \n",
    "    else: # mode == tf.estimator.ModeKeys.PREDICT\n",
    "        def mahalanobis_distance(error_vectors_reshaped, mean_vector, inverse_covariance_matrix, final_shape):\n",
    "            error_vectors_reshaped_centered = error_vectors_reshaped - mean_vector # time_shape = (current_batch_size * sequence_length, number_of_features), features_shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: error_vectors_reshaped_centered = \\n{}\".format(error_vectors_reshaped_centered))\n",
    "\n",
    "            mahalanobis_right_matrix_product = tf.matmul(a = inverse_covariance_matrix, # time_shape = (number_of_features, current_batch_size * sequence_length), features_shape = (sequence_length, current_batch_size * number_of_features)\n",
    "                                                         b = error_vectors_reshaped_centered,\n",
    "                                                         transpose_b = True)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_right_matrix_product = \\n{}\".format(mahalanobis_right_matrix_product))\n",
    "\n",
    "\n",
    "            mahalanobis_distance_vectorized = tf.matmul(a = error_vectors_reshaped_centered, # time_shape = (current_batch_size * sequence_length, current_batch_size * sequence_length), features_shape = (current_batch_size * number_of_features, current_batch_size * number_of_features)\n",
    "                                                        b = mahalanobis_right_matrix_product)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_vectorized = \\n{}\".format(mahalanobis_distance_vectorized))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_vectorized.shape = \\n{}\".format(mahalanobis_distance_vectorized.shape))\n",
    "\n",
    "            mahalanobis_distance_flat = tf.diag_part(input = mahalanobis_distance_vectorized) # time_shape = (current_batch_size * sequence_length,), features_shape = (current_batch_size * number_of_features,)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_flat = \\n{}\".format(mahalanobis_distance_flat))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_flat.shape = \\n{}\".format(mahalanobis_distance_flat.shape))\n",
    "\n",
    "            mahalanobis_distance_final_shaped = tf.reshape(tensor = mahalanobis_distance_flat, shape = [-1, final_shape]) # time_shape = (current_batch_size, sequence_length), features_shape = (current_batch_size, number_of_features)\n",
    "\n",
    "            return mahalanobis_distance_final_shaped\n",
    "          \n",
    "        absolute_error = tf.abs(x = Y - predictions) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error = \\n{}\".format(absolute_error))\n",
    "        \n",
    "        with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error,  # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "                                                            shape = [current_batch_size * params[\"sequence_length\"], number_of_features])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "\n",
    "            mahalanobis_distance_batch_time = mahalanobis_distance(error_vectors_reshaped = absolute_error_reshaped_batch_time,  # shape = (current_batch_size, sequence_length)\n",
    "                                                                   mean_vector = absolute_error_mean_batch_time_variable, \n",
    "                                                                   inverse_covariance_matrix = absolute_error_inverse_covariance_matrix_batch_time_variable, \n",
    "                                                                   final_shape = params[\"sequence_length\"])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_time = \\n{}\".format(mahalanobis_distance_batch_time))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_time.shape = \\n{}\".format(mahalanobis_distance_batch_time.shape))\n",
    "\n",
    "            # Features based\n",
    "            absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                             elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int32), \n",
    "                                                             dtype = tf.float64)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "\n",
    "            absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "                                                                shape = [current_batch_size * number_of_features, params[\"sequence_length\"]])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "\n",
    "            mahalanobis_distance_batch_features = mahalanobis_distance(error_vectors_reshaped = absolute_error_reshaped_batch_features, # shape = (current_batch_size, number_of_features)\n",
    "                                                                       mean_vector = absolute_error_mean_batch_features_variable, \n",
    "                                                                       inverse_covariance_matrix = absolute_error_inverse_covariance_matrix_batch_features_variable,\n",
    "                                                                       final_shape = number_of_features)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_features = \\n{}\".format(mahalanobis_distance_batch_features))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_features.shape = \\n{}\".format(mahalanobis_distance_batch_features.shape))\n",
    "            \n",
    "        batch_time_anomaly_flags = tf.where(condition = tf.reduce_any(input_tensor = tf.greater(x = tf.abs(x = mahalanobis_distance_batch_time), # shape = (current_batch_size,)\n",
    "                                                                                                y = params[\"time_anomaly_threshold\"]), \n",
    "                                                                      axis = 1), \n",
    "                                            x = tf.ones(shape = [current_batch_size], dtype = tf.int32), \n",
    "                                            y = tf.zeros(shape = [current_batch_size], dtype = tf.int32))\n",
    "        \n",
    "        batch_features_anomaly_flags = tf.where(condition = tf.reduce_any(input_tensor = tf.greater(x = tf.abs(x = mahalanobis_distance_batch_features), # shape = (current_batch_size,)\n",
    "                                                                                                    y = params[\"features_anomaly_threshold\"]), \n",
    "                                                                          axis = 1), \n",
    "                                                x = tf.ones(shape = [current_batch_size], dtype = tf.int32), \n",
    "                                                y = tf.zeros(shape = [current_batch_size], dtype = tf.int32))\n",
    "        \n",
    "        # Create predictions\n",
    "        predictions_dict = {\"predictions\": predictions, \n",
    "                            \"mahalanobis_distance_batch_time\": mahalanobis_distance_batch_time, \n",
    "                            \"mahalanobis_distance_batch_features\": mahalanobis_distance_batch_features, \n",
    "                            \"batch_time_anomaly_flags\": batch_time_anomaly_flags, \n",
    "                            \"batch_features_anomaly_flags\": batch_features_anomaly_flags}\n",
    "\n",
    "        # Create export outputs\n",
    "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = predictions_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our serving input function to accept the data at serving and send it in the right format to our custom estimator\n",
    "def serving_input_fn(sequence_length):\n",
    "    # This function fixes the shape and type of our input strings\n",
    "    def fix_shape_and_type_for_serving(placeholder):\n",
    "        current_batch_size = tf.shape(input = placeholder, out_type = tf.int64)[0]\n",
    "        \n",
    "        # String split each string in the batch and output the values from the resulting SparseTensors\n",
    "        split_string = tf.stack(values = tf.map_fn( # shape = (batch_size, sequence_length)\n",
    "            fn = lambda x: tf.string_split(source = [placeholder[x]], delimiter = ',').values, \n",
    "            elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "            dtype = tf.string), axis = 0)\n",
    "#         print(\"serving_input_fn: fix_shape_and_type_for_serving: split_string = {}\".format(split_string))\n",
    "        \n",
    "        # Convert each string in the split tensor to float\n",
    "        feature_tensor = tf.string_to_number(string_tensor = split_string, out_type = tf.float64) # shape = (batch_size, sequence_length)\n",
    "#         print(\"serving_input_fn: fix_shape_and_type_for_serving: feature_tensor = {}\".format(feature_tensor))\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    # This function fixes dynamic shape ambiguity of last dimension so that we will be able to use it in our DNN (since tf.layers.dense require the last dimension to be known)\n",
    "    def get_shape_and_set_modified_shape_2D(tensor, additional_dimension_sizes):\n",
    "        # Get static shape for tensor and convert it to list\n",
    "        shape = tensor.get_shape().as_list()\n",
    "        # Set outer shape to additional_dimension_sizes[0] since we know that this is the correct size\n",
    "        shape[1] = additional_dimension_sizes[0]\n",
    "        # Set the shape of tensor to our modified shape\n",
    "        tensor.set_shape(shape = shape) # shape = (batch_size, additional_dimension_sizes[0])\n",
    "#         print(\"serving_input_fn: get_shape_and_set_modified_shape_2D: tensor = {}, additional_dimension_sizes = {}\".format(tensor, additional_dimension_sizes))\n",
    "        return tensor\n",
    "            \n",
    "    # Create placeholders to accept the data sent to the model at serving time\n",
    "    feature_placeholders = { # all features come in as a batch of strings, shape = (batch_size,), this was so because of passing the arrays to online ml-engine prediction\n",
    "        feature: tf.placeholder(dtype = tf.string, shape = [None]) for feature in CSV_COLUMNS\n",
    "    }\n",
    "#     print(\"\\nserving_input_fn: feature_placeholders = {}\".format(feature_placeholders))\n",
    "    \n",
    "    # Create feature tensors\n",
    "    features = {key: fix_shape_and_type_for_serving(placeholder = tensor) for key, tensor in feature_placeholders.items()}\n",
    "#     print(\"serving_input_fn: features = {}\".format(features))\n",
    "    \n",
    "    # Fix dynamic shape ambiguity of feature tensors for our DNN\n",
    "    features = {key: get_shape_and_set_modified_shape_2D(tensor = tensor, additional_dimension_sizes = [sequence_length]) for key, tensor in features.items()}\n",
    "#     print(\"serving_input_fn: features = {}\".format(features))\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(args):\n",
    "    if args[\"tune_anomaly_thresholds\"] == False:\n",
    "        if args[\"calculate_error_distribution_statistics\"] == False:\n",
    "            # Create our custom estimator using our model function without calculating error distribution statistics\n",
    "            estimator = tf.estimator.Estimator(\n",
    "                model_fn = lstm_encoder_decoder_autoencoder_anomaly_detection,\n",
    "                model_dir = args['output_dir'],\n",
    "                params = {\n",
    "                    \"sequence_length\": args[\"sequence_length\"],\n",
    "                    \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "                    \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "                    \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "                    \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "                    \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "                    \"learning_rate\": args[\"learning_rate\"],\n",
    "                    \"calculate_error_distribution_statistics\": args[\"calculate_error_distribution_statistics\"],\n",
    "                    \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "                    \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "\n",
    "            early_stopping_hook = tf.contrib.estimator.stop_if_no_decrease_hook(\n",
    "                estimator = estimator,\n",
    "                metric_name = \"rmse\",\n",
    "                max_steps_without_decrease = 100,\n",
    "                min_steps = 1000,\n",
    "                run_every_secs = 60,\n",
    "                run_every_steps = None)\n",
    "\n",
    "            # Create train spec to read in our training data\n",
    "            train_spec = tf.estimator.TrainSpec(\n",
    "                input_fn = read_dataset(\n",
    "                    filename = args[\"train_file_pattern\"],\n",
    "                    mode = tf.estimator.ModeKeys.TRAIN, \n",
    "                    batch_size = args[\"train_batch_size\"],\n",
    "                    params = args),\n",
    "                max_steps = args[\"train_steps\"], \n",
    "                hooks = [early_stopping_hook])\n",
    "\n",
    "#             # Create exporter to save out the complete model to disk\n",
    "#             exporter = tf.estimator.BestExporter(\n",
    "#                 name = \"best_exporter\", \n",
    "#                 serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]),\n",
    "#                 exports_to_keep = 5)\n",
    "\n",
    "            # Create eval spec to read in our validation data and export our model\n",
    "            eval_spec = tf.estimator.EvalSpec(\n",
    "                input_fn = read_dataset(\n",
    "                    filename = args[\"eval_file_pattern\"], \n",
    "                    mode = tf.estimator.ModeKeys.EVAL, \n",
    "                    batch_size = args[\"eval_batch_size\"],\n",
    "                    params = args),\n",
    "                steps = None,\n",
    "                start_delay_secs = args[\"start_delay_secs\"], # start evaluating after N seconds\n",
    "                throttle_secs = args[\"throttle_secs\"])#,    # evaluate every N seconds\n",
    "#                 exporters = exporter)\n",
    "\n",
    "            # Create train and evaluate loop to train and evaluate our estimator\n",
    "            tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
    "        else: # args[\"calculate_error_distribution_statistics\"] == True\n",
    "            # Create our custom estimator using our model function NOW calculating error distribution statistics\n",
    "            estimator = tf.estimator.Estimator(\n",
    "                model_fn = lstm_encoder_decoder_autoencoder_anomaly_detection,\n",
    "                model_dir = args['output_dir'],\n",
    "                params = {\n",
    "                    \"sequence_length\": args[\"sequence_length\"],\n",
    "                    \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "                    \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "                    \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "                    \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "                    \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "                    \"learning_rate\": args[\"learning_rate\"],\n",
    "                    \"calculate_error_distribution_statistics\": args[\"calculate_error_distribution_statistics\"],\n",
    "                    \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "                    \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "\n",
    "            # Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "            estimator.evaluate(\n",
    "                input_fn = read_dataset(\n",
    "                    filename = arguments[\"eval_file_pattern\"], \n",
    "                    mode = tf.estimator.ModeKeys.EVAL, \n",
    "                    batch_size = 2**16,\n",
    "                    params = args),\n",
    "                steps = None)\n",
    "\n",
    "            # Export savedmodel with learned error distribution statistics to be used for inference\n",
    "\n",
    "            estimator.export_savedmodel(\n",
    "                export_dir_base = args['output_dir'], \n",
    "                serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]))\n",
    "    else: # args[\"tune_anomaly_thresholds\"] == True\n",
    "        # Create our custom estimator using our model function NOW calculating error distribution statistics\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn = lstm_encoder_decoder_autoencoder_anomaly_detection,\n",
    "            model_dir = args['output_dir'],\n",
    "            params = {\n",
    "                \"sequence_length\": args[\"sequence_length\"],\n",
    "                \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "                \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "                \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "                \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "                \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "                \"learning_rate\": args[\"learning_rate\"],\n",
    "                \"calculate_error_distribution_statistics\": False,\n",
    "                \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "                \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "\n",
    "        # Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "        estimator.evaluate(\n",
    "            input_fn = read_dataset(\n",
    "                filename = arguments[\"eval_file_pattern\"], \n",
    "                mode = tf.estimator.ModeKeys.EVAL, \n",
    "                batch_size = 2**16,\n",
    "                params = args),\n",
    "            steps = None)\n",
    "\n",
    "        # Export savedmodel with learned error distribution statistics to be used for inference\n",
    "\n",
    "        estimator.export_savedmodel(\n",
    "            export_dir_base = args['output_dir'], \n",
    "            serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]))\n",
    "        \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"train_file_pattern\"] = \"data/training_normal_sequences.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/validation_normal_1_sequences.csv\"\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Sequence shape hyperparameters\n",
    "arguments[\"sequence_length\"] = sequence_length\n",
    "arguments[\"horizon\"] = 0\n",
    "arguments[\"reverse_labels_sequence\"] = True\n",
    "\n",
    "# Architecture hyperparameters\n",
    "\n",
    "# LSTM hyperparameters\n",
    "arguments[\"encoder_lstm_hidden_units\"] = [64, 32, 16]\n",
    "arguments[\"decoder_lstm_hidden_units\"] = [16, 32, 64]\n",
    "arguments[\"lstm_dropout_output_keep_probs\"] = [1.0, 1.0, 1.0]\n",
    "\n",
    "# DNN hyperparameters\n",
    "arguments[\"dnn_hidden_units\"] = [1024, 256, 64]\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "arguments[\"train_steps\"] = 100\n",
    "arguments[\"learning_rate\"] = 0.01\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120\n",
    "\n",
    "# Anomaly thresholds\n",
    "arguments[\"calculate_error_distribution_statistics\"] = False\n",
    "arguments[\"tune_anomaly_thresholds\"] = False\n",
    "arguments[\"time_anomaly_threshold\"] = 2.0\n",
    "arguments[\"features_anomaly_threshold\"] = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_save_summary_steps': 100, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_log_step_count_steps': 100, '_train_distribute': None, '_experimental_distribute': None, '_global_id_in_cluster': 0, '_device_fn': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fef2a0cd080>, '_service': None, '_model_dir': 'trained_model', '_keep_checkpoint_max': 5, '_eval_distribute': None, '_task_id': 0, '_master': '', '_num_worker_replicas': 1, '_protocol': None, '_num_ps_replicas': 0, '_task_type': 'worker', '_evaluation_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3878543, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 100 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-19T21:27:14Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-19-21:27:16\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 0.015896702, mae = 0.095140144, rmse = 0.12597579\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Loss for final step: 0.016239801.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "shutil.rmtree(path = arguments[\"output_dir\"], ignore_errors = True) # start fresh each time\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_save_summary_steps': 100, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_log_step_count_steps': 100, '_train_distribute': None, '_experimental_distribute': None, '_global_id_in_cluster': 0, '_device_fn': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff047eaf048>, '_service': None, '_model_dir': 'trained_model', '_keep_checkpoint_max': 5, '_eval_distribute': None, '_task_id': 0, '_master': '', '_num_worker_replicas': 1, '_protocol': None, '_num_ps_replicas': 0, '_task_type': 'worker', '_evaluation_master': ''}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-19T21:27:21Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[15000,256] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dnn/dense_1/MatMul (defined at <ipython-input-26-7e53fd92d74d>:192) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dnn/dense_1/MatMul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1424, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 542, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-50765bce9976>\", line 2, in <module>\n    estimator = train_and_evaluate(arguments)\n  File \"<ipython-input-35-fa4063b0212e>\", line 83, in train_and_evaluate\n    steps = None)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 469, in evaluate\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 511, in _actual_eval\n    return _evaluate()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 493, in _evaluate\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1424, in _evaluate_build_graph\n    self._call_model_fn_eval(input_fn, self.config))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1460, in _call_model_fn_eval\n    features, labels, model_fn_lib.ModeKeys.EVAL, config)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"<ipython-input-26-7e53fd92d74d>\", line 192, in lstm_encoder_decoder_autoencoder_anomaly_detection\n    network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size * sequence_length, dnn_hidden_units[i])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/core.py\", line 188, in dense\n    return layer.apply(inputs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1227, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 530, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py\", line 975, in call\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5333, in mat_mul\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[15000,256] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dnn/dense_1/MatMul (defined at <ipython-input-26-7e53fd92d74d>:192) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[15000,256] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dnn/dense_1/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-50765bce9976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"calculate_error_distribution_statistics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-fa4063b0212e>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     params = args),\n\u001b[0;32m---> 83\u001b[0;31m                 steps = None)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Export savedmodel with learned error distribution statistics to be used for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    467\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m   def _actual_eval(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_actual_eval\u001b[0;34m(self, input_fn, strategy, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0meval_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mall_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             output_dir=self.eval_dir(name))\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_run\u001b[0;34m(self, checkpoint_path, scaffold, update_op, eval_dict, all_hooks, output_dir)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         config=self._session_config)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0mcurrent_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_STEP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    674\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[15000,256] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dnn/dense_1/MatMul (defined at <ipython-input-26-7e53fd92d74d>:192) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dnn/dense_1/MatMul', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1424, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 542, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-50765bce9976>\", line 2, in <module>\n    estimator = train_and_evaluate(arguments)\n  File \"<ipython-input-35-fa4063b0212e>\", line 83, in train_and_evaluate\n    steps = None)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 469, in evaluate\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 511, in _actual_eval\n    return _evaluate()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 493, in _evaluate\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1424, in _evaluate_build_graph\n    self._call_model_fn_eval(input_fn, self.config))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1460, in _call_model_fn_eval\n    features, labels, model_fn_lib.ModeKeys.EVAL, config)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"<ipython-input-26-7e53fd92d74d>\", line 192, in lstm_encoder_decoder_autoencoder_anomaly_detection\n    network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size * sequence_length, dnn_hidden_units[i])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/core.py\", line 188, in dense\n    return layer.apply(inputs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1227, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/layers/base.py\", line 530, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py\", line 975, in call\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5333, in mat_mul\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[15000,256] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dnn/dense_1/MatMul (defined at <ipython-input-26-7e53fd92d74d>:192) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "arguments[\"calculate_error_distribution_statistics\"] = True\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments[\"tune_anomaly_thresholds\"] = True\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now write into a python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/lstm_autoencoder_anomaly_detection_module\n",
    "python -m trainer.task \\\n",
    "    --train_file_pattern=\"data/training_normal_sequences.csv\" \\\n",
    "    --eval_file_pattern=\"data/validation_normal_1_sequences.csv\"  \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --batch_size=32 \\\n",
    "    --sequence_length=13 \\\n",
    "    --horizon=0 \\\n",
    "    --reverse_labels_sequence=True \\\n",
    "    --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "    --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "    --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "    --dnn_hidden_units=\"1024 256 64\" \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.1 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r data gs://$BUCKET/lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/trained_model\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/training_normal_sequences.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/validation_normal_1_sequences.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --batch_size=32 \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "  --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "  --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "  --dnn_hidden_units=\"1024 256 64\" \\\n",
    "  --train_steps=1000 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: mae\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 1\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: sequence_length\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 120\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: encoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: decoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: lstm_dropout_output_keep_probs\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"0.9 1.0 1.0\", \"0.95 0.95 1.0\", \"0.95 0.95 0.95\"]\n",
    "    - parameterName: dnn_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"256 128 64\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/hyperparam\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET/lstm_autoencoder/staging \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/train.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/eval.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r trained_model gs://qwiklabs-gcp-8923d4964bfbd247-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"lstm_autoencoder_anomaly_detection\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://$BUCKET/trained_model/export/exporter/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create $MODEL_NAME --regions $REGION\n",
    "gcloud ml-engine versions create $MODEL_VERSION --model $MODEL_NAME --origin $MODEL_LOCATION --runtime-version 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_instances = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local prediction from local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sequences.json', 'w') as outfile:\n",
    "  test_data_anomalous_string_list = [[np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list] for _ in range(0, number_of_prediction_instances)]\n",
    "  json_string = \"\"\n",
    "  for item in test_data_anomalous_string_list:\n",
    "    json_string += \"{\" + ','.join([\"{0}: \\\"{1}\\\"\".format('\\\"' + CSV_COLUMNS[i] + '\\\"', item[i]) for i in range(0, len(CSV_COLUMNS))]) + \"}\\n\"\n",
    "  json_string = json_string.replace(' ', '').replace(':', ': ').replace(',', ', ')\n",
    "  outfile.write(\"%s\" % json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_FEATURES_ANOMALY_FLAGS  BATCH_TIME_ANOMALY_FLAGS  MAHALANOBIS_DISTANCE_BATCH_FEATURES                            MAHALANOBIS_DISTANCE_BATCH_TIME                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     PREDICTIONS\n",
      "0                             0                         [0.3609838334886698, 4.285870630025189, 7.331315729850488]     [11.200225051015526, 6.302422447857527, 7.133295460598505, 3.6845064928930844, 1.1992155852273427, -0.5447650456691467, 1.9361679857458611, 2.5135337828994233, 3.3670350134756912, 2.801834665525715, 3.2638614318900023, 3.541391080605428, 3.664099186170625, 3.416207555117292, 2.564566278260998, 3.098551595086743, 2.8757662326783655, 2.944446882900374, 2.5923076746097533, 2.1807012131847516, 2.0858074862031426, 1.9917409626733913, 1.62776007937862, 1.8516518120374328, 1.6187862158429356, 1.3506691819893726, 1.4266776508038121, 0.916245277689105, 1.1588153124928775, 0.6932429883551378]       [[0.40790831607535405, 0.1314709457844174, 0.5576687073811238], [0.43602587445659946, 0.17757222683221155, 0.6069015880124863], [0.3512127548797239, 0.21285888671972972, 0.5175151726411527], [0.1909099143044662, 0.21988556982960333, 0.34479207136292733], [0.040418973163793795, 0.17753858364928443, 0.1429725034309734], [-0.0509568655966606, 0.09194563852431625, 0.004355091659595883], [-0.08330783203938308, 0.016570912079525543, -0.09101288320166176], [-0.06881216601098997, -0.029769675029759583, -0.14710909735112845], [-0.02545422735480067, -0.054139099720379724, -0.17432250347661074], [0.026162005532254992, -0.06318016409946059, -0.18271081384344537], [0.07035114897109203, -0.06326862594010416, -0.17972862965423084], [0.10298317583567794, -0.06115432572093968, -0.17086195101100035], [0.12275486068196033, -0.059759529818670276, -0.15992576179039994], [0.1325446447021621, -0.059709927281351854, -0.14905565544376917], [0.13507058867958704, -0.060570428974457, -0.1395431427826245], [0.1333047929537547, -0.06186325450600283, -0.13174619248597638], [0.12957501972301144, -0.06311568963235759, -0.12568779032052324], [0.12543274141445465, -0.06405043521587322, -0.12118708780378004], [0.12173961642047527, -0.06459182164907215, -0.11800065300607823], [0.11885158671119699, -0.06477537813727713, -0.11585305316272268], [0.11683186575243774, -0.06469143568155641, -0.11448686363663177], [0.11556024402109737, -0.0644478028583449, -0.11369037975106483], [0.11484933828459688, -0.06413491124649368, -0.11329287081646916], [0.11451330110251758, -0.06381515455948505, -0.11315895852080007], [0.11439953940226548, -0.06352461598267614, -0.11318362323599965], [0.11439803253932944, -0.06327971809577798, -0.1132899117191822], [0.1144384239273764, -0.06308409238539195, -0.11342533346170625], [0.11448172340816186, -0.06293406708460286, -0.11355719650153115], [0.1145107309835416, -0.06282258234253757, -0.11366782412073476], [0.1145212757344398, -0.06274161820933444, -0.11375020325989943]]\n",
      "0                             0                         [0.8219883497032462, 0.055461907539454736, 5.796212136374984]  [1.8645170892564253, 3.9836337735147618, 1.8542347166844362, -0.033575393978855395, 6.145771325074644, 1.768587041978376, 2.3834463585553176, 2.574674427996713, 3.2025060035922506, 3.462554262137925, 2.9688014712041824, 3.6392797244896276, 2.4264204124998616, 2.959295282571362, 2.80761968566804, 2.9381641266901752, 2.4564605322672155, 2.5326728489893084, 2.329083831591694, 2.4966532231496714, 2.6100743493246377, 2.2269274862369937, 1.8940177291539273, 1.2706479646215427, 2.0727683721480963, 1.283321209134646, 1.2595577603039132, 1.3285245101596748, 0.7936106573927502, 0.8289920919325134]  [[0.37855912823148397, 0.3276541141938409, 0.5632095514622795], [0.3501254623144073, 0.40179052461442666, 0.6251374222675912], [0.22515646361341451, 0.3602035910439996, 0.5419469824099867], [0.08054095374897387, 0.25646720021005315, 0.3662084132746196], [-0.03151181099608344, 0.11457717050204691, 0.1582802260107087], [-0.0735656770467564, -0.0015116772693178938, 0.0012455391787867809], [-0.06506950284183292, -0.062137909794032886, -0.10367219303485772], [-0.023467819203992712, -0.07973294050354368, -0.16834460180524838], [0.0265330855539838, -0.07879421952072602, -0.19678329564302918], [0.07057309376638986, -0.0707319806127257, -0.2010697614898708], [0.10301517396811792, -0.06387586121475072, -0.19211571596958757], [0.1228883435207395, -0.0601743945077822, -0.17758698776082535], [0.13245917797754408, -0.05963134608009171, -0.16206085728971217], [0.1348398765249712, -0.06067834918805032, -0.14827145051346688], [0.13301327220448572, -0.062305068821883136, -0.1370436583088163], [0.12932313629958392, -0.06378481999272204, -0.12850752240003463], [0.12526404902580357, -0.06476952444161771, -0.12235315458617271], [0.12167188230255364, -0.06521059217947714, -0.11813091332520045], [0.11886763975935492, -0.06524063241560435, -0.11542327699190935], [0.116894657302246, -0.06499721505617723, -0.11380188466180594], [0.11564816298125981, -0.06461904012203379, -0.11294234625261126], [0.11494736842320051, -0.06420762259338653, -0.11259339403471624], [0.11461117628280672, -0.06382351370814521, -0.11256543991687998], [0.11449046256068489, -0.06349531194356842, -0.11271733745163773], [0.11447777034805405, -0.06323117599133868, -0.11294912650683739], [0.1145046955963535, -0.063027991673204, -0.11319510701997565], [0.11453371482929951, -0.06287737118269199, -0.11341656780152568], [0.11454890669653782, -0.0627692359492831, -0.11359459674839492], [0.11454711575694608, -0.06269374285074342, -0.11372400575300895], [0.1145315361956404, -0.0626422519620754, -0.11380801994443521]]\n",
      "0                             0                         [-0.7618405752393946, 2.0472636261020174, 6.098479530952007]   [6.521727017579424, -1.13311344924653, 8.03878130454521, 1.633957236993516, 5.1722551804192936, 1.9725571764099188, 2.732036936348763, 3.3075975824484942, 2.837005259517098, 3.400724364028486, 3.221745996301097, 3.7083227645891315, 3.3432414771262673, 3.0155772309930704, 2.534317481099305, 3.105172199891673, 2.3694590811009135, 2.1743436566992056, 2.665290039374535, 2.113722510995461, 2.2822697733486925, 2.219368510787872, 1.3349604511571596, 1.6440898311880034, 1.3121233049677417, 1.453964453397509, 1.4051181140381663, 1.0402576931639516, 0.7785925296092168, 0.6789342070316884]           [[0.3829533731757644, 0.25815732060171304, 0.4567965700990432], [0.3802547786186998, 0.3152579366752464, 0.4723974947561075], [0.2653772710275144, 0.304541481470157, 0.37154542276460634], [0.11877047566911571, 0.24288612834565965, 0.2097726629395046], [0.0010326551867217704, 0.12807118106130783, 0.06064016587552695], [-0.05961507668009036, 0.028228527461563454, -0.04955901023801987], [-0.06509953014811934, -0.033945322831514635, -0.12010576428241318], [-0.035318298934147466, -0.0629528305111137, -0.15990288655529533], [0.010147646720637171, -0.07215897036483895, -0.17595558813617576], [0.0548440084753406, -0.07003508341556805, -0.1769293848515773], [0.09059275882963123, -0.0652907064400175, -0.1697265431384127], [0.11441595476573711, -0.061675897853288725, -0.15939178209303875], [0.12796162273256217, -0.06031461495696813, -0.14847365609648752], [0.13335130452901128, -0.06042678643953017, -0.13891004990388858], [0.1334564291837782, -0.06141387805819551, -0.131131195744939], [0.13072692612239134, -0.06260733380709199, -0.12516943084489726], [0.12693321667274482, -0.06359424004946845, -0.12079842793019624], [0.1231712414976621, -0.06422622604335601, -0.1177398888365393], [0.1200156011286763, -0.0644905251003336, -0.11567893493101356], [0.11765924918510122, -0.0644720107717644, -0.11436155562719413], [0.11607446942972642, -0.06427763501232706, -0.1135854042140042], [0.11511629080171952, -0.06400083568007225, -0.11319100571960736], [0.114609163966041, -0.06370674121820497, -0.11305326771630123], [0.11439242411530277, -0.06343413803570055, -0.11307442145664287], [0.11434005664299478, -0.06320195627342154, -0.11318133268811145], [0.11436454310405629, -0.06301583949869344, -0.11332249761250687], [0.11441212789593543, -0.06287356414459117, -0.11346446859163592], [0.11445469892883418, -0.06276894124797816, -0.11358791720674663], [0.11448107181045357, -0.06269447527643673, -0.11368413510788972], [0.11448998841759604, -0.06264291477333511, -0.11375161240931908]]\n",
      "0                             0                         [0.561971881159741, 3.144492070559409, 5.559988352106144]      [4.190194300663683, 1.4710439689342678, 4.347724378274937, 1.861738909890693, 0.8429147407495471, -0.007912718377042854, 2.986952287500884, 2.9942066846844533, 2.9259537577803285, 3.2956412674144637, 3.3808188797491345, 3.36897968755574, 3.3339141580150278, 3.1402571775057986, 3.197480884429736, 3.0238814589302065, 2.632343817529189, 2.603985277555193, 2.146164780937704, 2.0720342998955474, 2.027203436708546, 1.9680754954880402, 2.052353745457873, 1.6596117277828235, 1.6047091122912733, 1.4427771439659787, 1.225641852560581, 1.113845788056635, 0.9966067415032669, 0.7585169723201464]       [[0.33261414458119937, 0.21896942990028034, 0.561473342223354], [0.3018065150458164, 0.2830216394029052, 0.6208930625082533], [0.19757000647544615, 0.29002159021488016, 0.5396420088915846], [0.07351365799229508, 0.2437578119024959, 0.36796631276133585], [-0.026363715612333402, 0.15066521828713944, 0.16465325677232442], [-0.06861987119190817, 0.052246946464006516, 0.011506562521672592], [-0.06519227853509761, -0.01630774763028914, -0.09405059040099742], [-0.027567933848942225, -0.049168223177326, -0.16046216235419267], [0.021162466139000344, -0.062045737802643136, -0.1925035074126921], [0.06531660749677705, -0.0632281001604259, -0.2006602156833275], [0.0988261328006164, -0.06115362655381604, -0.19470491423226718], [0.12032034523644376, -0.05959110458546074, -0.18199936869840025], [0.1314161982843685, -0.05964502377341918, -0.16705126011434707], [0.13490061045591645, -0.06073568392028128, -0.1530723509586949], [0.13371224410945937, -0.06222120414851279, -0.1411968523298007], [0.13028912310782564, -0.06363714195074457, -0.13182903333663318], [0.12625694406243093, -0.0646657890761779, -0.12485333659427134], [0.1225572818349173, -0.06521441631032526, -0.1199218653541175], [0.11960441583431776, -0.06534743452577238, -0.11663445339721976], [0.11746885324425622, -0.06519322172812395, -0.11459958916752089], [0.11607293317688804, -0.06486480583301896, -0.11344739116225916], [0.11525103785615583, -0.06446701865831894, -0.1128986117211177], [0.11482333391200701, -0.06407062616388112, -0.11273889529711315], [0.11463675435152938, -0.06371569618364517, -0.11280806002548263], [0.11457848451040951, -0.06341929731748192, -0.11299116957643773], [0.11457488361388972, -0.06318383078181775, -0.11321108729373697], [0.11458394356512634, -0.06300379691294408, -0.11342075437982138], [0.11458618249509868, -0.06287026809450791, -0.1135953934677208], [0.11457593241939772, -0.06277359953191552, -0.11372580274668079], [0.11455464035908296, -0.0627048326538576, -0.11381261776280252]]\n",
      "0                             0                         [4.296835155684376, 2.3230477007660206, 7.638307525527196]     [1.5018513090971766, -0.0003341042588377169, 1.1468325377055717, 2.4337433649057965, 5.17726614332079, 0.9060270793904831, 2.290363901022369, 2.6714794881889974, 3.3774165689532363, 2.8899046718379386, 2.917528735162025, 2.958298734244091, 3.0031848648942785, 2.4135073321687393, 3.032376433669132, 2.5987650981295265, 3.2865171676445346, 2.842949417208943, 2.2592300914027943, 2.424759960987704, 2.5488456557167414, 2.2457027166557064, 2.0463726171511234, 1.8088567708041736, 1.282714968138201, 1.382127919145569, 1.4040029172804078, 1.1766876433827096, 1.1227833500186646, 0.780372557594342]   [[0.37075072321636565, 0.19914324185730398, 0.47755553428234665], [0.37615240662434624, 0.24423111535593994, 0.5035144503364641], [0.27527571766710324, 0.256325685689864, 0.40825126300785886], [0.1331674101284051, 0.23114033854089616, 0.24234445510893135], [0.012804513479247215, 0.1462707774461434, 0.08098589347953217], [-0.052853229027031934, 0.054988797166639264, -0.034861310734712686], [-0.06681043280008764, -0.010694806205991617, -0.10991675399505133], [-0.04103934674099553, -0.046502448128839774, -0.15493312097644799], [0.0028413601558562734, -0.06261128830101828, -0.17472716966723387], [0.04848063398630531, -0.06617117292624275, -0.17855129964326485], [0.08569944675554732, -0.06390225924700535, -0.17316130403549462], [0.11138874166690112, -0.06131853479152888, -0.16369568369407145], [0.12657862172242845, -0.0602776694319449, -0.1528045077413116], [0.13307323425180617, -0.06036523939615951, -0.14282924781408352], [0.1338424739034082, -0.06128389609035462, -0.1344035691490438], [0.1314054097523602, -0.06246943709236574, -0.1277288007384748], [0.12766674109762394, -0.06351302192731917, -0.12269662859594663], [0.12383933564295833, -0.06422305729484698, -0.11906894569274482], [0.12055820462382881, -0.06457443939030963, -0.11657846827038246], [0.11806630308717202, -0.06461989362084253, -0.11494570690545919], [0.11636345458401713, -0.06445979185706324, -0.11394857404410505], [0.115313605340214, -0.0641911078800646, -0.11340480918415669], [0.11474073954496551, -0.06388717750694622, -0.11317115059121688], [0.11447973954793765, -0.06359476981237216, -0.1131344110421661], [0.11439905599152243, -0.06333883228848047, -0.11320910429478903], [0.11440609619373508, -0.06312893618770907, -0.1133344725465194], [0.1144430792785865, -0.06296501429128415, -0.11347048541371213], [0.11447910839017221, -0.0628418286501794, -0.11359334336473648], [0.11450120706750032, -0.06275199335429314, -0.1136913786360382], [0.11450701483172095, -0.06268796597068416, -0.11376126227680045]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 2019-02-26 15:57:25.814662: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf /tools/**/**/**/**/**/*.pyc\n",
    "model_dir=$(ls ${PWD}/trained_model/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=${PWD}/trained_model/export/exporter/${model_dir} \\\n",
    "    --json-instances=./sequences.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud ML-Engine prediction from deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataframe to instances list to get sent to ML-Engine\n",
    "instances = [{column: np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list for column in CSV_COLUMNS} for _ in range(0, number_of_prediction_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send instance dictionary to receive response from ML-Engine for online prediction\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "request_data = {\"instances\": instances}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'lstm_autoencoder_anomaly_detection', 'v1')\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../../../../tutorials/machine_learning_deepdive/06_structured/babyweight/* lstm_encoder_decoder_autoencoder_anomaly_detection_module/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "lstm_encoder_decoder_autoencoder_anomaly_detection.ipynb\n",
      "lstm_encoder_decoder_autoencoder_anomaly_detection_module\n",
      "tf_mahalanobis_test.ipynb\n",
      "trained_model\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKG-INFO  setup.cfg  setup.py  trainer\n"
     ]
    }
   ],
   "source": [
    "!ls lstm_encoder_decoder_autoencoder_anomaly_detection_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
