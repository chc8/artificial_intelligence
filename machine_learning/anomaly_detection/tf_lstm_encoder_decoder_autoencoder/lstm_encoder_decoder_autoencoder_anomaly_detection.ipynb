{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "1.16.2\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "np.set_printoptions(threshold = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# BUCKET = 'qwiklabs-gcp-8923d4964bfbd247-bucket' # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n",
    "# PROJECT = 'qwiklabs-gcp-8923d4964bfbd247' # REPLACE WITH YOUR PROJECT ID\n",
    "# REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "\n",
    "# # Import os environment variables\n",
    "# os.environ['PROJECT'] = PROJECT\n",
    "# os.environ['BUCKET'] =  BUCKET\n",
    "# os.environ['REGION'] = REGION\n",
    "# os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_sequence_before_anomaly = 80.0\n",
    "percent_sequence_after_anomaly = 0.0\n",
    "\n",
    "def create_time_series_normal_parameters():\n",
    "    normal_frequency_noise_scale = 0.1\n",
    "    normal_frequence_noise_shift = 0.0\n",
    "\n",
    "    normal_amplitude_noise_scale = 1.0\n",
    "    normal_amplitude_noise_shift = 1.0\n",
    "\n",
    "    normal_noise_noise_scale = 0.2\n",
    "\n",
    "    normal_freq = (np.random.random() * normal_frequency_noise_scale) + normal_frequence_noise_shift\n",
    "    normal_ampl = np.random.random() * normal_amplitude_noise_scale + normal_amplitude_noise_shift\n",
    "\n",
    "    return {\"normal_freq\": normal_freq, \"normal_ampl\": normal_ampl, \"normal_noise_noise_scale\": normal_noise_noise_scale}\n",
    "  \n",
    "\n",
    "def create_time_series_normal(number_of_sequences, sequence_length, normal_freq, normal_ampl, normal_noise_noise_scale):\n",
    "    # Normal parameters\n",
    "    normal_noise = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length)]\n",
    "\n",
    "    sequence = np.sin(np.arange(0, number_of_sequences * sequence_length) * normal_freq) * normal_ampl + normal_noise\n",
    "\n",
    "    sequence = sequence.reshape(number_of_sequences, sequence_length)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def create_time_series_with_anomaly(number_of_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, normal_freq, normal_ampl, normal_noise_noise_scale):\n",
    "    sequence_length_before_anomaly = int(sequence_length * percent_sequence_before_anomaly / 100.0)\n",
    "    sequence_length_after_anomaly = int(sequence_length * percent_sequence_after_anomaly / 100.0)\n",
    "    sequence_length_anomaly = sequence_length - sequence_length_before_anomaly - sequence_length_after_anomaly\n",
    "\n",
    "    # normal parameters\n",
    "    normal_noise_before = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_before_anomaly)]\n",
    "    normal_noise_after = [np.random.random() * normal_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_after_anomaly)]\n",
    "\n",
    "    # Anomalous parameters\n",
    "    anomalous_frequency_noise_scale = 2.0\n",
    "    anomalous_frequence_noise_shift = 1.0\n",
    "\n",
    "    anomalous_amplitude_noise_scale = 1.0\n",
    "    anomalous_amplitude_noise_shift = 1.0\n",
    "\n",
    "    anomalous_noise_noise_scale = 1.0\n",
    "\n",
    "    anomalous_freq = (np.random.random() * anomalous_frequency_noise_scale) + anomalous_frequence_noise_shift\n",
    "    anomalous_ampl = np.random.random() * anomalous_amplitude_noise_scale + anomalous_amplitude_noise_shift\n",
    "    anomalous_noise = [np.random.random() * anomalous_noise_noise_scale for i in range(0, number_of_sequences * sequence_length_anomaly)]\n",
    "\n",
    "    sequence_before_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_before_anomaly) * normal_freq) * normal_ampl + normal_noise_before\n",
    "    sequence_before_anomaly = sequence_before_anomaly.reshape(number_of_sequences, sequence_length_before_anomaly)\n",
    "\n",
    "    sequence_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_anomaly) * anomalous_freq) * anomalous_ampl + anomalous_noise\n",
    "    sequence_anomaly = sequence_anomaly.reshape(number_of_sequences, sequence_length_anomaly)\n",
    "\n",
    "    sequence_after_anomaly = np.sin(np.arange(0, number_of_sequences * sequence_length_after_anomaly) * normal_freq) * normal_ampl + normal_noise_after\n",
    "    sequence_after_anomaly = sequence_after_anomaly.reshape(number_of_sequences, sequence_length_after_anomaly)\n",
    "\n",
    "    return np.concatenate(seq = [sequence_before_anomaly, sequence_anomaly, sequence_after_anomaly], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/seaborn/timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_normal_parameters = create_time_series_normal_parameters()\n",
    "\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_normal(5, 50, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "for i in range(0, 1):\n",
    "    sns.tsplot(create_time_series_with_anomaly(5, 50, percent_sequence_before_anomaly, percent_sequence_after_anomaly, test_normal_parameters[\"normal_freq\"], test_normal_parameters[\"normal_ampl\"], test_normal_parameters[\"normal_noise_noise_scale\"]).reshape(-1), color=flatui[i%len(flatui)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_normal_sequences = 7000\n",
    "\n",
    "number_of_validation_normal_1_sequences = 500\n",
    "number_of_validation_normal_2_sequences = 500\n",
    "number_of_validation_anomalous_sequences = 500\n",
    "\n",
    "number_of_test_normal_sequences = 750\n",
    "number_of_test_anomalous_sequences = 750\n",
    "\n",
    "sequence_length = 30\n",
    "number_of_tags = 3\n",
    "tag_columns = [\"tag_{0}\".format(tag) for tag in range(0, number_of_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'normal_ampl': 1.9475249860491697,\n",
       "  'normal_freq': 0.01584192150217918,\n",
       "  'normal_noise_noise_scale': 0.2},\n",
       " {'normal_ampl': 1.0355632935721508,\n",
       "  'normal_freq': 0.055277404808225045,\n",
       "  'normal_noise_noise_scale': 0.2},\n",
       " {'normal_ampl': 1.4030341975507445,\n",
       "  'normal_freq': 0.013360749074043755,\n",
       "  'normal_noise_noise_scale': 0.2}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_data_list = [create_time_series_normal_parameters() for tag in range(0, number_of_tags)]\n",
    "tag_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_normal_sequences_array.shape = \n",
      "(7000, 3)\n"
     ]
    }
   ],
   "source": [
    "training_normal_sequences_list = [create_time_series_normal(number_of_training_normal_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "training_normal_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = training_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_training_normal_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "np.random.shuffle(training_normal_sequences_array)\n",
    "print(\"training_normal_sequences_array.shape = \\n{}\".format(training_normal_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_normal_1_sequences_array.shape = \n",
      "(500, 3)\n",
      "validation_normal_2_sequences_array.shape = \n",
      "(500, 3)\n",
      "validation_anomalous_sequences_array.shape = \n",
      "(500, 3)\n"
     ]
    }
   ],
   "source": [
    "validation_normal_1_sequences_list = [create_time_series_normal(number_of_validation_normal_1_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_normal_1_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_normal_1_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_normal_1_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_normal_1_sequences_array.shape = \\n{}\".format(validation_normal_1_sequences_array.shape))\n",
    "\n",
    "validation_normal_2_sequences_list = [create_time_series_normal(number_of_validation_normal_2_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_normal_2_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_normal_2_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_normal_2_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_normal_2_sequences_array.shape = \\n{}\".format(validation_normal_2_sequences_array.shape))\n",
    "\n",
    "validation_anomalous_sequences_list = [create_time_series_with_anomaly(number_of_validation_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "validation_anomalous_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = validation_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_validation_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"validation_anomalous_sequences_array.shape = \\n{}\".format(validation_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_normal_sequences_array.shape = \n",
      "(750, 3)\n",
      "test_anomalous_sequences_array.shape = \n",
      "(750, 3)\n"
     ]
    }
   ],
   "source": [
    "test_normal_sequences_list = [create_time_series_normal(number_of_test_normal_sequences, sequence_length, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "test_normal_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = test_normal_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_test_normal_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"test_normal_sequences_array.shape = \\n{}\".format(test_normal_sequences_array.shape))\n",
    "\n",
    "test_anomalous_sequences_list = [create_time_series_with_anomaly(number_of_test_anomalous_sequences, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]) for tag in tag_data_list]\n",
    "test_anomalous_sequences_array = np.stack(arrays = list(map(lambda i: np.stack(arrays = list(map(lambda j: np.array2string(a = test_anomalous_sequences_list[i][j], separator = ',').replace('[', '').replace(']', '').replace(' ', '').replace('\\n', ''), np.arange(0, number_of_test_anomalous_sequences))), axis = 0), np.arange(0, number_of_tags))), axis = 1)\n",
    "print(\"test_anomalous_sequences_array.shape = \\n{}\".format(test_anomalous_sequences_array.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname = \"data/training_normal_sequences.csv\", X = training_normal_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/validation_normal_1_sequences.csv\", X = validation_normal_1_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/validation_normal_2_sequences.csv\", X = validation_normal_2_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/validation_anomalous_sequences.csv\", X = validation_anomalous_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "\n",
    "np.savetxt(fname = \"data/test_normal_sequences.csv\", X = test_normal_sequences_array, fmt = '%s', delimiter = \";\")\n",
    "np.savetxt(fname = \"data/test_anomalous_sequences.csv\", X = test_anomalous_sequences_array, fmt = '%s', delimiter = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09334518,1.23135587,1.18454321,1.18715809,1.20276939,1.34890742,1.25736152,1.31988699,1.33050778,1.41471097,1.33256746,1.43471285,1.36178443,1.54538296,1.46275368,1.45133096,1.44116678,1.59227143,1.51762858,1.62345916,1.53345913,1.66938851,1.55962669,1.61453309,1.59751447,1.66556417,1.71613291,1.76898485,1.82829549,1.84983369;1.01133855,1.04715431,1.04804043,1.17378783,1.22823625,1.2063486,1.09238734,1.0391804,1.21777921,1.14605429,1.1297649,1.1106751,1.06036791,1.06220372,1.01831349,1.04571207,0.86775101,0.90857003,0.86376484,0.92305462,0.77253842,0.8077711,0.72496066,0.59454909,0.58689402,0.58343425,0.521869,0.56545446,0.37063121,0.37529461;-1.0447985,-1.01352476,-0.99991499,-0.91831813,-1.06602407,-0.89233088,-1.02049535,-0.92798234,-1.0280558,-1.01523003,-0.9780802,-0.97257614,-0.92143421,-0.86142614,-0.78488461,-0.79369883,-0.90457045,-0.76224036,-0.81599666,-0.76553029,-0.77088744,-0.67555414,-0.72554334,-0.68036812,-0.74823075,-0.72663257,-0.62772808,-0.7339175,-0.68518865,-0.70824783\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/training_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13651054,0.14729824,0.16959427,0.27496085,0.16924638,0.28917539,0.20906794,0.39318951,0.40609081,0.32669432,0.36090802,0.51730797,0.37851585,0.48069682,0.49591388,0.58894915,0.66939404,0.60619104,0.61923174,0.61364775,0.62918004,0.74272047,0.76457668,0.83378733,0.83370942,0.94142596,0.79327617,0.87378747,0.89598643,0.97075329;0.15875145,0.11342139,0.207435,0.35102017,0.3960671,0.38142225,0.33763026,0.43874307,0.64198651,0.63768112,0.59940227,0.73556775,0.65359185,0.87128149,0.82217122,0.93913123,0.97937015,1.01904909,1.05371946,0.90894473,0.95854259,1.14237466,1.0557496,1.08220677,1.09536456,1.04827613,1.12848511,1.06595731,1.21703382,1.14397017;0.03325058,0.20862645,0.18937148,0.11956649,0.18197099,0.25696082,0.214506,0.1470369,0.32243779,0.23358832,0.2982618,0.30500499,0.28191433,0.31867641,0.33320401,0.4357944,0.32649675,0.33118367,0.43351438,0.37160471,0.43650489,0.4346276,0.45279294,0.58590025,0.55055841,0.56054394,0.60190643,0.68076225,0.59773927,0.66030751\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_normal_1_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0131407,0.19118705,0.17269535,0.23220334,0.13797863,0.32321035,0.22479225,0.22031483,0.35869533,0.3432007,0.38439478,0.49664973,0.52643208,0.44534953,0.57484002,0.65778785,0.54730237,0.56543393,0.71845901,0.68154773,0.74046208,0.80191794,0.76659508,0.82048169,0.83162389,0.91238209,0.84516065,0.90434344,0.85866823,0.89231094;0.10485297,0.20184041,0.13076869,0.36447143,0.26103125,0.30843966,0.40190737,0.47631938,0.45504758,0.55892378,0.7239961,0.69968774,0.68038845,0.87817048,0.89260429,0.93594427,0.90157294,0.90470636,0.86918505,1.09223108,1.04151552,1.12520467,1.15655469,0.99898318,1.06696755,1.1060501,1.06396708,1.23003515,1.13091994,1.04811066;0.18104903,0.0806697,0.11646478,0.12885308,0.26618081,0.24753533,0.14984098,0.25982717,0.33618068,0.35639114,0.28116006,0.3599422,0.30319658,0.39943281,0.31854486,0.4080272,0.37700435,0.40885458,0.44092071,0.53230144,0.41847549,0.55580362,0.50763194,0.43647916,0.61161863,0.61300044,0.56639089,0.542903,0.56015224,0.58627042\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_normal_2_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1206576,0.1150755,0.14973248,0.22119321,0.31017765,0.35247631,0.25049819,0.37826132,0.31492857,0.45226709,0.47581427,0.34978724,0.39667108,0.53737245,0.46508813,0.61428226,0.67410573,0.52082185,0.68568826,0.59693679,0.79615816,0.70703643,0.73203437,0.88470394,0.70187499,1.85921853,-0.84502845,2.05054609,-0.68923731,0.13333427;0.05142222,0.18644054,0.12475574,0.26101984,0.29032398,0.36812085,0.34283431,0.48510113,0.49250726,0.59265866,0.57040546,0.78095698,0.72904615,0.84040115,0.75465522,0.93943279,0.90704077,1.03070451,1.02508635,0.9836995,0.98490287,0.96636595,1.09822746,1.18509633,0.21002422,2.20304179,-0.04906288,-0.93953046,0.77798639,1.92350719;0.12770125,0.06320607,0.08324674,0.06457963,0.27478996,0.18115352,0.29677864,0.18634884,0.34740464,0.26237683,0.21768602,0.29951191,0.25157399,0.25936118,0.44136792,0.47652915,0.44749912,0.33009347,0.47242029,0.48934555,0.4642589,0.49299497,0.45044119,0.60375751,0.83533865,1.20394357,-1.45910826,1.79373783,-0.19583716,0.39241597\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/validation_anomalous_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01509001,0.07279898,0.13229896,0.16506912,0.25501267,0.32947171,0.3553219,0.25407533,0.3434575,0.30243407,0.41955571,0.389043,0.40948832,0.4577192,0.500916,0.61357373,0.54752502,0.54667661,0.55361089,0.58867506,0.65761025,0.82883406,0.71279031,0.86385043,0.77051089,0.92022243,0.81734776,0.82288249,1.02758552,0.88732371;0.17482564,0.13276341,0.29246002,0.24709633,0.22955698,0.3221539,0.45572161,0.55498357,0.60870035,0.63040272,0.67669617,0.59574524,0.67085029,0.7179933,0.85266246,0.82581002,0.96422373,0.97671517,1.04669022,1.07652287,1.0052743,0.99147652,1.07014172,1.06002927,1.18207561,1.17653166,1.07521189,1.08632381,1.21338362,1.09049525;0.1874617,0.07612139,0.12928038,0.22814261,0.14610596,0.10746661,0.20684867,0.28264799,0.34003137,0.17265301,0.28914123,0.25966316,0.41829669,0.34753412,0.38583261,0.33315996,0.44585319,0.42211809,0.34573147,0.5514462,0.41779239,0.44778042,0.45589498,0.46674982,0.5667958,0.5886779,0.61679596,0.49924142,0.55285614,0.55341382\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/test_normal_sequences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0829368,0.14727274,0.06268084,0.20739894,0.30452796,0.16318792,0.2155324,0.27024369,0.26678897,0.37833029,0.42729364,0.4487033,0.50008429,0.56130947,0.47066102,0.62644076,0.58587853,0.712628,0.65017674,0.73105868,0.74132643,0.74201429,0.75141059,0.79807481,0.54578061,2.03098258,-0.73858209,1.25312905,0.86550568,-0.61832249;0.18638214,0.17741443,0.16375631,0.34262257,0.27577837,0.33109575,0.367629,0.52598851,0.48290948,0.61604352,0.65334421,0.7713239,0.76437619,0.82272608,0.78023196,0.86301658,0.99312602,0.9370514,1.0416653,0.9890225,1.02907469,0.98166502,1.16429863,1.14637519,0.12129894,0.90940193,-0.92975957,1.84556829,-0.77597363,1.60073258;0.02589579,0.06769502,0.16641206,0.1953343,0.14095126,0.16393035,0.26772832,0.14461671,0.29311857,0.2589938,0.31718226,0.25359096,0.35348749,0.38895212,0.44593139,0.31798384,0.38321602,0.33441159,0.37142653,0.47877284,0.50085356,0.45489786,0.46059634,0.52319161,0.46682822,1.99896989,-0.45005521,-0.40238378,1.46761813,0.10644536\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/test_anomalous_sequences.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging to be level of INFO\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "CSV_COLUMNS = tag_columns\n",
    "\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[\"\"], [\"\"], [\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size, params):\n",
    "    def _input_fn():\n",
    "#         print(\"\\nread_dataset: _input_fn: filename = \\n{}\".format(filename))\n",
    "#         print(\"read_dataset: _input_fn: mode = \\n{}\".format(mode))\n",
    "#         print(\"read_dataset: _input_fn: batch_size = \\n{}\".format(batch_size))\n",
    "#         print(\"read_dataset: _input_fn: params = \\n{}\\n\".format(params))\n",
    "\n",
    "        def decode_csv(value_column, sequence_length):\n",
    "            def convert_sequences_from_strings_to_floats(features):\n",
    "                def split_and_convert_string(string_tensor):\n",
    "                    # Split string tensor into a sparse tensor based on delimiter\n",
    "                    split_string = tf.string_split(source = tf.expand_dims(input = string_tensor, axis = 0), delimiter = \",\")\n",
    "#                     print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: split_string = \\n{}\".format(split_string))\n",
    "\n",
    "                    # Converts the values of the sparse tensor to floats\n",
    "                    converted_tensor = tf.string_to_number(split_string.values, out_type = tf.float64)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: converted_tensor = \\n{}\".format(converted_tensor))\n",
    "\n",
    "                    # Create a new sparse tensor with the new converted values, because the original sparse tensor values are immutable\n",
    "                    new_sparse_tensor = tf.SparseTensor(indices = split_string.indices, values = converted_tensor, dense_shape = split_string.dense_shape)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: new_sparse_tensor = \\n{}\".format(new_sparse_tensor))\n",
    "\n",
    "                    # Create a dense tensor of the float values that were converted from text csv\n",
    "                    dense_floats = tf.sparse_tensor_to_dense(sp_input = new_sparse_tensor, default_value = 0.0)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats = \\n{}\".format(dense_floats))\n",
    "\n",
    "                    dense_floats_vector = tf.squeeze(input = dense_floats, axis = 0)\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: split_and_convert_string: dense_floats_vector = \\n{}\\n\".format(dense_floats_vector))\n",
    "\n",
    "                    return dense_floats_vector\n",
    "                    \n",
    "#                 print(\"\\nread_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "                for column in CSV_COLUMNS:\n",
    "#                     print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: column = \\n{}\".format(column))\n",
    "                    features[column] = split_and_convert_string(features[column])\n",
    "                    features[column].set_shape([sequence_length])\n",
    "\n",
    "#                 print(\"read_dataset: _input_fn: decode_csv: convert_sequences_from_strings_to_floats: features = \\n{}\".format(features))\n",
    "\n",
    "                return features\n",
    "                \n",
    "#             print(\"\\nread_dataset: _input_fn: decode_csv: value_column = \\n{}\".format(value_column))\n",
    "            columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS, field_delim = \";\")\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: columns = \\n{}\".format(columns))\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            features = convert_sequences_from_strings_to_floats(features)\n",
    "#             print(\"read_dataset: _input_fn: decode_csv: features = \\n{}\".format(features))\n",
    "            return features\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename = filename)\n",
    "#         print(\"\\nread_dataset: _input_fn: file_list = \\n{}\".format(file_list))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(filenames = file_list)    # Read text file\n",
    "#         print(\"read_dataset: _input_fn: dataset.TextLineDataset(file_list) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Decode the CSV file into a features dictionary of tensors\n",
    "        dataset = dataset.map(map_func = lambda x: decode_csv(x, params[\"sequence_length\"]))\n",
    "#         print(\"read_dataset: _input_fn: dataset.map(decode_csv) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine amount of times to repeat file based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        # Repeat files num_epoch times\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "#         print(\"read_dataset: _input_fn: dataset.repeat(num_epochs) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Group the data into batches\n",
    "        dataset = dataset.batch(batch_size = batch_size)\n",
    "#         print(\"read_dataset: _input_fn: dataset.batch(batch_size) = \\n{}\".format(dataset))\n",
    "        \n",
    "        # Determine if we should shuffle based on if we are training or evaluating\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "#             print(\"read_dataset: _input_fn: dataset.shuffle(buffer_size = 10 * batch_size) = \\n{}\".format(dataset))\n",
    "\n",
    "        # Create a iterator and then pull the next batch of features from the example queue\n",
    "        batch_features = dataset.make_one_shot_iterator().get_next()\n",
    "#         print(\"read_dataset: _input_fn: batch_features = \\n{}\".format(batch_features))\n",
    "\n",
    "        return batch_features\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_out_input_function(args):\n",
    "    with tf.Session() as sess:\n",
    "        fn = read_dataset(\n",
    "          filename = args[\"train_file_pattern\"],\n",
    "          mode = tf.estimator.ModeKeys.EVAL,\n",
    "          batch_size = args[\"batch_size\"],\n",
    "          params = args)\n",
    "\n",
    "        features = sess.run(fn())\n",
    "        print(\"try_out_input_function: features = {}\".format(features))\n",
    "\n",
    "        print(\"try_out_input_function: features[tag_0].shape = {}\".format(features[\"tag_0\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_out_input_function(args = arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model function to be used in our custom estimator\n",
    "def lstm_encoder_decoder_autoencoder_anomaly_detection(features, labels, mode, params):\n",
    "#     print(\"\\nlstm_encoder_decoder_autoencoder_anomaly_detection: features = \\n{}\".format(features))\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: labels = \\n{}\".format(labels))\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mode = \\n{}\".format(mode))\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: params = \\n{}\".format(params))\n",
    "\n",
    "    # 0. Get input sequence tensor into correct shape\n",
    "    # Get dynamic batch size in case there was a partially filled batch\n",
    "    current_batch_size = tf.shape(input = features[CSV_COLUMNS[0]], out_type = tf.int64)[0]\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: current_batch_size = \\n{}\".format(current_batch_size))\n",
    "\n",
    "    # Get the number of features \n",
    "    number_of_features = len(CSV_COLUMNS)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: number_of_features = \\n{}\".format(number_of_features))\n",
    "\n",
    "    # Stack all of the features into a 3-D tensor\n",
    "    X = tf.stack(values = list(features.values()), axis = 2) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: X = \\n{}\".format(X))\n",
    "\n",
    "    # Unstack all of 3-D features tensor into a sequence(list) of 2-D tensors of shape = (current_batch_size, number_of_features)\n",
    "    X_sequence = tf.unstack(value = X, num = params[\"sequence_length\"], axis = 1)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: X_sequence = \\n{}\".format(X_sequence))\n",
    "\n",
    "    # Since this is an autoencoder, the features are the labels. It works better though to have the labels in reverse order\n",
    "    if params[\"reverse_labels_sequence\"] == True:\n",
    "        Y = tf.reverse_sequence(input = X,  # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "                                seq_lengths = tf.tile(input = tf.constant(value = [params[\"sequence_length\"]], dtype = tf.int64), \n",
    "                                                      multiples = tf.expand_dims(input = current_batch_size, axis = 0)), \n",
    "                                seq_axis = 1, \n",
    "                                batch_axis = 0)\n",
    "    else:\n",
    "        Y = X  # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: Y = \\n{}\".format(Y))\n",
    "  \n",
    "  ################################################################################\n",
    "  \n",
    "    # 1. Create encoder of encoder-decoder LSTM stacks\n",
    "    def create_LSTM_stack(lstm_hidden_units, lstm_dropout_output_keep_probs):\n",
    "        # First create a list of LSTM cells using our list of lstm hidden unit sizes\n",
    "        lstm_cells = [tf.contrib.rnn.BasicLSTMCell(num_units = units, forget_bias = 1.0, state_is_tuple = True) for units in lstm_hidden_units] # list of LSTM cells\n",
    "#         print(\"\\nlstm_encoder_decoder_autoencoder_anomaly_detection: create_LSTM_stack: lstm_cells = \\n{}\".format(lstm_cells))\n",
    "\n",
    "        # Next apply a dropout wrapper to our stack of LSTM cells, in this case just on the outputs\n",
    "        dropout_lstm_cells = [tf.nn.rnn_cell.DropoutWrapper(cell = lstm_cells[cell_index], \n",
    "                                                            input_keep_prob = 1.0, \n",
    "                                                            output_keep_prob = lstm_dropout_output_keep_probs[cell_index], \n",
    "                                                            state_keep_prob = 1.0) for cell_index in range(len(lstm_cells))]\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: create_LSTM_stack: dropout_lstm_cells = \\n{}\".format(dropout_lstm_cells))\n",
    "\n",
    "        # Create a stack of layers of LSTM cells\n",
    "        stacked_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells = dropout_lstm_cells, state_is_tuple = True) # combines list into MultiRNNCell object\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: create_LSTM_stack: stacked_lstm_cells = \\n{}\\n\".format(stacked_lstm_cells))\n",
    "\n",
    "        return stacked_lstm_cells\n",
    "  \n",
    "    # Create our decoder now\n",
    "    decoder_stacked_lstm_cells = create_LSTM_stack(params[\"decoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_stacked_lstm_cells = \\n{}\".format(decoder_stacked_lstm_cells))\n",
    "  \n",
    "    # Create the encoder variable scope\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        # Create separate encoder cells with their own weights separate from decoder\n",
    "        encoder_stacked_lstm_cells = create_LSTM_stack(params[\"encoder_lstm_hidden_units\"], params[\"lstm_dropout_output_keep_probs\"])\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_stacked_lstm_cells = {}\".format(encoder_stacked_lstm_cells))\n",
    "\n",
    "        # Encode the input sequence using our encoder stack of LSTMs\n",
    "        encoder_outputs, encoder_states = tf.nn.static_rnn(cell = encoder_stacked_lstm_cells, \n",
    "                                                           inputs = X_sequence, \n",
    "                                                           initial_state = encoder_stacked_lstm_cells.zero_state(batch_size = tf.cast(x = current_batch_size, dtype = tf.int32), dtype = tf.float64), \n",
    "                                                           dtype = tf.float64)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_outputs = \\n{}\".format(encoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_states = \\n{}\".format(encoder_states)) # tuple of final encoder c_state and h_state for each layer\n",
    "\n",
    "        # We just pass on the final c and h states of the encoder\"s last layer, so extract that and drop the others\n",
    "        encoder_final_states = encoder_states[-1]\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_final_states = \\n{}\".format(encoder_final_states))\n",
    "\n",
    "        # Extract the c and h states from the tuple\n",
    "        encoder_final_c, encoder_final_h = encoder_final_states\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_final_c = \\n{}\".format(encoder_final_c))\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: encoder_final_h = \\n{}\".format(encoder_final_h))\n",
    "\n",
    "        # In case the decoder\"s first layer\"s number of units is different than encoder\"s last layer\"s number of units, use a dense layer to map to the correct shape\n",
    "        encoder_final_c_dense = tf.layers.dense(inputs = encoder_final_c, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "        encoder_final_h_dense = tf.layers.dense(inputs = encoder_final_h, units = params[\"decoder_lstm_hidden_units\"][0], activation = None)\n",
    "\n",
    "        # The decoder\"s first layer\"s state comes from the encoder, the rest of the layers\" initial states are zero\n",
    "        decoder_intial_states = tuple([tf.contrib.rnn.LSTMStateTuple(c = encoder_final_c_dense, h = encoder_final_h_dense)] + [tf.contrib.rnn.LSTMStateTuple(c = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64), h = tf.zeros(shape = [current_batch_size, units], dtype = tf.float64)) for units in params[\"decoder_lstm_hidden_units\"][1:]])\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_intial_states = \\n{}\".format(decoder_intial_states))\n",
    "    \n",
    "    ################################################################################\n",
    "\n",
    "    # 2. Create decoder of encoder-decoder LSTM stacks\n",
    "    # The rnn_decoder function takes labels during TRAIN/EVAL and a start token followed by its previous predictions during PREDICT\n",
    "    # Starts with an intial state of the final encoder states\n",
    "    def rnn_decoder(decoder_inputs, initial_state, cell, inference):\n",
    "        # Create the decoder variable scope\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            # Load in our initial state from our encoder\n",
    "            state = initial_state # tuple of final encoder c_state and h_state of final encoder layer\n",
    "#             print(\"\\nlstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: state = \\n{}\".format(state))\n",
    "            \n",
    "            # Create an empty list to store our hidden state output for every timestep\n",
    "            outputs = []\n",
    "            \n",
    "            # Begin with no previous output\n",
    "            previous_output = None\n",
    "            \n",
    "            # Loop over all of our decoder_inputs which will be sequence_length long\n",
    "            for index, decoder_input in enumerate(decoder_inputs):\n",
    "                # If there has been a previous output then we will determine the next input\n",
    "                if previous_output is not None:\n",
    "                    # Create the input layer to our DNN\n",
    "                    network = previous_output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: network = \\n{}\".format(network))\n",
    "                    \n",
    "                    # Create our dnn variable scope\n",
    "                    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "                        # Add hidden layers with the given number of units/neurons per layer\n",
    "                        for units in params[\"dnn_hidden_units\"]:\n",
    "                            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size, dnn_hidden_units[i])\n",
    "#                             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: network = {}, units = {}\".format(network, units))\n",
    "                            \n",
    "                        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "                        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size, number_of_features)\n",
    "#                         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: logits = \\n{}\\n\".format(logits))\n",
    "                    \n",
    "                    # If we are in inference then we will overwrite our next decoder_input with the logits we just calculated.\n",
    "                    # Otherwise, we leave the decoder_input input as it was from the enumerated list\n",
    "                    # We have to calculate the logits even when not using them so that the correct dnn subgraph will be generated here and after the encoder-decoder for both training and inference\n",
    "                    if inference == True:\n",
    "                        decoder_input = logits # shape = (current_batch_size, number_of_features)\n",
    "\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: decoder_input = \\n{}\\n\".format(decoder_input))\n",
    "                \n",
    "                # If this isn\"t our first time through the loop, just reuse(share) the same variables for each iteration within the current variable scope\n",
    "                if index > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                # Run the decoder input through the decoder stack picking up from the previous state\n",
    "                output, state = cell(decoder_input, state)\n",
    "#                 print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: output = \\n{}\".format(output)) # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#                 print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: rnn_decoder: state = \\n{}\".format(state)) # tuple of final decoder c_state and h_state\n",
    "                \n",
    "                # Append the current decoder hidden state output to the outputs list\n",
    "                outputs.append(output) # growing list eventually sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "                \n",
    "                # Set the previous output to the output just calculated\n",
    "                previous_output = output # shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "        return outputs, state\n",
    "  \n",
    "    # Train our decoder now\n",
    "  \n",
    "    # Encoder-decoders work differently during training/evaluation and inference so we will have two separate subgraphs for each\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN    or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # Break 3-D labels tensor into a list of 2-D tensors\n",
    "        unstacked_labels = tf.unstack(value = Y, num = params[\"sequence_length\"], axis = 1) # list of sequence_length long of shape = (current_batch_size, number_of_features)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: unstacked_labels = \\n{}\".format(unstacked_labels))\n",
    "\n",
    "        # Call our decoder using the labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to false\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = unstacked_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = False)\n",
    "    else:\n",
    "        # Since this is inference create fake labels. The list length needs to be the output sequence length even though only the first element is actually used (as our go signal)\n",
    "        fake_labels = [tf.zeros(shape = [current_batch_size, number_of_features], dtype = tf.float64) for _ in range(params[\"sequence_length\"])]\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: fake_labels = \\n{}\".format(fake_labels))\n",
    "        \n",
    "        # Call our decoder using fake labels as our inputs, the encoder final state as our initial state, our other LSTM stack as our cells, and inference set to true\n",
    "        decoder_outputs, decoder_states = rnn_decoder(decoder_inputs = fake_labels, initial_state = decoder_intial_states, cell = decoder_stacked_lstm_cells, inference = True)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_outputs = \\n{}\".format(decoder_outputs)) # list sequence_length long of shape = (current_batch_size, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: decoder_states = \\n{}\".format(decoder_states)) # tuple of final decoder c_state and h_state\n",
    "    \n",
    "    # Stack together the list of rank 2 decoder output tensors into one rank 3 tensor\n",
    "    stacked_decoder_outputs = tf.stack(values = decoder_outputs, axis = 1) # shape = (current_batch_size, sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: stacked_decoder_outputs = \\n{}\".format(stacked_decoder_outputs))\n",
    "    \n",
    "    # Reshape rank 3 decoder outputs into rank 2 by folding sequence length into batch size\n",
    "    reshaped_stacked_decoder_outputs = tf.reshape(tensor = stacked_decoder_outputs, shape = [current_batch_size * params[\"sequence_length\"], params[\"decoder_lstm_hidden_units\"][-1]]) # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: reshaped_stacked_decoder_outputs = \\n{}\".format(reshaped_stacked_decoder_outputs))\n",
    "\n",
    "    ################################################################################\n",
    "    \n",
    "    # 3. Create the DNN structure now after the encoder-decoder LSTM stack\n",
    "    # Create the input layer to our DNN\n",
    "    network = reshaped_stacked_decoder_outputs # shape = (current_batch_size * sequence_length, lstm_hidden_units[-1])\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: network = \\n{}\".format(network))\n",
    "    \n",
    "    # Reuse the same variable scope as we used within our decoder (for inference)\n",
    "    with tf.variable_scope(name_or_scope = \"dnn\", reuse = tf.AUTO_REUSE):\n",
    "        # Add hidden layers with the given number of units/neurons per layer\n",
    "        for units in params[\"dnn_hidden_units\"]:\n",
    "            network = tf.layers.dense(inputs = network, units = units, activation = tf.nn.relu) # shape = (current_batch_size * sequence_length, dnn_hidden_units[i])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: network = {}, units = {}\".format(network, units))\n",
    "\n",
    "        # Connect the final hidden layer to a dense layer with no activation to get the logits\n",
    "        logits = tf.layers.dense(inputs = network, units = number_of_features, activation = None) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: logits = \\n{}\".format(logits))\n",
    "    \n",
    "    # Now that we are through the final DNN for each sequence element for each example in the batch, reshape the predictions to match our labels\n",
    "    predictions = tf.reshape(tensor = logits, shape = [current_batch_size, params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: predictions = \\n{}\".format(predictions))\n",
    "    \n",
    "    with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "        # Time based\n",
    "        absolute_error_count_batch_time_variable = tf.get_variable(name = \"absolute_error_count_batch_time_variable\", # shape = ()\n",
    "                                                                   dtype = tf.int64,\n",
    "                                                                   initializer = tf.zeros(shape = [], \n",
    "                                                                                          dtype = tf.int64),\n",
    "                                                                   trainable = False)\n",
    "        \n",
    "        absolute_error_mean_batch_time_variable = tf.get_variable(name = \"absolute_error_mean_batch_time_variable\", # shape = (number_of_features,)\n",
    "                                                                  dtype = tf.float64,\n",
    "                                                                  initializer = tf.zeros(shape = [number_of_features], \n",
    "                                                                                         dtype = tf.float64),\n",
    "                                                                  trainable = False)\n",
    "        \n",
    "        absolute_error_covariance_matrix_batch_time_variable = tf.get_variable(name = \"absolute_error_covariance_matrix_batch_time_variable\", # shape = (number_of_features, number_of_features)\n",
    "                                                                               dtype = tf.float64,\n",
    "                                                                               initializer = tf.zeros(shape = [number_of_features, number_of_features], \n",
    "                                                                                                      dtype = tf.float64),\n",
    "                                                                               trainable = False)\n",
    "\n",
    "        absolute_error_inverse_covariance_matrix_batch_time_variable = tf.get_variable(name = \"absolute_error_inverse_covariance_matrix_batch_time_variable\", # shape = (number_of_features, number_of_features)\n",
    "                                                                                       dtype = tf.float64,\n",
    "                                                                                       initializer = tf.zeros(shape = [number_of_features, number_of_features], \n",
    "                                                                                                              dtype = tf.float64),\n",
    "                                                                                       trainable = False)\n",
    "\n",
    "        # Features based\n",
    "        absolute_error_count_batch_features_variable = tf.get_variable(name = \"absolute_error_count_batch_features_variable\", # shape = ()\n",
    "                                                                       dtype = tf.int64,\n",
    "                                                                       initializer = tf.zeros(shape = [], \n",
    "                                                                                              dtype = tf.int64),\n",
    "                                                                       trainable = False)\n",
    "        \n",
    "        absolute_error_mean_batch_features_variable = tf.get_variable(name = \"absolute_error_mean_batch_features_variable\", # shape = (sequence_length,)\n",
    "                                                                      dtype = tf.float64,\n",
    "                                                                      initializer = tf.zeros(shape = [params[\"sequence_length\"]], \n",
    "                                                                                             dtype = tf.float64),\n",
    "                                                                      trainable = False)\n",
    "        \n",
    "        absolute_error_covariance_matrix_batch_features_variable = tf.get_variable(name = \"absolute_error_covariance_matrix_batch_features_variable\", # shape = (sequence_length, sequence_length)\n",
    "                                                                                   dtype = tf.float64,\n",
    "                                                                                   initializer = tf.zeros(shape = [params[\"sequence_length\"], params[\"sequence_length\"]], \n",
    "                                                                                                          dtype = tf.float64),\n",
    "                                                                                   trainable = False)\n",
    "\n",
    "        absolute_error_inverse_covariance_matrix_batch_features_variable = tf.get_variable(name = \"absolute_error_inverse_covariance_matrix_batch_features_variable\", # shape = (sequence_length, sequence_length)\n",
    "                                                                                           dtype = tf.float64,\n",
    "                                                                                           initializer = tf.zeros(shape = [params[\"sequence_length\"], params[\"sequence_length\"]], \n",
    "                                                                                                                  dtype = tf.float64),\n",
    "                                                                                           trainable = False)\n",
    "    \n",
    "    # Now branch off based on which mode we are in\n",
    "    predictions_dict = None\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    export_outputs = None\n",
    "    \n",
    "    # 3. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        \n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step(),\n",
    "            learning_rate = params[\"learning_rate\"],\n",
    "            optimizer = \"Adam\")\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.losses.mean_squared_error(labels = Y, predictions = predictions)\n",
    "        eval_metric_ops = {\n",
    "            \"rmse\": tf.metrics.root_mean_squared_error(labels = Y, predictions = predictions),\n",
    "            \"mae\": tf.metrics.mean_absolute_error(labels = Y, predictions = predictions)\n",
    "        }\n",
    "\n",
    "        if params[\"calculate_error_distribution_statistics\"] == True:\n",
    "            absolute_error = tf.abs(x = Y - predictions) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error = \\n{}\".format(absolute_error))\n",
    "\n",
    "            ################################################################################\n",
    "    \n",
    "            with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "                # This function updates the count of records used\n",
    "                def update_count(count_a, count_b):\n",
    "                    return count_a + count_b\n",
    "                \n",
    "                # This function updates the mahalanobis distance variables when the current_batch_size equals 1\n",
    "                def singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable):\n",
    "                    # This function updates the mean vector incrementally\n",
    "                    def update_mean_incremental(count_a, mean_a, value_b):\n",
    "                        return (mean_a * tf.cast(x = count_a, dtype = tf.float64) + tf.squeeze(input = value_b, axis = 0)) / tf.cast(x = count_a + 1, dtype = tf.float64)\n",
    "\n",
    "                    # This function updates the covariance matrix incrementally\n",
    "                    def update_covariance_incremental(count_a, mean_a, cov_a, value_b, mean_ab, sample_covariance):\n",
    "                        if sample_covariance == True:\n",
    "                            cov_ab = (cov_a * tf.cast(x = count_a - 1, dtype = tf.float64) + tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, transpose_a = True)) / tf.cast(x = count_a, dtype = tf.float64)\n",
    "                        else:\n",
    "                            cov_ab = (cov_a * tf.cast(x = count_a, dtype = tf.float64) + tf.matmul(a = value_b - mean_a, b = value_b - mean_ab, transpose_a = True)) / tf.cast(x = count_a + 1, dtype = tf.float64)\n",
    "                        return cov_ab\n",
    "                    \n",
    "                    # Time based\n",
    "                    absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error, shape = [current_batch_size * params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "                    \n",
    "                    # Calculate new combined mean to use for incremental covariance matrix calculation\n",
    "                    mean_ab = update_mean_incremental(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                      mean_a = absolute_error_mean_batch_time_variable, \n",
    "                                                      value_b = absolute_error_reshaped_batch_time) # shape = (number_of_features,)\n",
    "\n",
    "                    # Update running variables from single example\n",
    "                    absolute_error_covariance_matrix_batch_time_variable.assign(value = update_covariance_incremental(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                                                      mean_a = absolute_error_mean_batch_time_variable, \n",
    "                                                                                                                      cov_a = absolute_error_covariance_matrix_batch_time_variable, \n",
    "                                                                                                                      value_b = absolute_error_reshaped_batch_time, \n",
    "                                                                                                                      mean_ab = mean_ab, \n",
    "                                                                                                                      sample_covariance = True)) # shape = (number_of_features, number_of_features)\n",
    "                    absolute_error_mean_batch_time_variable.assign(value = mean_ab) # shape = (number_of_features,)\n",
    "                    absolute_error_count_batch_time_variable.assign(value = update_count(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                     count_b = 1)) # shape = ()\n",
    "                    \n",
    "                    absolute_error_inverse_covariance_matrix_batch_time_variable.assign(value = tf.matrix_inverse(input = absolute_error_covariance_matrix_batch_time_variable)) # shape = (number_of_features, number_of_features)\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    \n",
    "                    # Features based\n",
    "                    absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                                       elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "                                                                       dtype = tf.float64)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "                    absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, shape = [current_batch_size * number_of_features, params[\"sequence_length\"]]) # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "                    \n",
    "                    # Calculate new combined mean to use for incremental covariance matrix calculation\n",
    "                    mean_ab = update_mean_incremental(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                      mean_a = absolute_error_mean_batch_features_variable, \n",
    "                                                      value_b = absolute_error_reshaped_batch_features) # shape = (sequence_length,)\n",
    "\n",
    "                    # Update running variables from single example\n",
    "                    absolute_error_covariance_matrix_batch_features_variable.assign(value = update_covariance_incremental(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                                                          mean_a = absolute_error_mean_batch_features_variable, \n",
    "                                                                                                                          cov_a = absolute_error_covariance_matrix_batch_features_variable, \n",
    "                                                                                                                          value_b = absolute_error_reshaped_batch_features, \n",
    "                                                                                                                          mean_ab = mean_ab, \n",
    "                                                                                                                          sample_covariance = True)) # shape = (sequence_length, sequence_length)\n",
    "                    absolute_error_mean_batch_features_variable.assign(value = mean_ab) # shape = (sequence_length,)\n",
    "                    absolute_error_count_batch_features_variable.assign(value = update_count(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                         count_b = 1)) # shape = ()\n",
    "                    \n",
    "                    absolute_error_inverse_covariance_matrix_batch_features_variable.assign(value = tf.matrix_inverse(input = absolute_error_covariance_matrix_batch_features_variable)) # shape = (sequence_length, sequence_length)\n",
    "                    \n",
    "                    return absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable\n",
    "\n",
    "                # This function updates the mahalanobis distance variables when the current_batch_size does NOT equal 1\n",
    "                def non_singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable):\n",
    "                    # This function updates the mean vector using a batch of data\n",
    "                    def update_mean_batch(count_a, mean_a, count_b, mean_b):\n",
    "                        return (mean_a * tf.cast(x = count_a, dtype = tf.float64) + mean_b * tf.cast(x = count_b, dtype = tf.float64)) / tf.cast(x = count_a + count_b, dtype = tf.float64)\n",
    "\n",
    "                    # This function updates the covariance matrix using a batch of data\n",
    "                    def update_covariance_batch(count_a, mean_a, cov_a, count_b, mean_b, cov_b, sample_covariance):\n",
    "#                         print(\"update_covariance_batch: count_a = \\n{}\".format(count_a))\n",
    "#                         print(\"update_covariance_batch: mean_a = \\n{}\".format(mean_a))\n",
    "#                         print(\"update_covariance_batch: cov_a = \\n{}\".format(cov_a))\n",
    "#                         print(\"update_covariance_batch: count_b = \\n{}\".format(count_b))\n",
    "#                         print(\"update_covariance_batch: mean_b = \\n{}\".format(mean_b))\n",
    "#                         print(\"update_covariance_batch: cov_b = \\n{}\".format(cov_b))\n",
    "                        \n",
    "                        mean_diff = tf.expand_dims(input = mean_a - mean_b, axis = 0)\n",
    "\n",
    "                        if sample_covariance == True:\n",
    "                            cov_ab = (cov_a * tf.cast(x = count_a - 1, dtype = tf.float64) + cov_b * tf.cast(x = count_b - 1, dtype = tf.float64) + tf.matmul(a = mean_diff, b = mean_diff, transpose_a = True) * tf.cast(x = count_a * count_b, dtype = tf.float64) / tf.cast(x = count_a + count_b, dtype = tf.float64)) / tf.cast(x = count_a + count_b - 1, dtype = tf.float64)\n",
    "                        else:\n",
    "                            cov_ab = (cov_a * tf.cast(x = count_a, dtype = tf.float64) + cov_b * tf.cast(x = count_b, dtype = tf.float64) + tf.matmul(a = mean_diff, b = mean_diff, transpose_a = True) * tf.cast(x = count_a * count_b, dtype = tf.float64) / tf.cast(x = count_a + count_b, dtype = tf.float64)) / tf.cast(x = count_a + count_b, dtype = tf.float64)\n",
    "                        return cov_ab\n",
    "                    \n",
    "                    # Time based\n",
    "                    \n",
    "                    # Find statistics of batch\n",
    "                    absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error, shape = [current_batch_size * params[\"sequence_length\"], number_of_features]) # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "\n",
    "                    absolute_error_mean_batch_time = tf.reduce_mean(input_tensor = absolute_error_reshaped_batch_time, axis = 0) # shape = (current_batch_size * sequence_length,)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mean_batch_time = \\n{}\".format(absolute_error_mean_batch_time))\n",
    "\n",
    "                    absolute_error_reshaped_batch_time_centered = absolute_error_reshaped_batch_time - absolute_error_mean_batch_time # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time_centered = \\n{}\".format(absolute_error_reshaped_batch_time_centered))\n",
    "\n",
    "                    absolute_error_reshaped_batch_time_covariance_matrix = tf.matmul(a = absolute_error_reshaped_batch_time_centered, # shape = (number_of_features, number_of_features)\n",
    "                                                                                     b = absolute_error_reshaped_batch_time_centered, \n",
    "                                                                                     transpose_a = True) / tf.cast(x = current_batch_size * params[\"sequence_length\"] - 1, dtype = tf.float64)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_time_covariance_matrix))\n",
    "                    \n",
    "                    # Update running variables from batch statistics\n",
    "                    absolute_error_covariance_matrix_batch_time_variable.assign(value = update_covariance_batch(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                                                mean_a = absolute_error_mean_batch_time_variable, \n",
    "                                                                                                                cov_a = absolute_error_covariance_matrix_batch_time_variable, \n",
    "                                                                                                                count_b = current_batch_size, \n",
    "                                                                                                                mean_b = absolute_error_mean_batch_time, \n",
    "                                                                                                                cov_b = absolute_error_reshaped_batch_time_covariance_matrix, \n",
    "                                                                                                                sample_covariance = True)) # shape = (number_of_features, number_of_features)\n",
    "                    \n",
    "                    absolute_error_mean_batch_time_variable.assign(value = update_mean_batch(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                             mean_a = absolute_error_mean_batch_time_variable, \n",
    "                                                                                             count_b = current_batch_size, \n",
    "                                                                                             mean_b = absolute_error_mean_batch_time)) # shape = (number_of_features,)\n",
    "                    \n",
    "                    absolute_error_count_batch_time_variable.assign(value = update_count(count_a = absolute_error_count_batch_time_variable, \n",
    "                                                                                         count_b = current_batch_size)) # shape = ()\n",
    "\n",
    "                    absolute_error_inverse_covariance_matrix_batch_time_variable.assign(value = tf.matrix_inverse(input = absolute_error_covariance_matrix_batch_time_variable)) # shape = (number_of_features, number_of_features)\n",
    "\n",
    "                    ################################################################################\n",
    "\n",
    "                    # Features based\n",
    "                    \n",
    "                    # Find statistics of batch\n",
    "                    absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                                       elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "                                                                       dtype = tf.float64)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "                    absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, shape = [current_batch_size * number_of_features, params[\"sequence_length\"]]) # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "\n",
    "                    absolute_error_mean_batch_features = tf.reduce_mean(input_tensor = absolute_error_reshaped_batch_features, axis = 0) # shape = (sequence_length,)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mean_batch_features = \\n{}\".format(absolute_error_mean_batch_features))\n",
    "\n",
    "                    absolute_error_reshaped_batch_features_centered = absolute_error_reshaped_batch_features - absolute_error_mean_batch_features # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features_centered = \\n{}\".format(absolute_error_reshaped_batch_features_centered))\n",
    "\n",
    "                    absolute_error_reshaped_batch_features_covariance_matrix = tf.matmul(a = absolute_error_reshaped_batch_features_centered, # shape = (sequence_length, sequence_length)\n",
    "                                                                                         b = absolute_error_reshaped_batch_features_centered, \n",
    "                                                                                         transpose_a = True) / tf.cast(x = current_batch_size * number_of_features - 1, dtype = tf.float64)\n",
    "#                     print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features_covariance_matrix = \\n{}\".format(absolute_error_reshaped_batch_features_covariance_matrix))\n",
    "\n",
    "                    # Update running variables from batch statistics\n",
    "                    absolute_error_covariance_matrix_batch_features_variable.assign(value = update_covariance_batch(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                                                    mean_a = absolute_error_mean_batch_features_variable, \n",
    "                                                                                                                    cov_a = absolute_error_covariance_matrix_batch_features_variable, \n",
    "                                                                                                                    count_b = current_batch_size, \n",
    "                                                                                                                    mean_b = absolute_error_mean_batch_features, \n",
    "                                                                                                                    cov_b = absolute_error_reshaped_batch_features_covariance_matrix, \n",
    "                                                                                                                    sample_covariance = True)) # shape = (sequence_length, sequence_length)\n",
    "                    \n",
    "                    absolute_error_mean_batch_features_variable.assign(value = update_mean_batch(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                                 mean_a = absolute_error_mean_batch_features_variable, \n",
    "                                                                                                 count_b = current_batch_size, \n",
    "                                                                                                 mean_b = absolute_error_mean_batch_features)) # shape = (sequence_length,)\n",
    "                    \n",
    "                    absolute_error_count_batch_features_variable.assign(value = update_count(count_a = absolute_error_count_batch_features_variable, \n",
    "                                                                                             count_b = current_batch_size)) # shape = ()\n",
    "\n",
    "                    absolute_error_inverse_covariance_matrix_batch_features_variable.assign(value = tf.matrix_inverse(input = absolute_error_covariance_matrix_batch_features_variable)) # shape = (sequence_length, sequence_length)\n",
    "\n",
    "                    return absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable\n",
    "                \n",
    "                # Check if batch is a singleton or not, very important for covariance math\n",
    "                singleton_batch_condition = tf.equal(x = current_batch_size, y = 1) # shape = ()\n",
    "                \n",
    "                absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable = \\\n",
    "                    tf.cond(pred = singleton_batch_condition, \n",
    "                            true_fn = lambda: singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable), \n",
    "                            false_fn = lambda: non_singleton_batch_mahalanobis_distance_variable_updating(absolute_error, absolute_error_count_batch_time_variable, absolute_error_mean_batch_time_variable, absolute_error_covariance_matrix_batch_time_variable, absolute_error_inverse_covariance_matrix_batch_time_variable, absolute_error_count_batch_features_variable, absolute_error_mean_batch_features_variable, absolute_error_covariance_matrix_batch_features_variable, absolute_error_inverse_covariance_matrix_batch_features_variable))\n",
    "                \n",
    "    else: # mode == tf.estimator.ModeKeys.PREDICT\n",
    "        def mahalanobis_distance(error_vectors_reshaped, mean_vector, inverse_covariance_matrix, final_shape):\n",
    "            error_vectors_reshaped_centered = error_vectors_reshaped - mean_vector # time_shape = (current_batch_size * sequence_length, number_of_features), features_shape = (current_batch_size * number_of_features, sequence_length)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: error_vectors_reshaped_centered = \\n{}\".format(error_vectors_reshaped_centered))\n",
    "\n",
    "            mahalanobis_right_matrix_product = tf.matmul(a = inverse_covariance_matrix, # time_shape = (number_of_features, current_batch_size * sequence_length), features_shape = (sequence_length, current_batch_size * number_of_features)\n",
    "                                                         b = error_vectors_reshaped_centered,\n",
    "                                                         transpose_b = True)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_right_matrix_product = \\n{}\".format(mahalanobis_right_matrix_product))\n",
    "\n",
    "\n",
    "            mahalanobis_distance_vectorized = tf.matmul(a = error_vectors_reshaped_centered, # time_shape = (current_batch_size * sequence_length, current_batch_size * sequence_length), features_shape = (current_batch_size * number_of_features, current_batch_size * number_of_features)\n",
    "                                                        b = mahalanobis_right_matrix_product)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_vectorized = \\n{}\".format(mahalanobis_distance_vectorized))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_vectorized.shape = \\n{}\".format(mahalanobis_distance_vectorized.shape))\n",
    "\n",
    "            mahalanobis_distance_flat = tf.diag_part(input = mahalanobis_distance_vectorized) # time_shape = (current_batch_size * sequence_length,), features_shape = (current_batch_size * number_of_features,)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_flat = \\n{}\".format(mahalanobis_distance_flat))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance: mahalanobis_distance_flat.shape = \\n{}\".format(mahalanobis_distance_flat.shape))\n",
    "\n",
    "            mahalanobis_distance_final_shaped = tf.reshape(tensor = mahalanobis_distance_flat, shape = [-1, final_shape]) # time_shape = (current_batch_size, sequence_length), features_shape = (current_batch_size, number_of_features)\n",
    "\n",
    "            return mahalanobis_distance_final_shaped\n",
    "          \n",
    "        absolute_error = tf.abs(x = Y - predictions) # shape = (current_batch_size, sequence_length, number_of_features)\n",
    "#         print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error = \\n{}\".format(absolute_error))\n",
    "        \n",
    "        with tf.variable_scope(name_or_scope = \"mahalanobis_distance_variables\", reuse = tf.AUTO_REUSE):\n",
    "            # Time based\n",
    "            absolute_error_reshaped_batch_time = tf.reshape(tensor = absolute_error,  # shape = (current_batch_size * sequence_length, number_of_features)\n",
    "                                                            shape = [current_batch_size * params[\"sequence_length\"], number_of_features])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_time = \\n{}\".format(absolute_error_reshaped_batch_time))\n",
    "\n",
    "            mahalanobis_distance_batch_time = mahalanobis_distance(error_vectors_reshaped = absolute_error_reshaped_batch_time,  # shape = (current_batch_size, sequence_length)\n",
    "                                                                   mean_vector = absolute_error_mean_batch_time_variable, \n",
    "                                                                   inverse_covariance_matrix = absolute_error_inverse_covariance_matrix_batch_time_variable, \n",
    "                                                                   final_shape = params[\"sequence_length\"])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_time = \\n{}\".format(mahalanobis_distance_batch_time))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_time.shape = \\n{}\".format(mahalanobis_distance_batch_time.shape))\n",
    "\n",
    "            # Features based\n",
    "            absolute_error_mapped_batch_features = tf.map_fn(fn = lambda x: tf.transpose(a = absolute_error[x, :, :]), # shape = (current_batch_size, number_of_features, sequence_length)\n",
    "                                                             elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "                                                             dtype = tf.float64)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_mapped_batch_features = \\n{}\".format(absolute_error_mapped_batch_features))\n",
    "\n",
    "            absolute_error_reshaped_batch_features = tf.reshape(tensor = absolute_error_mapped_batch_features, # shape = (current_batch_size * number_of_features, sequence_length)\n",
    "                                                                shape = [current_batch_size * number_of_features, params[\"sequence_length\"]])\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: absolute_error_reshaped_batch_features = \\n{}\".format(absolute_error_reshaped_batch_features))\n",
    "\n",
    "            mahalanobis_distance_batch_features = mahalanobis_distance(error_vectors_reshaped = absolute_error_reshaped_batch_features, # shape = (current_batch_size, number_of_features)\n",
    "                                                                       mean_vector = absolute_error_mean_batch_features_variable, \n",
    "                                                                       inverse_covariance_matrix = absolute_error_inverse_covariance_matrix_batch_features_variable,\n",
    "                                                                       final_shape = number_of_features)\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_features = \\n{}\".format(mahalanobis_distance_batch_features))\n",
    "#             print(\"lstm_encoder_decoder_autoencoder_anomaly_detection: mahalanobis_distance_batch_features.shape = \\n{}\".format(mahalanobis_distance_batch_features.shape))\n",
    "            \n",
    "        batch_time_anomaly_flags = tf.where(condition = tf.reduce_any(input_tensor = tf.greater(x = tf.abs(x = mahalanobis_distance_batch_time), # shape = (current_batch_size,)\n",
    "                                                                                                y = params[\"time_anomaly_threshold\"]), \n",
    "                                                                      axis = 1), \n",
    "                                            x = tf.ones(shape = [current_batch_size], dtype = tf.int64), \n",
    "                                            y = tf.zeros(shape = [current_batch_size], dtype = tf.int64))\n",
    "        \n",
    "        batch_features_anomaly_flags = tf.where(condition = tf.reduce_any(input_tensor = tf.greater(x = tf.abs(x = mahalanobis_distance_batch_features), # shape = (current_batch_size,)\n",
    "                                                                                                    y = params[\"features_anomaly_threshold\"]), \n",
    "                                                                          axis = 1), \n",
    "                                                x = tf.ones(shape = [current_batch_size], dtype = tf.int64), \n",
    "                                                y = tf.zeros(shape = [current_batch_size], dtype = tf.int64))\n",
    "        \n",
    "        # Create predictions\n",
    "        predictions_dict = {\"predictions\": predictions, \n",
    "                            \"mahalanobis_distance_batch_time\": mahalanobis_distance_batch_time, \n",
    "                            \"mahalanobis_distance_batch_features\": mahalanobis_distance_batch_features, \n",
    "                            \"batch_time_anomaly_flags\": batch_time_anomaly_flags, \n",
    "                            \"batch_features_anomaly_flags\": batch_features_anomaly_flags}\n",
    "\n",
    "        # Create export outputs\n",
    "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = predictions_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our serving input function to accept the data at serving and send it in the right format to our custom estimator\n",
    "def serving_input_fn(sequence_length):\n",
    "    # This function fixes the shape and type of our input strings\n",
    "    def fix_shape_and_type_for_serving(placeholder):\n",
    "        current_batch_size = tf.shape(input = placeholder, out_type = tf.int64)[0]\n",
    "        \n",
    "        # String split each string in the batch and output the values from the resulting SparseTensors\n",
    "        split_string = tf.stack(values = tf.map_fn( # shape = (batch_size, sequence_length)\n",
    "            fn = lambda x: tf.string_split(source = [placeholder[x]], delimiter = ',').values, \n",
    "            elems = tf.range(start = 0, limit = current_batch_size, dtype = tf.int64), \n",
    "            dtype = tf.string), axis = 0)\n",
    "#         print(\"serving_input_fn: fix_shape_and_type_for_serving: split_string = {}\".format(split_string))\n",
    "        \n",
    "        # Convert each string in the split tensor to float\n",
    "        feature_tensor = tf.string_to_number(string_tensor = split_string, out_type = tf.float64) # shape = (batch_size, sequence_length)\n",
    "#         print(\"serving_input_fn: fix_shape_and_type_for_serving: feature_tensor = {}\".format(feature_tensor))\n",
    "        \n",
    "        return feature_tensor\n",
    "    \n",
    "    # This function fixes dynamic shape ambiguity of last dimension so that we will be able to use it in our DNN (since tf.layers.dense require the last dimension to be known)\n",
    "    def get_shape_and_set_modified_shape_2D(tensor, additional_dimension_sizes):\n",
    "        # Get static shape for tensor and convert it to list\n",
    "        shape = tensor.get_shape().as_list()\n",
    "        # Set outer shape to additional_dimension_sizes[0] since we know that this is the correct size\n",
    "        shape[1] = additional_dimension_sizes[0]\n",
    "        # Set the shape of tensor to our modified shape\n",
    "        tensor.set_shape(shape = shape) # shape = (batch_size, additional_dimension_sizes[0])\n",
    "#         print(\"serving_input_fn: get_shape_and_set_modified_shape_2D: tensor = {}, additional_dimension_sizes = {}\".format(tensor, additional_dimension_sizes))\n",
    "        return tensor\n",
    "            \n",
    "    # Create placeholders to accept the data sent to the model at serving time\n",
    "    feature_placeholders = { # all features come in as a batch of strings, shape = (batch_size,), this was so because of passing the arrays to online ml-engine prediction\n",
    "        feature: tf.placeholder(dtype = tf.string, shape = [None]) for feature in CSV_COLUMNS\n",
    "    }\n",
    "#     print(\"\\nserving_input_fn: feature_placeholders = {}\".format(feature_placeholders))\n",
    "    \n",
    "    # Create feature tensors\n",
    "    features = {key: fix_shape_and_type_for_serving(placeholder = tensor) for key, tensor in feature_placeholders.items()}\n",
    "#     print(\"serving_input_fn: features = {}\".format(features))\n",
    "    \n",
    "    # Fix dynamic shape ambiguity of feature tensors for our DNN\n",
    "    features = {key: get_shape_and_set_modified_shape_2D(tensor = tensor, additional_dimension_sizes = [sequence_length]) for key, tensor in features.items()}\n",
    "#     print(\"serving_input_fn: features = {}\".format(features))\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(args):\n",
    "    if args[\"tune_anomaly_thresholds\"] == False:\n",
    "        if args[\"calculate_error_distribution_statistics\"] == False:\n",
    "            # Create our custom estimator using our model function without calculating error distribution statistics\n",
    "            estimator = tf.estimator.Estimator(\n",
    "                model_fn = lstm_encoder_decoder_autoencoder_anomaly_detection,\n",
    "                model_dir = args['output_dir'],\n",
    "                params = {\n",
    "                    \"sequence_length\": args[\"sequence_length\"],\n",
    "                    \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "                    \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "                    \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "                    \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "                    \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "                    \"learning_rate\": args[\"learning_rate\"],\n",
    "                    \"calculate_error_distribution_statistics\": args[\"calculate_error_distribution_statistics\"],\n",
    "                    \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "                    \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "\n",
    "            early_stopping_hook = tf.contrib.estimator.stop_if_no_decrease_hook(\n",
    "                estimator = estimator,\n",
    "                metric_name = \"rmse\",\n",
    "                max_steps_without_decrease = 100,\n",
    "                min_steps = 1000,\n",
    "                run_every_secs = 60,\n",
    "                run_every_steps = None)\n",
    "\n",
    "            # Create train spec to read in our training data\n",
    "            train_spec = tf.estimator.TrainSpec(\n",
    "                input_fn = read_dataset(\n",
    "                    filename = args[\"train_file_pattern\"],\n",
    "                    mode = tf.estimator.ModeKeys.TRAIN, \n",
    "                    batch_size = args[\"train_batch_size\"],\n",
    "                    params = args),\n",
    "                max_steps = args[\"train_steps\"], \n",
    "                hooks = [early_stopping_hook])\n",
    "\n",
    "#             # Create exporter to save out the complete model to disk\n",
    "#             exporter = tf.estimator.BestExporter(\n",
    "#                 name = \"best_exporter\", \n",
    "#                 serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]),\n",
    "#                 exports_to_keep = 5)\n",
    "\n",
    "            # Create eval spec to read in our validation data and export our model\n",
    "            eval_spec = tf.estimator.EvalSpec(\n",
    "                input_fn = read_dataset(\n",
    "                    filename = args[\"eval_file_pattern\"], \n",
    "                    mode = tf.estimator.ModeKeys.EVAL, \n",
    "                    batch_size = args[\"eval_batch_size\"],\n",
    "                    params = args),\n",
    "                steps = None,\n",
    "                start_delay_secs = args[\"start_delay_secs\"], # start evaluating after N seconds\n",
    "                throttle_secs = args[\"throttle_secs\"])#,    # evaluate every N seconds\n",
    "#                 exporters = exporter)\n",
    "\n",
    "            # Create train and evaluate loop to train and evaluate our estimator\n",
    "            tf.estimator.train_and_evaluate(estimator = estimator, train_spec = train_spec, eval_spec = eval_spec)\n",
    "        else: # args[\"calculate_error_distribution_statistics\"] == True\n",
    "            # Create our custom estimator using our model function NOW calculating error distribution statistics\n",
    "            estimator = tf.estimator.Estimator(\n",
    "                model_fn = lstm_encoder_decoder_autoencoder_anomaly_detection,\n",
    "                model_dir = args['output_dir'],\n",
    "                params = {\n",
    "                    \"sequence_length\": args[\"sequence_length\"],\n",
    "                    \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "                    \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "                    \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "                    \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "                    \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "                    \"learning_rate\": args[\"learning_rate\"],\n",
    "                    \"calculate_error_distribution_statistics\": args[\"calculate_error_distribution_statistics\"],\n",
    "                    \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "                    \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "\n",
    "            # Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "            estimator.evaluate(\n",
    "                input_fn = read_dataset(\n",
    "                    filename = arguments[\"eval_file_pattern\"], \n",
    "                    mode = tf.estimator.ModeKeys.EVAL, \n",
    "                    batch_size = 32,\n",
    "                    params = args),\n",
    "                steps = None)\n",
    "\n",
    "            # Export savedmodel with learned error distribution statistics to be used for inference\n",
    "\n",
    "            estimator.export_savedmodel(\n",
    "                export_dir_base = args['output_dir'], \n",
    "                serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]))\n",
    "    else: # args[\"tune_anomaly_thresholds\"] == True\n",
    "        # Create our custom estimator using our model function NOW calculating error distribution statistics\n",
    "        estimator = tf.estimator.Estimator(\n",
    "            model_fn = lstm_encoder_decoder_autoencoder_anomaly_detection,\n",
    "            model_dir = args['output_dir'],\n",
    "            params = {\n",
    "                \"sequence_length\": args[\"sequence_length\"],\n",
    "                \"reverse_labels_sequence\": args[\"reverse_labels_sequence\"],\n",
    "                \"encoder_lstm_hidden_units\": args[\"encoder_lstm_hidden_units\"],\n",
    "                \"decoder_lstm_hidden_units\": args[\"decoder_lstm_hidden_units\"],\n",
    "                \"lstm_dropout_output_keep_probs\": args[\"lstm_dropout_output_keep_probs\"], \n",
    "                \"dnn_hidden_units\": args[\"dnn_hidden_units\"], \n",
    "                \"learning_rate\": args[\"learning_rate\"],\n",
    "                \"calculate_error_distribution_statistics\": False,\n",
    "                \"time_anomaly_threshold\": args[\"time_anomaly_threshold\"], \n",
    "                \"features_anomaly_threshold\": args[\"features_anomaly_threshold\"]})\n",
    "\n",
    "        # Get final mahalanobis statistics over the entire validation_1 dataset\n",
    "        estimator.evaluate(\n",
    "            input_fn = read_dataset(\n",
    "                filename = arguments[\"eval_file_pattern\"], \n",
    "                mode = tf.estimator.ModeKeys.EVAL, \n",
    "                batch_size = 32,\n",
    "                params = args),\n",
    "            steps = None)\n",
    "\n",
    "        # Export savedmodel with learned error distribution statistics to be used for inference\n",
    "\n",
    "        estimator.export_savedmodel(\n",
    "            export_dir_base = args['output_dir'], \n",
    "            serving_input_receiver_fn = lambda: serving_input_fn(args[\"sequence_length\"]))\n",
    "        \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {}\n",
    "# File arguments\n",
    "arguments[\"train_file_pattern\"] = \"data/training_normal_sequences.csv\"\n",
    "arguments[\"eval_file_pattern\"] = \"data/validation_normal_1_sequences.csv\"\n",
    "arguments[\"output_dir\"] = \"trained_model\"\n",
    "\n",
    "# Sequence shape hyperparameters\n",
    "arguments[\"sequence_length\"] = sequence_length\n",
    "arguments[\"horizon\"] = 0\n",
    "arguments[\"reverse_labels_sequence\"] = True\n",
    "\n",
    "# Architecture hyperparameters\n",
    "\n",
    "# LSTM hyperparameters\n",
    "arguments[\"encoder_lstm_hidden_units\"] = [64, 32, 16]\n",
    "arguments[\"decoder_lstm_hidden_units\"] = [16, 32, 64]\n",
    "arguments[\"lstm_dropout_output_keep_probs\"] = [1.0, 1.0, 1.0]\n",
    "\n",
    "# DNN hyperparameters\n",
    "arguments[\"dnn_hidden_units\"] = [1024, 256, 64]\n",
    "\n",
    "# Training parameters\n",
    "arguments[\"train_batch_size\"] = 32\n",
    "arguments[\"eval_batch_size\"] = 32\n",
    "arguments[\"train_steps\"] = 100\n",
    "arguments[\"learning_rate\"] = 0.01\n",
    "arguments[\"start_delay_secs\"] = 60\n",
    "arguments[\"throttle_secs\"] = 120\n",
    "\n",
    "# Anomaly thresholds\n",
    "arguments[\"calculate_error_distribution_statistics\"] = False\n",
    "arguments[\"tune_anomaly_thresholds\"] = False\n",
    "arguments[\"time_anomaly_threshold\"] = 2.0\n",
    "arguments[\"features_anomaly_threshold\"] = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26b769e198>, '_tf_random_seed': None, '_num_ps_replicas': 0, '_task_type': 'worker', '_global_id_in_cluster': 0, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_is_chief': True, '_experimental_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_save_checkpoints_steps': None, '_train_distribute': None, '_device_fn': None, '_keep_checkpoint_every_n_hours': 10000, '_eval_distribute': None, '_master': '', '_evaluation_master': '', '_model_dir': 'trained_model', '_keep_checkpoint_max': 5, '_task_id': 0, '_protocol': None}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1056024, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 100 into trained_model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-23T14:43:03Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-23-14:43:05\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 0.020502344, mae = 0.10997396, rmse = 0.14328055\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Loss for final step: 0.024290485.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "shutil.rmtree(path = arguments[\"output_dir\"], ignore_errors = True) # start fresh each time\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f26b769e898>, '_tf_random_seed': None, '_num_ps_replicas': 0, '_task_type': 'worker', '_global_id_in_cluster': 0, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_is_chief': True, '_experimental_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_service': None, '_save_checkpoints_steps': None, '_train_distribute': None, '_device_fn': None, '_keep_checkpoint_every_n_hours': 10000, '_eval_distribute': None, '_master': '', '_evaluation_master': '', '_model_dir': 'trained_model', '_keep_checkpoint_max': 5, '_task_id': 0, '_protocol': None}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "update_covariance_batch: count_a = \n",
      "<tf.Variable 'mahalanobis_distance_variables/absolute_error_count_batch_time_variable:0' shape=() dtype=int64_ref>\n",
      "update_covariance_batch: mean_a = \n",
      "<tf.Variable 'mahalanobis_distance_variables/absolute_error_mean_batch_time_variable:0' shape=(3,) dtype=float64_ref>\n",
      "update_covariance_batch: cov_a = \n",
      "<tf.Variable 'mahalanobis_distance_variables/absolute_error_covariance_matrix_batch_time_variable:0' shape=(3, 3) dtype=float64_ref>\n",
      "update_covariance_batch: count_b = \n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
      "update_covariance_batch: mean_b = \n",
      "Tensor(\"mahalanobis_distance_variables_1/cond/Mean:0\", shape=(3,), dtype=float64)\n",
      "update_covariance_batch: cov_b = \n",
      "Tensor(\"mahalanobis_distance_variables_1/cond/truediv_4:0\", shape=(3, 3), dtype=float64)\n",
      "update_covariance_batch: count_a = \n",
      "<tf.Variable 'mahalanobis_distance_variables/absolute_error_count_batch_features_variable:0' shape=() dtype=int64_ref>\n",
      "update_covariance_batch: mean_a = \n",
      "<tf.Variable 'mahalanobis_distance_variables/absolute_error_mean_batch_features_variable:0' shape=(30,) dtype=float64_ref>\n",
      "update_covariance_batch: cov_a = \n",
      "<tf.Variable 'mahalanobis_distance_variables/absolute_error_covariance_matrix_batch_features_variable:0' shape=(30, 30) dtype=float64_ref>\n",
      "update_covariance_batch: count_b = \n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int64)\n",
      "update_covariance_batch: mean_b = \n",
      "Tensor(\"mahalanobis_distance_variables_1/cond/Mean_1:0\", shape=(30,), dtype=float64)\n",
      "update_covariance_batch: cov_b = \n",
      "Tensor(\"mahalanobis_distance_variables_1/cond/truediv_8:0\", shape=(30, 30), dtype=float64)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-23T14:43:09Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-23-14:43:13\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 0.020502344, mae = 0.10997396, rmse = 0.14328055\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predict_export_outputs']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Restoring parameters from trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_model/temp-b'1553352193'/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "arguments[\"calculate_error_distribution_statistics\"] = True\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments[\"tune_anomaly_thresholds\"] = True\n",
    "estimator = train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now write into a python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf trained_model\n",
    "export PYTHONPATH=$PYTHONPATH:$PWD/lstm_autoencoder_anomaly_detection_module\n",
    "python -m trainer.task \\\n",
    "    --train_file_pattern=\"data/training_normal_sequences.csv\" \\\n",
    "    --eval_file_pattern=\"data/validation_normal_1_sequences.csv\"  \\\n",
    "    --output_dir=$PWD/trained_model \\\n",
    "    --job-dir=./tmp \\\n",
    "    --batch_size=32 \\\n",
    "    --sequence_length=13 \\\n",
    "    --horizon=0 \\\n",
    "    --reverse_labels_sequence=True \\\n",
    "    --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "    --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "    --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "    --dnn_hidden_units=\"1024 256 64\" \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.1 \\\n",
    "    --start_delay_secs=60 \\\n",
    "    --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r data gs://$BUCKET/lstm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/trained_model\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/training_normal_sequences.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/validation_normal_1_sequences.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --batch_size=32 \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --encoder_lstm_hidden_units=\"64 32 16\" \\\n",
    "  --encoder_lstm_hidden_units=\"16 32 64\" \\\n",
    "  --lstm_dropout_output_keep_probs=\"0.9 0.95 1.0\" \\\n",
    "  --dnn_hidden_units=\"1024 256 64\" \\\n",
    "  --train_steps=1000 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    hyperparameterMetricTag: mae\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 1\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: sequence_length\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 120\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: encoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: decoder_lstm_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"64 32 16\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: lstm_dropout_output_keep_probs\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"0.9 1.0 1.0\", \"0.95 0.95 1.0\", \"0.95 0.95 0.95\"]\n",
    "    - parameterName: dnn_hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"256 128 64\", \"256 128 16\", \"64 64 64\"]\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://$BUCKET/lstm_autoencoder/hyperparam\n",
    "JOBNAME=job_lstm_autoencoder$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$PWD/lstm_autoencoder_anomaly_detection_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET/lstm_autoencoder/staging \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  --runtime-version=1.8 \\\n",
    "  -- \\\n",
    "  --train_file_pattern=gs://$BUCKET/lstm_autoencoder/data/train.csv \\\n",
    "  --eval_file_pattern=gs://$BUCKET/lstm_autoencoder/data/eval.csv  \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --sequence_length=13 \\\n",
    "  --horizon=0 \\\n",
    "  --reverse_labels_sequence=True \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=60 \\\n",
    "  --throttle_secs=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil -m cp -r trained_model gs://qwiklabs-gcp-8923d4964bfbd247-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"lstm_autoencoder_anomaly_detection\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://$BUCKET/trained_model/export/exporter/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create $MODEL_NAME --regions $REGION\n",
    "gcloud ml-engine versions create $MODEL_VERSION --model $MODEL_NAME --origin $MODEL_LOCATION --runtime-version 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prediction_instances = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local prediction from local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sequences.json', 'w') as outfile:\n",
    "  test_data_anomalous_string_list = [[np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"normal_freq\"], tag[\"normal_ampl\"], tag[\"normal_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list] for _ in range(0, number_of_prediction_instances)]\n",
    "  json_string = \"\"\n",
    "  for item in test_data_anomalous_string_list:\n",
    "    json_string += \"{\" + ','.join([\"{0}: \\\"{1}\\\"\".format('\\\"' + CSV_COLUMNS[i] + '\\\"', item[i]) for i in range(0, len(CSV_COLUMNS))]) + \"}\\n\"\n",
    "  json_string = json_string.replace(' ', '').replace(':', ': ').replace(',', ', ')\n",
    "  outfile.write(\"%s\" % json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_FEATURES_ANOMALY_FLAGS  BATCH_TIME_ANOMALY_FLAGS  MAHALANOBIS_DISTANCE_BATCH_FEATURES                            MAHALANOBIS_DISTANCE_BATCH_TIME                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     PREDICTIONS\n",
      "0                             0                         [0.3609838334886698, 4.285870630025189, 7.331315729850488]     [11.200225051015526, 6.302422447857527, 7.133295460598505, 3.6845064928930844, 1.1992155852273427, -0.5447650456691467, 1.9361679857458611, 2.5135337828994233, 3.3670350134756912, 2.801834665525715, 3.2638614318900023, 3.541391080605428, 3.664099186170625, 3.416207555117292, 2.564566278260998, 3.098551595086743, 2.8757662326783655, 2.944446882900374, 2.5923076746097533, 2.1807012131847516, 2.0858074862031426, 1.9917409626733913, 1.62776007937862, 1.8516518120374328, 1.6187862158429356, 1.3506691819893726, 1.4266776508038121, 0.916245277689105, 1.1588153124928775, 0.6932429883551378]       [[0.40790831607535405, 0.1314709457844174, 0.5576687073811238], [0.43602587445659946, 0.17757222683221155, 0.6069015880124863], [0.3512127548797239, 0.21285888671972972, 0.5175151726411527], [0.1909099143044662, 0.21988556982960333, 0.34479207136292733], [0.040418973163793795, 0.17753858364928443, 0.1429725034309734], [-0.0509568655966606, 0.09194563852431625, 0.004355091659595883], [-0.08330783203938308, 0.016570912079525543, -0.09101288320166176], [-0.06881216601098997, -0.029769675029759583, -0.14710909735112845], [-0.02545422735480067, -0.054139099720379724, -0.17432250347661074], [0.026162005532254992, -0.06318016409946059, -0.18271081384344537], [0.07035114897109203, -0.06326862594010416, -0.17972862965423084], [0.10298317583567794, -0.06115432572093968, -0.17086195101100035], [0.12275486068196033, -0.059759529818670276, -0.15992576179039994], [0.1325446447021621, -0.059709927281351854, -0.14905565544376917], [0.13507058867958704, -0.060570428974457, -0.1395431427826245], [0.1333047929537547, -0.06186325450600283, -0.13174619248597638], [0.12957501972301144, -0.06311568963235759, -0.12568779032052324], [0.12543274141445465, -0.06405043521587322, -0.12118708780378004], [0.12173961642047527, -0.06459182164907215, -0.11800065300607823], [0.11885158671119699, -0.06477537813727713, -0.11585305316272268], [0.11683186575243774, -0.06469143568155641, -0.11448686363663177], [0.11556024402109737, -0.0644478028583449, -0.11369037975106483], [0.11484933828459688, -0.06413491124649368, -0.11329287081646916], [0.11451330110251758, -0.06381515455948505, -0.11315895852080007], [0.11439953940226548, -0.06352461598267614, -0.11318362323599965], [0.11439803253932944, -0.06327971809577798, -0.1132899117191822], [0.1144384239273764, -0.06308409238539195, -0.11342533346170625], [0.11448172340816186, -0.06293406708460286, -0.11355719650153115], [0.1145107309835416, -0.06282258234253757, -0.11366782412073476], [0.1145212757344398, -0.06274161820933444, -0.11375020325989943]]\n",
      "0                             0                         [0.8219883497032462, 0.055461907539454736, 5.796212136374984]  [1.8645170892564253, 3.9836337735147618, 1.8542347166844362, -0.033575393978855395, 6.145771325074644, 1.768587041978376, 2.3834463585553176, 2.574674427996713, 3.2025060035922506, 3.462554262137925, 2.9688014712041824, 3.6392797244896276, 2.4264204124998616, 2.959295282571362, 2.80761968566804, 2.9381641266901752, 2.4564605322672155, 2.5326728489893084, 2.329083831591694, 2.4966532231496714, 2.6100743493246377, 2.2269274862369937, 1.8940177291539273, 1.2706479646215427, 2.0727683721480963, 1.283321209134646, 1.2595577603039132, 1.3285245101596748, 0.7936106573927502, 0.8289920919325134]  [[0.37855912823148397, 0.3276541141938409, 0.5632095514622795], [0.3501254623144073, 0.40179052461442666, 0.6251374222675912], [0.22515646361341451, 0.3602035910439996, 0.5419469824099867], [0.08054095374897387, 0.25646720021005315, 0.3662084132746196], [-0.03151181099608344, 0.11457717050204691, 0.1582802260107087], [-0.0735656770467564, -0.0015116772693178938, 0.0012455391787867809], [-0.06506950284183292, -0.062137909794032886, -0.10367219303485772], [-0.023467819203992712, -0.07973294050354368, -0.16834460180524838], [0.0265330855539838, -0.07879421952072602, -0.19678329564302918], [0.07057309376638986, -0.0707319806127257, -0.2010697614898708], [0.10301517396811792, -0.06387586121475072, -0.19211571596958757], [0.1228883435207395, -0.0601743945077822, -0.17758698776082535], [0.13245917797754408, -0.05963134608009171, -0.16206085728971217], [0.1348398765249712, -0.06067834918805032, -0.14827145051346688], [0.13301327220448572, -0.062305068821883136, -0.1370436583088163], [0.12932313629958392, -0.06378481999272204, -0.12850752240003463], [0.12526404902580357, -0.06476952444161771, -0.12235315458617271], [0.12167188230255364, -0.06521059217947714, -0.11813091332520045], [0.11886763975935492, -0.06524063241560435, -0.11542327699190935], [0.116894657302246, -0.06499721505617723, -0.11380188466180594], [0.11564816298125981, -0.06461904012203379, -0.11294234625261126], [0.11494736842320051, -0.06420762259338653, -0.11259339403471624], [0.11461117628280672, -0.06382351370814521, -0.11256543991687998], [0.11449046256068489, -0.06349531194356842, -0.11271733745163773], [0.11447777034805405, -0.06323117599133868, -0.11294912650683739], [0.1145046955963535, -0.063027991673204, -0.11319510701997565], [0.11453371482929951, -0.06287737118269199, -0.11341656780152568], [0.11454890669653782, -0.0627692359492831, -0.11359459674839492], [0.11454711575694608, -0.06269374285074342, -0.11372400575300895], [0.1145315361956404, -0.0626422519620754, -0.11380801994443521]]\n",
      "0                             0                         [-0.7618405752393946, 2.0472636261020174, 6.098479530952007]   [6.521727017579424, -1.13311344924653, 8.03878130454521, 1.633957236993516, 5.1722551804192936, 1.9725571764099188, 2.732036936348763, 3.3075975824484942, 2.837005259517098, 3.400724364028486, 3.221745996301097, 3.7083227645891315, 3.3432414771262673, 3.0155772309930704, 2.534317481099305, 3.105172199891673, 2.3694590811009135, 2.1743436566992056, 2.665290039374535, 2.113722510995461, 2.2822697733486925, 2.219368510787872, 1.3349604511571596, 1.6440898311880034, 1.3121233049677417, 1.453964453397509, 1.4051181140381663, 1.0402576931639516, 0.7785925296092168, 0.6789342070316884]           [[0.3829533731757644, 0.25815732060171304, 0.4567965700990432], [0.3802547786186998, 0.3152579366752464, 0.4723974947561075], [0.2653772710275144, 0.304541481470157, 0.37154542276460634], [0.11877047566911571, 0.24288612834565965, 0.2097726629395046], [0.0010326551867217704, 0.12807118106130783, 0.06064016587552695], [-0.05961507668009036, 0.028228527461563454, -0.04955901023801987], [-0.06509953014811934, -0.033945322831514635, -0.12010576428241318], [-0.035318298934147466, -0.0629528305111137, -0.15990288655529533], [0.010147646720637171, -0.07215897036483895, -0.17595558813617576], [0.0548440084753406, -0.07003508341556805, -0.1769293848515773], [0.09059275882963123, -0.0652907064400175, -0.1697265431384127], [0.11441595476573711, -0.061675897853288725, -0.15939178209303875], [0.12796162273256217, -0.06031461495696813, -0.14847365609648752], [0.13335130452901128, -0.06042678643953017, -0.13891004990388858], [0.1334564291837782, -0.06141387805819551, -0.131131195744939], [0.13072692612239134, -0.06260733380709199, -0.12516943084489726], [0.12693321667274482, -0.06359424004946845, -0.12079842793019624], [0.1231712414976621, -0.06422622604335601, -0.1177398888365393], [0.1200156011286763, -0.0644905251003336, -0.11567893493101356], [0.11765924918510122, -0.0644720107717644, -0.11436155562719413], [0.11607446942972642, -0.06427763501232706, -0.1135854042140042], [0.11511629080171952, -0.06400083568007225, -0.11319100571960736], [0.114609163966041, -0.06370674121820497, -0.11305326771630123], [0.11439242411530277, -0.06343413803570055, -0.11307442145664287], [0.11434005664299478, -0.06320195627342154, -0.11318133268811145], [0.11436454310405629, -0.06301583949869344, -0.11332249761250687], [0.11441212789593543, -0.06287356414459117, -0.11346446859163592], [0.11445469892883418, -0.06276894124797816, -0.11358791720674663], [0.11448107181045357, -0.06269447527643673, -0.11368413510788972], [0.11448998841759604, -0.06264291477333511, -0.11375161240931908]]\n",
      "0                             0                         [0.561971881159741, 3.144492070559409, 5.559988352106144]      [4.190194300663683, 1.4710439689342678, 4.347724378274937, 1.861738909890693, 0.8429147407495471, -0.007912718377042854, 2.986952287500884, 2.9942066846844533, 2.9259537577803285, 3.2956412674144637, 3.3808188797491345, 3.36897968755574, 3.3339141580150278, 3.1402571775057986, 3.197480884429736, 3.0238814589302065, 2.632343817529189, 2.603985277555193, 2.146164780937704, 2.0720342998955474, 2.027203436708546, 1.9680754954880402, 2.052353745457873, 1.6596117277828235, 1.6047091122912733, 1.4427771439659787, 1.225641852560581, 1.113845788056635, 0.9966067415032669, 0.7585169723201464]       [[0.33261414458119937, 0.21896942990028034, 0.561473342223354], [0.3018065150458164, 0.2830216394029052, 0.6208930625082533], [0.19757000647544615, 0.29002159021488016, 0.5396420088915846], [0.07351365799229508, 0.2437578119024959, 0.36796631276133585], [-0.026363715612333402, 0.15066521828713944, 0.16465325677232442], [-0.06861987119190817, 0.052246946464006516, 0.011506562521672592], [-0.06519227853509761, -0.01630774763028914, -0.09405059040099742], [-0.027567933848942225, -0.049168223177326, -0.16046216235419267], [0.021162466139000344, -0.062045737802643136, -0.1925035074126921], [0.06531660749677705, -0.0632281001604259, -0.2006602156833275], [0.0988261328006164, -0.06115362655381604, -0.19470491423226718], [0.12032034523644376, -0.05959110458546074, -0.18199936869840025], [0.1314161982843685, -0.05964502377341918, -0.16705126011434707], [0.13490061045591645, -0.06073568392028128, -0.1530723509586949], [0.13371224410945937, -0.06222120414851279, -0.1411968523298007], [0.13028912310782564, -0.06363714195074457, -0.13182903333663318], [0.12625694406243093, -0.0646657890761779, -0.12485333659427134], [0.1225572818349173, -0.06521441631032526, -0.1199218653541175], [0.11960441583431776, -0.06534743452577238, -0.11663445339721976], [0.11746885324425622, -0.06519322172812395, -0.11459958916752089], [0.11607293317688804, -0.06486480583301896, -0.11344739116225916], [0.11525103785615583, -0.06446701865831894, -0.1128986117211177], [0.11482333391200701, -0.06407062616388112, -0.11273889529711315], [0.11463675435152938, -0.06371569618364517, -0.11280806002548263], [0.11457848451040951, -0.06341929731748192, -0.11299116957643773], [0.11457488361388972, -0.06318383078181775, -0.11321108729373697], [0.11458394356512634, -0.06300379691294408, -0.11342075437982138], [0.11458618249509868, -0.06287026809450791, -0.1135953934677208], [0.11457593241939772, -0.06277359953191552, -0.11372580274668079], [0.11455464035908296, -0.0627048326538576, -0.11381261776280252]]\n",
      "0                             0                         [4.296835155684376, 2.3230477007660206, 7.638307525527196]     [1.5018513090971766, -0.0003341042588377169, 1.1468325377055717, 2.4337433649057965, 5.17726614332079, 0.9060270793904831, 2.290363901022369, 2.6714794881889974, 3.3774165689532363, 2.8899046718379386, 2.917528735162025, 2.958298734244091, 3.0031848648942785, 2.4135073321687393, 3.032376433669132, 2.5987650981295265, 3.2865171676445346, 2.842949417208943, 2.2592300914027943, 2.424759960987704, 2.5488456557167414, 2.2457027166557064, 2.0463726171511234, 1.8088567708041736, 1.282714968138201, 1.382127919145569, 1.4040029172804078, 1.1766876433827096, 1.1227833500186646, 0.780372557594342]   [[0.37075072321636565, 0.19914324185730398, 0.47755553428234665], [0.37615240662434624, 0.24423111535593994, 0.5035144503364641], [0.27527571766710324, 0.256325685689864, 0.40825126300785886], [0.1331674101284051, 0.23114033854089616, 0.24234445510893135], [0.012804513479247215, 0.1462707774461434, 0.08098589347953217], [-0.052853229027031934, 0.054988797166639264, -0.034861310734712686], [-0.06681043280008764, -0.010694806205991617, -0.10991675399505133], [-0.04103934674099553, -0.046502448128839774, -0.15493312097644799], [0.0028413601558562734, -0.06261128830101828, -0.17472716966723387], [0.04848063398630531, -0.06617117292624275, -0.17855129964326485], [0.08569944675554732, -0.06390225924700535, -0.17316130403549462], [0.11138874166690112, -0.06131853479152888, -0.16369568369407145], [0.12657862172242845, -0.0602776694319449, -0.1528045077413116], [0.13307323425180617, -0.06036523939615951, -0.14282924781408352], [0.1338424739034082, -0.06128389609035462, -0.1344035691490438], [0.1314054097523602, -0.06246943709236574, -0.1277288007384748], [0.12766674109762394, -0.06351302192731917, -0.12269662859594663], [0.12383933564295833, -0.06422305729484698, -0.11906894569274482], [0.12055820462382881, -0.06457443939030963, -0.11657846827038246], [0.11806630308717202, -0.06461989362084253, -0.11494570690545919], [0.11636345458401713, -0.06445979185706324, -0.11394857404410505], [0.115313605340214, -0.0641911078800646, -0.11340480918415669], [0.11474073954496551, -0.06388717750694622, -0.11317115059121688], [0.11447973954793765, -0.06359476981237216, -0.1131344110421661], [0.11439905599152243, -0.06333883228848047, -0.11320910429478903], [0.11440609619373508, -0.06312893618770907, -0.1133344725465194], [0.1144430792785865, -0.06296501429128415, -0.11347048541371213], [0.11447910839017221, -0.0628418286501794, -0.11359334336473648], [0.11450120706750032, -0.06275199335429314, -0.1136913786360382], [0.11450701483172095, -0.06268796597068416, -0.11376126227680045]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 2019-02-26 15:57:25.814662: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "rm -rf /tools/**/**/**/**/**/*.pyc\n",
    "model_dir=$(ls ${PWD}/trained_model/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=${PWD}/trained_model/export/exporter/${model_dir} \\\n",
    "    --json-instances=./sequences.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCloud ML-Engine prediction from deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataframe to instances list to get sent to ML-Engine\n",
    "instances = [{column: np.array2string(a = create_time_series_with_anomaly(1, sequence_length, percent_sequence_before_anomaly, percent_sequence_after_anomaly, tag[\"clean_freq\"], tag[\"clean_ampl\"], tag[\"clean_noise_noise_scale\"]), separator = ',').replace('[','').replace(']','').replace('\\n','') for tag in tag_data_list for column in CSV_COLUMNS} for _ in range(0, number_of_prediction_instances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send instance dictionary to receive response from ML-Engine for online prediction\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials)\n",
    "\n",
    "request_data = {\"instances\": instances}\n",
    "\n",
    "parent = 'projects/%s/models/%s/versions/%s' % (PROJECT, 'lstm_autoencoder_anomaly_detection', 'v1')\n",
    "response = api.projects().predict(body = request_data, name = parent).execute()\n",
    "print(\"response = {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../../../../tutorials/machine_learning_deepdive/06_structured/babyweight/* lstm_encoder_decoder_autoencoder_anomaly_detection_module/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "lstm_encoder_decoder_autoencoder_anomaly_detection.ipynb\n",
      "lstm_encoder_decoder_autoencoder_anomaly_detection_module\n",
      "tf_mahalanobis_test.ipynb\n",
      "trained_model\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PKG-INFO  setup.cfg  setup.py  trainer\n"
     ]
    }
   ],
   "source": [
    "!ls lstm_encoder_decoder_autoencoder_anomaly_detection_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
